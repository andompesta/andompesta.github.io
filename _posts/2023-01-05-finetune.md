---
layout: post
mathjax: true
title:  "Finetune large models"
author: "Sandro Cavallari"
tag: "Deep Learning"
comments_id: 8
---

Nowadays, it is common practice to develop new machine learning projects starting from a large pre-trained model and fine-tune it on the task at hand.
[Sam Altman](https://youtu.be/WHoWGNQRXb0?t=170), at the time of writing the CEO of OpenAI, recently mentioned that he envision a feature where the most valuable startups are the one capable of tune publicly available foundation models to specific domain; rather than train a propretary model from scratch.
Thus, tune or fine-tune large models is a key capability that machine learning practitoners need to learn as much as being able to train a deep neural networks was a majour skill that each scientist had perfected in the last few years.

On the web there are a pletora of [recipes for training neural networks](http://karpathy.github.io/2019/04/25/recipe/) (thanks karpathy you saved me multiple times), but only a limited amount of resources specifically takling the fine-tuning problem.
To this end, this article aims at describe the strategy that I usually adopt when fine-tuning a large trasnformer model.

