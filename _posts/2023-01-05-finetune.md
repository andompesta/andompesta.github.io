---
layout: post
mathjax: true
title:  "Finetune Large Models"
author: "Sandro Cavallari"
tag: "Deep Learning"
comments_id: 8
---

Nowadays, it is common practice to develop new machine learning projects starting from a large pre-trained model and fine-tune it on the task at hand.
[Sam Altman](https://youtu.be/WHoWGNQRXb0?t=170){:target="_blank"}, at the time of writing the CEO of OpenAI, recently mentioned that he envision a feature where the most valuable startups are the one capable of tune publicly available foundation models to specific domain; rather than train a propretary model from scratch.
Thus, tune or fine-tune large models is a key capability that machine learning practitoners need to learn as much as being able to train a deep neural networks was a majour skill that each scientist had perfected in the last few years.

<div style="text-align:center;" id="fig:intro_finetuning">
    <figure>
        <img src="{{site.baseurl}}/assets/img/finetune/intro.png" style="max-width: 98%">
        <figcaption style="font-size:small;">
            Figure 1: Model finetuning
        </figcaption>
    </figure>
</div>



On the web there are a pletora of [recipes for training neural networks](http://karpathy.github.io/2019/04/25/recipe/){:target="_blank"} (thanks karpathy you saved me multiple times), but only a limited amount of resources specifically takling the fine-tuning problem.
To this end, this article aims at describe the strategy that I adopt when fine-tuning a large trasnformer model.


# The Procedure

The procedeure here-described is build on the key principle that: *the fine-tuning step should the continuation of the training phase, but on a different dataset*.
Main focus is on avoinding the introduction of any unneccesary difference between the original and fine-tuning tasks at every stage of the pipeline.
Therefore we will minimise the number of parameters that needs to be adapted: resulting in greater transferability between tasks. 


### 1. Dataset Preprocessing

It might sounds obious, but most of the time I experienced catastrofic forgetting was due to a bad implementation of the preprocessing steps applied to the fine-tuned data.

On the one hand, for natural language processing tasks special cares need to be taken during the tokenization of the input text. 
Specifically, always double check that the correct tokenizers is applied, in particular check that the correct PAD-token is used.
Nowadays [HuggingFace](https://huggingface.co/) made a terrifc effor in providing a bug-free implementation of most Transformers models and their respective tokenizers, but untill a few years ago sub-word tokenizers were extreamly challening to reimplement.
Another error prone transformation is the creation of the input mask: double check that causal and PAD masks are correctly combined and applied in the correct layer.

On the other hand, for compute vision tasks always double check that the proper transformations are applied to the input images.
Order of the input channel, and normalization and interpolation strategy for the resizing steps are among the most error prone functionality to correctly re-create.

Finally, if you are working with graphs structure, double check how you uniformize to a fixed size a batch of nodes having a different amount of neighours.

### 2. Optimizer

According to the key principle, the fine-tuning procedure should use the same optimizser used during training.
While this is not alwasy possbile, most of the foundation models [[1]](#ref:foundation-models) are trained using an optimizers known as AdamW [[2]](#ref:loshchilov).
AdamW is a variation of the well known Adam that better generalize to unknown exaples thanks to the decupling of the main loss and the regularization term known as *weight decay*.
Note that back in the days PyTorch did not provided a proper implementation of the AdamW optimizers; thus multiple open-source projects provided their own implementation.
Across the many the one provided by Meta's [fairseq](https://github.com/facebookresearch/fairseq/blob/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/optim/adam.py#L110){:target="_blank"} is higly reiaiable and I still use it nowadays.


### 3. Weight Decay

By default, AdamW will apply the weight decay mechanism to all model's parameters, yet in most cases weight decay is *NOT applyed to all weights*.
Weight decay objective is to regularise the training process toward learning models with weight smaller in magnitude.
To this end, biases and gains are usually not included in the weight decay loss for multiple reasons:

 1. Biases are used to shift the activation function of a neuron and do not typically have a significant impact on the overall complexity of the model. As a result, applying weight decay to the biases would not have a significant effect on the model's ability to overfit.

 2. Gains are commonly used in normalization layers to enable high expressivity while ensuring gaussian-like activations. Thus, like biases, do not typically have a significant impact on the model complexity. On the contrary, weight decay applied to gains might limit the expressivity of a model resulting in underfitting.

 3. Overall, weight decay is typically applied only to the weights of a model, rather than the biases or gains, in order to encourage the model to use small weights and reduce the risk of overfitting. 
In PyTorch this is commonly achieved by creating two groups of parameters for the optimizers:

```python
def get_group_params(
    model: nn.Module,
    weight_decay: float = 0.01,
    no_decay_patterns: Optional[List[str]] = ["bias", "ln_.+weight", "ln_.+bias"],
) -> list[dict[str, Any]]:
    """function generating the appropriate group's parameters to optimize

    :param model: model to train
    :type model: nn.Module
    :param weight_decay: weight decay factor, defaults to 0.01 
    :type weight_decay: float
    :param no_decay_patterns: regex patterns used to identify parameters for witch no decay is used, defaults to ["bias", "ln_.+weight", "ln_.+bias"]
    :type no_decay_patterns: Optional[List[str]], optional
    :return: two groups of parameters to optimize
    :rtype: list[dict[str, Any]]
    """
    optimizer_grouped_parameters = [
        dict(
            params=[
                # parameters with weight decay
                p for n, p in model.named_parameters() if not any(
                    [re.search(pattern, n) for pattern in no_decay_patterns]
                )
            ],
            weight_decay=weight_decay,
        ),
        dict(
            params=[
                # parameters without weight decay
                p for n, p in model.named_parameters() if any(
                    [re.search(pattern, n) for pattern in no_decay_patterns]
                )
            ],
            weight_decay=0.,
        )
    ]
    return optimizer_grouped_parameters
```

If you don't like regex patterns, karpathy provided a nite alternative implementation in its [miniGPT](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L224) repository.

### 4. Learning Rate

While finetuning your models, you will be surprised but you will find out that the best performing learning rate will be quite small.
A sound starting point is a learning rate of `5e-5`.
As overmentioned, such learning rate is extreamly small compared to when you train a neural network from strach, but it will fit the new data well and prevent catastrofic forgetting.

Moreover, in many cases it is benficial to use a learning rate scheduler while fine-tuning your model.
Across the many learning rate schedulers a linear decay scheduler with warmup phase works well while being simple to implement.

```python
from torch.optim.lr_scheduler import LambdaLR
from torch.optim.optimizer import Optimizer

def get_linear_scheduler_with_warmup(
    optim: Optimizer,
    num_warmup_step: float,
    num_training_step: int,
    last_epoch: int = -1,
):
    """
    get a scheduler that linearly increase the learning data between [0, num_warmup_steps) and then linearly decrease
    it between [num_warmup_steps, num_training_step).
    """

    def lr_lambda(current_step):
        if current_step < num_warmup_step:
            return float(current_step) / float(max(1.0, num_warmup_step))
        return max(
            0.0,
            float(num_training_step - current_step) /
            float(max(1.0, num_training_step - num_warmup_step)),
        )

    return LambdaLR(optim, lr_lambda, last_epoch)
```

The assuption is to linearnly increase the learning rate during the warmup phase (usualy lasting for one epoch), so to gradually align the weights of the models to the new task.
Afterward, the learning rate is gradually decreased to allows for finegrained adjustments of the weights.
Fig. [[2]](#fig:scheduler) shows how the learning rate is adjust across different epochs.

<div style="text-align:center;" id="fig:scheduler">
    <figure>
        <img src="{{site.baseurl}}/assets/img/finetune/scheduler.png" style="max-width: 98%">
        <figcaption style="font-size:small;">
            Figure 2: Example of linear scheduler with warmup. 
        </figcaption>
    </figure>
</div>


### 5. Parameter feezing


## 6. Overfit

Overfit a single batch and achieve 100 training performances and decraseing eval performance

# Refereces

<ol>
    <li id="ref:foundation-models"> Bommasani, Rishi, et al. "On the opportunities and risks of foundation models." arXiv preprint arXiv:2108.07258 (2021).</li>
    <li id="ref:loshchilov"> Loshchilov, Ilya, and Frank Hutter. "Decoupled weight decay regularization." arXiv preprint arXiv:1711.05101 (2017).</li>
</ol>