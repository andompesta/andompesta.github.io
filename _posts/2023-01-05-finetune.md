---
layout: post
mathjax: true
title:  "Finetune Large Models"
author: "Sandro Cavallari"
tag: "Deep Learning"
comments_id: 8
---

Nowadays, it is common practice to develop new machine learning projects starting from a large pre-trained model and fine-tune it on the task at hand.
[Sam Altman](https://youtu.be/WHoWGNQRXb0?t=170){:target="_blank"}, at the time of writing the CEO of OpenAI, recently mentioned that he envision a feature where the most valuable startups are the one capable of tune publicly available foundation models to specific domain; rather than train a propretary model from scratch.
Thus, tune or fine-tune large models is a key capability that machine learning practitoners need to learn as much as being able to train a deep neural networks was a majour skill that each scientist had perfected in the last few years.

<div style="text-align:center;" id="fig:intro_finetuning">
    <figure>
        <img src="{{site.baseurl}}/assets/img/finetune/intro.png" style="max-width: 98%">
        <figcaption style="font-size:small;">
            Figure 1: Model finetuning
        </figcaption>
    </figure>
</div>



On the web there are a pletora of [recipes for training neural networks](http://karpathy.github.io/2019/04/25/recipe/){:target="_blank"} (thanks karpathy you saved me multiple times), but only a limited amount of resources specifically takling the fine-tuning problem.
To this end, this article aims at describe the strategy that I adopt when fine-tuning a large trasnformer model.


# The Procedure

The procedeure here-described is build on the key principle that: *the fine-tuning step should the continuation of the training phase, but on a different dataset*.
Main focus is on avoinding the introduction of any unneccesary difference between the original and fine-tuning tasks at every stage of the pipeline.
Therefore we will minimise the number of parameters that needs to be adapted: resulting in greater transferability between tasks. 


### 1. Dataset Preprocessing

It might sounds obious, but most of the time I experienced catastrofic forgetting was due to a bad implementation of the preprocessing steps applied to the fine-tuned data.

On the one hand, for natural language processing tasks special cares need to be taken during the tokenization of the input text. 
Specifically, always double check that the correct tokenizers is applied, in particular check that the correct PAD-token is used.
Nowadays [HuggingFace](https://huggingface.co/) made a terrifc effor in providing a bug-free implementation of most Transformers models and their respective tokenizers, but untill a few years ago sub-word tokenizers were extreamly challening to reimplement.
Another error prone transformation is the creation of the input mask: double check that causal and PAD masks are correctly combined and applied in the correct layer.

On the other hand, for compute vision tasks always double check that the proper transformations are applied to the input images.
Order of the input channel, and normalization and interpolation strategy for the resizing steps are among the most error prone functionality to correctly re-create.

Finally, if you are working with graphs structure, double check how you uniformize to a fixed size a batch of nodes having a different amount of neighours.

### 2. Optimizer

According to the key principle, the fine-tuning procedure should use the same optimizser used during training.
While this is not alwasy possbile, most of the foundation models [[1]](#ref:foundation-models) are trained using an optimizers known as AdamW [[2]](#ref:loshchilov).
AdamW is a variation of the well known Adam that better generalize to unknown exaples, while being stable during training.
Note that back in the days PyTorch did not provided a proper implementation of the AdamW optimizers; thus multiple open-source projects provided their own implementation.
Across the many I found the one provided by Meta's [fairseq](https://github.com/facebookresearch/fairseq/blob/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/optim/adam.py#L110){:target="_blank"} to be the most reiaiable and I still use it nowadays.

### 3. Weight Decay

By default, AdamW applies the weight decay to all models parameters, however 

 - optimizer implementation
 - weight decay
 - learning rate
 - learning rate scheduling

### 3. Parameter feezing


### 4. Overfit

Overfit a single batch and achieve 100 training performances and decraseing eval performance

# Refereces

<ol>
    <li id="ref:foundation-models"> Bommasani, Rishi, et al. "On the opportunities and risks of foundation models." arXiv preprint arXiv:2108.07258 (2021).</li>
    <li id="ref:loshchilov"> Loshchilov, Ilya, and Frank Hutter. "Decoupled weight decay regularization." arXiv preprint arXiv:1711.05101 (2017).</li>
</ol>