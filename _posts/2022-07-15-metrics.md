---
layout: post
mathjax: true
title: "Evaluation Metrics for Recommendation Systems"
author: "Sandro Cavallari"
tag: "Deep Learning"
comments_id: 1
---

Recommendation systems are one of the most well known application for machine learning as it is usually modelled as a binary classification task, where the positive class ($y^+$) represents clicks or conversions, and the negative class ($y^-$) represents examples without interaction.
In the most common case binary classifiers are implemented as single output logistic regressors.
As such they do not directly provide a class lable, but rather the predicted probability of the postive class $$\rmP( y = y^+)$$.
The final label is defined comparing the predicted probability to a threshold $t$ such that:

$$
\hat{y} = \begin{cases}
1 & \text{if} ~ \rmP( y = y^+) \geq t \\
0 & \text{o.w.}
\end{cases}
$$

Common sense suggests that a reasonable threshold could be $t=0.5$.
However, this is hardly the case in the real-world as datasets are hardly perfectly ballanced, models are not always perfectly calibrated and different applications have different [sensitivity vs specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) tradeoff.
To overcome this problem we need metrics able to identify the best performing model indipendently of the threshold.

<div style="text-align:center;">
<table style="border:none; background:transparent; text-align:center;" align="center">
    <tbody>
    <tr>
        <td style="border:none;" rowspan="2"></td>
        <td style="border:none;"></td>
        <td style="background:#bbeeee;" colspan="2"><b>Predicted condition</b></td>
    </tr>
    <tr>
        <td style="background:#eeeeee;"><b>Total population </b> <br> $P + N $ </td>
        <td style="background:#ccffff;"><b> Positive</b> <br> $PP$</td>
        <td style="background:#aadddd;"><b> Negative</b> <br> $PN$</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2" class="nowrap unsortable" style="line-height:99%;vertical-align:middle;padding:.4em .4em .2em;background-position:50% .4em !important;min-width:0.875em;max-width:0.875em;width:0.875em;overflow:hidden;background:#eeeebb;"><div style="-webkit-writing-mode: vertical-rl; -o-writing-mode: vertical-rl; -ms-writing-mode: tb-rl;writing-mode: tb-rl; writing-mode: vertical-rl; layout-flow: vertical-ideographic;display: inline-block; -ms-transform: rotate(180deg); -webkit-transform: rotate(180deg); transform: rotate(180deg);;-ms-transform: none ;padding-left:1px;text-align:center;"><b>Actual condition</b></div>
        </td>
        <td style="background:#ffffcc;"><b>Positive <br> $P$</b></td>
        <td style="background:#ccffcc;"><b>True&nbsp;positive <br> <span style="font-size:85%;">hit</span></b> <br> $TP$ </td>
        <td style="background:#ffdddd;"><b>False&nbsp;negative <br> <span style="font-size:85%;">miss</span></b> <br> $FN$ </td>
        <td style="background:#eeffee;"><b>True positive rate <br> <span style="font-size:85%;"> recall&nbsp;/&nbsp;sensitivity</span></b> <br> $TPR = \frac{TP}{P} = 1 - FNR$
        </td>
        <td style="background:#ffeeee;"><b>False&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;">miss&nbsp;rate</span></b> <br> $FNR = \frac{FN}{P} = 1 - TPR$
        </td>
    </tr>
    <tr>
        <td style="background:#ddddaa;"><b>Negative</b> <br> $N$ </td>
        <td style="background:#ffcccc;"><b>False&nbsp;positive <br> <span style="font-size:85%;">type&nbsp;I&nbsp;error / false&nbsp;alarm</span> </b> <br> $FP$
        </td>
        <td style="background:#bbeebb;"><b>True&nbsp;negative <br> <span style="font-size:85%;">correct&nbsp;rejection</span></b> <br> $TN$
        </td>
        <td style="background:#eedddd;"><b>False&nbsp;positive&nbsp;rate <br> <span style="font-size:85%;">probability&nbsp;of&nbsp;false&nbsp;alarm</span> </b> <br> $ FPR = \frac{FP}{N} = 1 − TNR $
        </td>
        <td style="background:#ddeedd;"><b>True&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;"> specificity / selectivity </span></b> <br> $ TNR = \frac{TN}{N} = 1 − FPR $
        </td>
    </tr>
    <tr>
        <td style="border:none;"></td>
        <td style="border:none;"></td>
        <td style="background:#eeffee;"><b>Precision<br> <span style="font-size:85%;">positive&nbsp;predictive&nbsp;value</span> </b> <br> $PR = \frac{TP}{PP}$
        </td>
        <td style="background:#ffeeee;"><b>Fale&nbsp;omission&nbsp;rate </b> <br> $FOR = \frac{FN}{PN}$
        </td>
    </tr>
    </tbody>
</table>
<p style="font-size:small;">Table 1: definition of all the metric reported and build on to of a Confusion matrix, credito to <a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="blank">wikipedia</a></p>
</div>


## ROC-AUC

The Receiver Operating Characteristic (ROC) is an analysis tool initially proposed by [Provost et al.](#ref:provost) aiming at comparing classifiers performance.
The ROC curve is build as the interpolation between the **False Positive Rate** (FPR), on the x-axe, and the **True Positive Rate** (TPR), on the y-axe, computed at differnet thresholds level.
Intuitively, the TPR represents how well a classifier can identify all the positive examples; whereas the FPR indicate how likely an error will occurs in the negative class (for a formal definition of TPR and FPR consult the confusion matrix reported in Tab. 1).

<div style="text-align:center;">
    <img src="{{site.baseurl}}/assets/img/metrics/roc-auc.png" style="max-width: 70%">
    <p style="font-size:small;">Figure 1: example of an ROC-AUC curve</p>
</div>

In order to easily compare ROC curves generated by multiple models, the area under such generated curves is used as a single scalar value  representative of the classifier performances.
Thus, ROC-AUC is defineas as:

$$
\text{ROC-AUC} = \int_0^1 TPR ~ \delta \small FPR.
$$

Note that ROC-AUC exibit the following properties [[5](#ref:flach)]:
  1. It can be interpreted as "the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one" [[2](#ref:fernandez)][[3](#ref:fawcett)]. Under this interpretation is possible to define the ROC-AUC as the **expected accuracy** obtained by the classifier for each thresholds.
  2. An universal baseline is always available, as a random classifier will have ROC-AUC of 0.5.
  3. It cares about the ranking obtained from the predictions, but does not consider the actual predicted value.
  4. The perfect model is represented by the point $(0, 1)$ and has a area of 1.
  5. The point $(0, 0)$ indentify a model that never issue a positive prediction.
  6. The point $(1, 1)$ implement the oppisite strategy, here only positive predictions are made.
  7. ROC-AUC is a linear space, thus allows for easy interpolation and visual interpretation.

### Highly Imbalanced Dataset

ROC curves are known to be "over-optimistic" at scoring model performances when the datasets are higly skwed and there is a high interest in evaluating the model w.r.t. the positive class.
For example, consider the case where you have two datasets; the former composed by 100 negative examples and 50 positive examples, while the later composed by 100 negative examples and 25 positive examples.
As shown in Fig. 2a , let assume that the negative examples overlap with the positive onces according to an uniform distribution.

<div style="text-align:center;">
    <figure>
        <figure>
            <img src="{{site.baseurl}}/assets/img/metrics/datasets.png" style="max-width: 90%">
            <figcaption style="font-size:small;">
                Figure 2.a: Two datasets with same label distribution but different amount of positive examples. Dataset 1 contains 50 positive examples while Dataset 2 contains only 25 positive examples. In both cases there are 100 negative lables.
            </figcaption>
        </figure>
        <figure>
            <img src="{{site.baseurl}}/assets/img/metrics/datasets-auc.png" style="max-width: 100%">
            <figcaption style="font-size:small;">
                Figure 2.b: ROC-AUC computed on D1 and D2.
            </figcaption>
        </figure>
    </figure>
</div>

Let try to compute the AUC for both of the datasets:
  - then for D1, as threshold ($t$) moves from $0$ to $0.5$ the True Positive Rate remains constant to 1; instead for $t > 0.5$ both TPR and FPR decrease linearly since positive and negative examples start to be homogeneusly mixed.
  - similarly for D2, when $0 \leq t \leq 0.75$ then $TPR = 1$; while for $t>0.75$ TRP and FPR decreases linearnly.

A graphical representatin is provided in Fig 2.b where it is shown how the ROC-AUC of the second dataset is bigger than the first dataset even if the models have actually the same maximum F1-score (maximum F1-score on D1 is achieved with $t=0.5$ while on D2 the best threshold is $t=0.5$).
A deeper analysis suggests that this over-estimation problem arise when a large change in the false positive leads only to a small change in false positive rate since the dataset is dominated by the nagative class [[4](#ref:davis)].

## Precision-Recall curve

When the main objective is to evaluate a model on the positive class, then PR-curves are more informative.
Fig. 3 shows how PR-curves are build by ploting the **Precision** as a function of the True Positive Rate (or Recall).
By inspecting Tab. 1, it is visible how PR-curves effectivly do consider only statistics related to the positive class; thus are inherently robust to highly skewed datasets [[6](#ref:branco)].
Perhapes motivated by the similarity with ROC-curves, PR-curves became a popular alternative to analyis models on higly skewed datasets.

<div style="text-align:center;">
    <figure>
        <img src="{{site.baseurl}}/assets/img/metrics/pr-curve.png" style="max-width: 80%">
        <figcaption style="font-size:small;">
            Figure 3: Precision-Recall curve and PR-AUC.
        </figcaption>
    </figure>
</div>

As for ROC curves, the PR-AUC is defined as:

$$
\text{PR-AUC} = \int_0^1 PR ~ \delta \small TPR.
$$

Overall, we can distinuish the following properties for the PR-AUC:
  1. PR-AUC has no intrinsic meaning expect the geometric one.
  2. While ROC-AUC has a baseline value at 0.5, in PR-AUC there is no universal baseline to compare with.
  3. PR-AUC is not directly connected with F1-score; thus is unrelated to the calibration score of the model.
  4. The perfect model is represented by the point $(1, 1)$ and has a area of 1.
  5. PR space is an hyperbolic space, thus more difficult to visualy identify similarly performing models.

## LogLoss

One of the main characteristis of the previous their focus on the ranking obtained from the model output, but they ignore the predicted value itself.
In most reccomandation systems this is not an issue, but there are tasks were the predicted score is actually important, e.g., biddings strategies for real-time ads allocation.
In this problem instances choosing a threshold $t$ is not important, rather there is high interest in having a predictions that, on average, *reliabily* resemble the groud true, given these predictions.
That is: "the probability associated with the predicted class label should reflect its ground truth correctness likelihood" [[7](#ref:guo-calibdation)].
For example, consider a dataset composed of 10 examples and assume that the model assigne a probability of 0.6 to every example.
Then, if the model is **calibrated**, we expect 6 examples to belong to the positive class.

Note that:
  1. Calibration is a mesure of **uncertanty**, not accuracy.
  2. A calibrated model allowing for a higher interpretability as the provided probabilities have intrisnic meaning.
A common metric used to monitor the model's calibration is the negative **LogLoss**:

$$
\mathcal{L} = - \frac{1}{N} \sum_{i=1}^N \Big( y \log(\hat{p}) + (1 - y) \log(1 - \hat{p}) \Big)
$$

where $y$ represnt the ground true label, $\hat{p}$ is the predicted probability and $N$ is the size of the dataset.
The LogLoss has a long application history as training loss for (deep) logistic regressions; yet here a claim is made to adopt it also as a validation and test metrics.
Moder deep neural network are higly accurete, but are also known to be overconfident showing poor uncertanty estimation [[8](#ref:sensoy-uncertanty)].
Thus, it is handy to have a scalar metric able to summarize the calbration characteristics of a model and compare it to the error in the prediction.


## Birer Score

Another popular method to capture the model calibration in a scalar value is by computing the Brier Score[[9](#ref:brier-curves)]:

$$
\mathcal{BS} = \frac{1}{N} \sum_{i=1}^N \Big( y_i - \hat{p}_i \Big)^2
$$

The Brier Score is the mean-squared-error between the ground ture and the predicted probabilities; thus the lower the value the better.

## Refereces

<ol>
    <li id="ref:provost"> Provost, F., Fawcett, T. & Kohavi, R. The case against accuracy estimation for comparing induction algorithms. Int. Conf. Mach. Learn. 445 (1998)
    </li>
    <li id="ref:fernandez"> Fernández, Alberto, et al. Learning from imbalanced data sets. Vol. 10. Berlin: Springer, 2018. </li>
    <li id="ref:fawcett"> Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/10.1016/j.patrec.2005.10.010 </li>
    <li id="ref:davis"> Davis, J., & Goadrich, M. (2006). The relationship between Precision-Recall and ROC curves. ICML 2006 - Proceedings of the 23rd International Conference on Machine Learning, 2006, 233–240. </li>
    <li id="ref:flach"> Flach, P. A. & Kull, M. Precision-Recall-Gain curves: PR analysis done right. Adv. Neural Inf. Process. Syst. 2015-Janua, 838–846 (2015). </li>
    <li id="ref:branco"> Branco, P., Torgo, L., & Ribeiro, R. (2015). A Survey of Predictive Modelling under Imbalanced Distributions. 1–48. http://arxiv.org/abs/1505.01658 </li>
    <li id="ref:guo-calibdation"> Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. 34th International Conference on Machine Learning, ICML 2017, 3, 2130–2143. </li>
    <li id="ref:sensoy-uncertanty"> Sensoy, M., Kaplan, L., & Kandemir, M. (2018). Evidential deep learning to quantify classification uncertainty. ArXiv, NeurIPS. </li>
    <li id="ref:brier-curves"> Hernández-Orallo, J., Flach, P., & Ferri, C. (2011). Brier curves: A new cost-based visualisation of classifier performance. Proceedings of the 28th International Conference on Machine Learning, ICML 2011, 585–592. </li>
</ol>