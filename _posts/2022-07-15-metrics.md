---
layout: post
mathjax: true
title: "Reccomandation Metrics"
author: "Sandro Cavallari"
tag: "Deep Learning"
---

Reccomandation systems are one of the most well known application for machine learning as it is usually modelled as a binary classification task, where the positive class ($y^+$) represents clicks or conversions, and the negative class ($y^-$) represents examples without interaction.
In the most common case binary classifiers are implemented as single output logistic regressors.
As such they do not directly provide a class lable, but rather the predicted probability of the postive class $$\rmP( y = y^+)$$.
The final label is defined comparing the predicted probability to a threshold $t$ such that:

$$
\hat{y} = \begin{cases}
1 & \text{if} ~ \rmP( y = y^+) \geq t \\
0 & \text{o.w.}
\end{cases}
$$

Common sense suggests that a reasonable threshold could be $t=0.5$.
However, this is hardly the case in the real-world as datasets are hardly perfectly ballanced, models are not always perfectly calibrated and different applications have different [sensitivity vs specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) tradeoff.
To overcome this problem we need metrics able to identify the best performing model indipendently of the threshold.

<div style="text-align:center;">
<table style="border:none; background:transparent; text-align:center;" align="center">
    <tbody>
    <tr>
        <td style="border:none;" rowspan="2"></td>
        <td style="border:none;"></td>
        <td style="background:#bbeeee;" colspan="2"><b>Predicted condition</b></td>
    </tr>
    <tr>
        <td style="background:#eeeeee;">Total population <br> $ = P + N $ </td>
        <td style="background:#ccffff;"><b> Positive</b> <br> $PP$</td>
        <td style="background:#aadddd;"><b> Negative</b> <br> $PN$</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2" class="nowrap unsortable" style="line-height:99%;vertical-align:middle;padding:.4em .4em .2em;background-position:50% .4em !important;min-width:0.875em;max-width:0.875em;width:0.875em;overflow:hidden;background:#eeeebb;"><div style="-webkit-writing-mode: vertical-rl; -o-writing-mode: vertical-rl; -ms-writing-mode: tb-rl;writing-mode: tb-rl; writing-mode: vertical-rl; layout-flow: vertical-ideographic;display: inline-block; -ms-transform: rotate(180deg); -webkit-transform: rotate(180deg); transform: rotate(180deg);;-ms-transform: none ;padding-left:1px;text-align:center;"><b>Actual condition</b></div>
        </td>
        <td style="background:#ffffcc;"><b>Positive <br> $P$</b></td>
        <td style="background:#ccffcc;"><b>True&nbsp;positive <br> <span style="font-size:85%;">hit</span></b> <br> $TP$ </td>
        <td style="background:#ffdddd;"><b>False&nbsp;negative <br> <span style="font-size:85%;">miss</span></b> <br> $FN$ </td>
        <td style="background:#eeffee;"><b>True positive rate <br> <span style="font-size:85%;"> recall&nbsp;/&nbsp;sensitivity</span></b> <br> $TPR = \frac{TP}{P} = 1 - FNR$
        </td>
        <td style="background:#ffeeee;"><b>False&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;">miss&nbsp;rate</span></b> <br> $FNR = \frac{FN}{P} = 1 - TPR$
        </td>
    </tr>
    <tr>
        <td style="background:#ddddaa;"><b>Negative</b> <br> $N$ </td>
        <td style="background:#ffcccc;"><b>False&nbsp;positive <br> <span style="font-size:85%;">type&nbsp;I&nbsp;error / false&nbsp;alarm</span> </b> <br> $FP$
        </td>
        <td style="background:#bbeebb;"><b>True&nbsp;negative <br> <span style="font-size:85%;">correct&nbsp;rejection</span></b> <br> $TN$
        </td>
        <td style="background:#eedddd;"><b>False&nbsp;positive&nbsp;rate <br> <span style="font-size:85%;">probability&nbsp;of&nbsp;false&nbsp;alarm</span> </b> <br> $ FPR = \frac{FP}{N} = 1 − TNR $
        </td>
        <td style="background:#ddeedd;"><b>True&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;"> specificity / selectivity </span></b> <br> $ TNR = \frac{TN}{N} = 1 − FPR $
        </td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td style="background:#eeffee;"><b>Precision<br> <span style="font-size:85%;">positive&nbsp;predictive&nbsp;value</span> </b> <br> $PR = \frac{TP}{PP}$
        </td>
        <td style="background:#ffeeee;"><b>Fale&nbsp;omission&nbsp;rate </b> <br> $FOR = \frac{FN}{PN}$
        </td>
    </tr>
    </tbody>
</table>
<p style="font-size:small;">Table 1: definition of all the metric reported and build on to of a Confusion matrix, credito to <a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="blank">wikipedia</a></p>
</div>


# ROC-AUC

The Receiver Operating Characteristic (ROC) is an analysis tool initially proposed by [Provost et al.](#ref:provost) aiming at comparing classifiers performance.
The ROC curve is build as the interpolation between the **False Positive Rate** (FPR), on the x-axe, and the **True Positive Rate** (TPR), on the y-axe, computed at differnet thresholds level.
Intuitively, the TPR represents how well a classifier can identify all the positive examples; whereas the FPR indicate how likely an error will occurs in the negative class (for a formal definition of TPR and FPR consult the confusion matrix reported in Tab. 1).

<div style="text-align:center;">
    <img src="{{site.baseurl}}/assets/img/metrics/roc-auc.png" style="max-width: 70%">
    <p style="font-size:small;">Figure 1: example of an ROC-AUC curve</p>
</div>

In order to easily compare ROC curves generated by multiple models, the area under such generated curves is used as a single scalar value  representative of the classifier performances.
Thus, ROC-AUC is defineas as:

$$
\text{ROC-AUC} = \int_0^1 ROC(t) \delta t.
$$

Note that ROC-AUC exibit the following properties:
  1. It can be interpreted as "the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one" [[2]](#ref:fernandez)[[3]](#ref:fawcett).
  2. It cares about the ranking obtained from the predictions, but does not consider the actual predicted value.
  3. The perfect model is represented by the point $(0, 1)$ and has a area of 1.
  4. The point $(0, 0)$ indentify a model that never issue a positive prediction.
  5. The point $(1, 1)$ implement the oppisite strategy, here only positive predictions are made.
  6. They are considered to be robust to changes in the portion of positive to negaiteve instance as TPR and FPR indipendently consider only the positive or only the negative class.

### Highly Imbalanced Dataset

ROC curves are known to be "over-optimistic" at scoring model performances when the datasets are higly skwed and there is a high interest in evaluating the model w.r.t. the positive class.
For example, consider the case where you have two datasets; the former composed by 100 negative examples and 50 positive examples, while the later composed by 100 negative examples and 25 positive examples.
As shown in Fig. 2a , let assume that the negative examples overlap with the positive onces according to an uniform distribution.

<div style="text-align:center;">
    <figure>
        <figure>
            <img src="{{site.baseurl}}/assets/img/metrics/datasets.png" style="max-width: 90%">
            <figcaption style="font-size:small;">
                Figure 2.a: Two datasets with same label distribution but different amount of positive examples. Dataset 1 contains 50 positive examples while Dataset 2 contains only 25 positive examples. In both cases there are 100 negative lables.
            </figcaption>
        </figure>
        <figure>
            <img src="{{site.baseurl}}/assets/img/metrics/datasets-auc.png" style="max-width: 90%">
            <figcaption style="font-size:small;">
                Figure 2.b: ROC-AUC computed on D1 and D2.
            </figcaption>
        </figure>
    </figure>
</div>

Let try to compute the AUC for both of the datasets:
  - then for D1, as threshold ($t$) moves from $0$ to $0.5$ the True Positive Rate remains constant to 1; instead for $t > 0.5$ both TPR and FPR decrease linearly since positive and negative examples start to be homogeneusly mixed.
  - similarly for D2, when $0 \leq t \leq 0.75$ then $TPR = 1$; while for $t>0.75$ TRP and FPR decreases linearnly.

A graphical representatin is provided in Fig 2.b where it is shown how the ROC-AUC of the second dataset is bigger than the first dataset even if the models have actually the same maximum F1-score (maximum F1-score on D1 is achieved with $t=0.5$ while on D2 the best threshold is $t=0.5$).
A deeper analysis suggests that this over-estimation problem arise when a large change in the false positive leads only to a small change in false positive rate since the dataset is dominated by the nagative class [[4]](#ref:davis)].

# Precision-Recall curve

When the main objective is to evaluate a model on the positive class, then PR-curves are more informative as they only focus on the positive class.

# Refereces

<ol>
    <li id="ref:provost"> Provost, F., Fawcett, T. & Kohavi, R. The case against accuracy estimation for comparing induction algorithms. Int. Conf. Mach. Learn. 445 (1998)
    </li>
    <li id="ref:fernandez"> Fernández, Alberto, et al. Learning from imbalanced data sets. Vol. 10. Berlin: Springer, 2018. </li>
    <li id="ref:fawcett"> Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/10.1016/j.patrec.2005.10.010 </li>
    <li id="ref:davis"> Davis, J., & Goadrich, M. (2006). The relationship between Precision-Recall and ROC curves. ICML 2006 - Proceedings of the 23rd International Conference on Machine Learning, 2006, 233–240. </li>
</ol>

https://medium.com/dataman-in-ai/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8
https://towardsdatascience.com/read-this-before-using-roc-auc-as-a-metric-c84c2d5af621
https://towardsdatascience.com/imbalanced-data-stop-using-roc-auc-and-use-auprc-instead-46af4910a494
https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/
https://stats.stackexchange.com/questions/262616/roc-vs-precision-recall-curves-on-imbalanced-dataset
https://stats.stackexchange.com/questions/262616/roc-vs-precision-recall-curves-on-imbalanced-dataset
https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves
https://www.kaggle.com/general/7517