---
layout: post
mathjax: true
title: "Reccomandation Metrics"
author: "Sandro Cavallari"
tag: "Deep Learning"
---

Reccomandation systems are one of the most well known application for machine learning as it is usually modelled as a binary classification task, where the positive class ($y^+$) represents clicks or conversions, and the negative class ($y^-$) represents examples without interaction.
In the most common case binary classifiers are implemented as single output logistic regressors.
As such they do not directly provide a class lable, but rather the predicted probability of the postive class $$\rmP( y = y^+)$$.
The final label is defined comparing the predicted probability to a threshold $t$ such that:

$$
\hat{y} = \begin{cases}
1 & \text{if} ~ \rmP( y = y^+) \geq t \\
0 & \text{o.w.}
\end{cases}
$$

Common sense suggests that a reasonable threshold could be $t=0.5$.
However, this is hardly the case in the real-world as datasets are hardly perfectly ballanced, models are not always perfectly calibrated and different applications have different [sensitivity vs specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) tradeoff.
To overcome this problem we need metrics able to identify the best performing model indipendently of the threshold.

<div>
<table style="border:none; background:transparent; text-align:center;" align="center">
    <tbody>
    <tr>
        <td style="border:none;" rowspan="2"></td>
        <td style="border:none;"></td>
        <td style="background:#bbeeee;" colspan="2"><b>Predicted condition</b></td>
    </tr>
    <tr>
        <td style="background:#eeeeee;">Total population <br> $ = P + N $ </td>
        <td style="background:#ccffff;"><b> Positive</b> <br> $PP$</td>
        <td style="background:#aadddd;"><b> Negative</b> <br> $PN$</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2" class="nowrap unsortable" style="line-height:99%;vertical-align:middle;padding:.4em .4em .2em;background-position:50% .4em !important;min-width:0.875em;max-width:0.875em;width:0.875em;overflow:hidden;background:#eeeebb;"><div style="-webkit-writing-mode: vertical-rl; -o-writing-mode: vertical-rl; -ms-writing-mode: tb-rl;writing-mode: tb-rl; writing-mode: vertical-rl; layout-flow: vertical-ideographic;display: inline-block; -ms-transform: rotate(180deg); -webkit-transform: rotate(180deg); transform: rotate(180deg);;-ms-transform: none ;padding-left:1px;text-align:center;"><b>Actual condition</b></div>
        </td>
        <td style="background:#ffffcc;"><b>Positive <br> $P$</b></td>
        <td style="background:#ccffcc;"><b>True&nbsp;positive <br> <span style="font-size:85%;">hit</span></b> <br> $TP$ </td>
        <td style="background:#ffdddd;"><b>False&nbsp;negative <br> <span style="font-size:85%;">miss</span></b> <br> $FN$ </td>
        <td style="background:#eeffee;"><b>True Positive Rate <br> <span style="font-size:85%;"> recall&nbsp;/&nbsp;sensitivity</span></b> <br> $TPR = \frac{TP}{P} = 1 - FNR$
        </td>
        <td style="background:#ffeeee;"><b>False&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;">miss&nbsp;rate</span></b> <br> $FNR = \frac{FN}{P} = 1 - TPR$
        </td>
    </tr>
    <tr>
        <td style="background:#ddddaa;"><b>Negative</b> <br> $N$ </td>
        <td style="background:#ffcccc;"><b>False&nbsp;positive <br> <span style="font-size:85%;">type&nbsp;I&nbsp;error / false&nbsp;alarm</span> </b> <br> $FP$
        </td>
        <td style="background:#bbeebb;"><b>True&nbsp;negative <br> <span style="font-size:85%;">correct&nbsp;rejection</span></b> <br> $TN$
        </td>
        <td style="background:#eedddd;"><b>False&nbsp;positive&nbsp;rate <br> <span style="font-size:85%;">probability&nbsp;of&nbsp;false&nbsp;alarm</span> </b> <br> $ FPR = \frac{FP}{N} = 1 − TNR $
        </td>
        <td style="background:#ddeedd;"><b>True&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;"> specificity / selectivity </span></b> <br> $ TNR = \frac{TN}{N} = 1 − FPR $
        </td>
    </tr>
    </tbody>
</table>
<p style="font-size:small;">Table 1: Confusion matrix, credito to <a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="blank">wikipedia</a></p>
</div>


# ROC-AUC

The Receiver Operating Characteristic — Area Under the Curve (**ROC-AUC**) is a metric initially proposed by [Provost et al.](#ref:provost) aiming at comparing classifiers performance.
The rationals behind ROC-AUC are:
  1. missclassification costs are not equals;
  2. class distribution for the target environment is not known in advance.





# Refereces

<ol>
    <li id="ref:provost"> Provost, F., Fawcett, T. & Kohavi, R. The case against accuracy estimation for comparing induction algorithms. Int. Conf. Mach. Learn. 445 (1998) </li>
</ol>

https://medium.com/dataman-in-ai/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8
https://towardsdatascience.com/read-this-before-using-roc-auc-as-a-metric-c84c2d5af621
https://towardsdatascience.com/imbalanced-data-stop-using-roc-auc-and-use-auprc-instead-46af4910a494
https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/
https://stats.stackexchange.com/questions/262616/roc-vs-precision-recall-curves-on-imbalanced-dataset
