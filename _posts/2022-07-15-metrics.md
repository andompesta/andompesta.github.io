---
layout: post
mathjax: true
title: "Reccomandation Metrics"
author: "Sandro Cavallari"
tag: "Deep Learning"
---

Reccomandation systems are one of the most well known application for machine learning as it is usually modelled as a binary classification task, where the positive class ($y^+$) represents clicks or conversions, and the negative class ($y^-$) represents examples without interaction.
In the most common case binary classifiers are implemented as single output logistic regressors.
As such they do not directly provide a class lable, but rather the predicted probability of the postive class $$\rmP( y = y^+)$$.
The final label is defined comparing the predicted probability to a threshold $t$ such that:

$$
\hat{y} = \begin{cases}
1 & \text{if} ~ \rmP( y = y^+) \geq t \\
0 & \text{o.w.}
\end{cases}
$$

Common sense suggests that a reasonable threshold could be $t=0.5$.
However, this is hardly the case in the real-world as datasets are hardly perfectly ballanced, models are not always perfectly calibrated and different applications have different [sensitivity vs specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) tradeoff.
To overcome this problem we need metrics able to identify the best performing model indipendently of the threshold.

<div style="text-align:center;">
<table style="border:none; background:transparent; text-align:center;" align="center">
    <tbody>
    <tr>
        <td style="border:none;" rowspan="2"></td>
        <td style="border:none;"></td>
        <td style="background:#bbeeee;" colspan="2"><b>Predicted condition</b></td>
    </tr>
    <tr>
        <td style="background:#eeeeee;">Total population <br> $ = P + N $ </td>
        <td style="background:#ccffff;"><b> Positive</b> <br> $PP$</td>
        <td style="background:#aadddd;"><b> Negative</b> <br> $PN$</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td rowspan="2" class="nowrap unsortable" style="line-height:99%;vertical-align:middle;padding:.4em .4em .2em;background-position:50% .4em !important;min-width:0.875em;max-width:0.875em;width:0.875em;overflow:hidden;background:#eeeebb;"><div style="-webkit-writing-mode: vertical-rl; -o-writing-mode: vertical-rl; -ms-writing-mode: tb-rl;writing-mode: tb-rl; writing-mode: vertical-rl; layout-flow: vertical-ideographic;display: inline-block; -ms-transform: rotate(180deg); -webkit-transform: rotate(180deg); transform: rotate(180deg);;-ms-transform: none ;padding-left:1px;text-align:center;"><b>Actual condition</b></div>
        </td>
        <td style="background:#ffffcc;"><b>Positive <br> $P$</b></td>
        <td style="background:#ccffcc;"><b>True&nbsp;positive <br> <span style="font-size:85%;">hit</span></b> <br> $TP$ </td>
        <td style="background:#ffdddd;"><b>False&nbsp;negative <br> <span style="font-size:85%;">miss</span></b> <br> $FN$ </td>
        <td style="background:#eeffee;"><b>True Positive Rate <br> <span style="font-size:85%;"> recall&nbsp;/&nbsp;sensitivity</span></b> <br> $TPR = \frac{TP}{P} = 1 - FNR$
        </td>
        <td style="background:#ffeeee;"><b>False&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;">miss&nbsp;rate</span></b> <br> $FNR = \frac{FN}{P} = 1 - TPR$
        </td>
    </tr>
    <tr>
        <td style="background:#ddddaa;"><b>Negative</b> <br> $N$ </td>
        <td style="background:#ffcccc;"><b>False&nbsp;positive <br> <span style="font-size:85%;">type&nbsp;I&nbsp;error / false&nbsp;alarm</span> </b> <br> $FP$
        </td>
        <td style="background:#bbeebb;"><b>True&nbsp;negative <br> <span style="font-size:85%;">correct&nbsp;rejection</span></b> <br> $TN$
        </td>
        <td style="background:#eedddd;"><b>False&nbsp;positive&nbsp;rate <br> <span style="font-size:85%;">probability&nbsp;of&nbsp;false&nbsp;alarm</span> </b> <br> $ FPR = \frac{FP}{N} = 1 − TNR $
        </td>
        <td style="background:#ddeedd;"><b>True&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;"> specificity / selectivity </span></b> <br> $ TNR = \frac{TN}{N} = 1 − FPR $
        </td>
    </tr>
    </tbody>
</table>
<p style="font-size:small;">Table 1: definition of all the metric reported and build on to of a Confusion matrix, credito to <a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="blank">wikipedia</a></p>
</div>


# ROC-AUC

The Receiver Operating Characteristic (ROC) is an analysis tool initially proposed by [Provost et al.](#ref:provost) aiming at comparing classifiers performance.
The ROC curve is build as the interpolation between the **False Positive Rate** (FPR), on the x-axe, and the **True Positive Rate** (TPR), on the y-axe, computed at differnet thresholds level.
Intuitively, the TPR represents how well a classifier can identify all the positive examples; whereas the FPR indicate how likely an error will occurs in the negative class (for formal definitions see the confusion matrix definition in Tab. 1).
Note that a perfect model will be represented by the point $(0, 1)$, however, trade-off exists as decreasing the threshold $t$ will improve the TPR at the expences of the FPR.


<div style="text-align:center;">
    <img src="{{site.baseurl}}/assets/img/metrics/roc-auc.png" style="max-width: 70%">
    <p style="font-size:small;">Figure 1: example of an ROC-AUC curve</p>
</div>

In order to easily compare ROC curves generated by multiple models, the area under such generated curves is used as a single scalar value  representative of the classifier performances.
Thus, ROC-AUC is defineas as:

$$
\text{ROC-AUC} = \int_0^1 ROC(t) \delta t.
$$

Note that ROC-AUC exibit the following properties:
  1. It is considered to be robust to class imbalance as it is mainly a "recall" metric for the positive class (which is usually the less common class)
  2. It can be interpreted as "the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one" [[2]](#ref:fernandez).
  3. It cares about the ranking obtained from the predictions, but does not consider the actual predicted value.



# Refereces

<ol>
    <li id="ref:provost"> Provost, F., Fawcett, T. & Kohavi, R. The case against accuracy estimation for comparing induction algorithms. Int. Conf. Mach. Learn. 445 (1998)
    </li>
    <li id="ref:fernandez"> Fernández, Alberto, et al. Learning from imbalanced data sets. Vol. 10. Berlin: Springer, 2018. </li>
</ol>

https://medium.com/dataman-in-ai/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8
https://towardsdatascience.com/read-this-before-using-roc-auc-as-a-metric-c84c2d5af621
https://towardsdatascience.com/imbalanced-data-stop-using-roc-auc-and-use-auprc-instead-46af4910a494
https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/
https://stats.stackexchange.com/questions/262616/roc-vs-precision-recall-curves-on-imbalanced-dataset
