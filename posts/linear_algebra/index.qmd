---
categories:
- Linear Algebra

author: Sandro Cavallari
date: 2020-12-23
title:  "Basic Principles of Linear Algebra"
# number-sections: true
bibliography: references.bib
csl: diabetologia.csl
comments_id: 2
format:
  html:
    toc: true
---

Linear algebra is the branch of math and statistics that is devoted to the study of matrices and vectors. As such, it is broadly used to model real-world problems in phisitcs and machine learning. Such post is a collections of my notes obtained from the 3Blue1Brown series on linear-algebra [@youtubeEssenceLinear] and Murphy's new book [@murphy2022probabilistic].

# Basic Matrix Operations

1. **transpose**: given a matrix $A \in \mathbb{R}^{m \times n}$, its transpose $A^T$ is obtained ''flipping'' the rows and colums

$$
A = \left[\begin{array}{cccc}
  a_{11} & a_{12} & ... & a_{1n}\\
  a_{21} & a_{22} & ... & a_{2n}\\
  \vdots & \vdots & \vdots & \vdots \\
  a_{m1} & a_{m2} & ... & a_{mn}\\
\end{array}\right]
\Rightarrow
A^T = \left[\begin{array}{cccc}
  a_{11} & a_{21} & ... & a_{m1}\\
  a_{12} & a_{22} & ... & a_{m2}\\
  \vdots & \vdots & \vdots & \vdots \\
  a_{1n} & a_{2n} & ... & a_{mn}\\
\end{array}\right].
$$

The most important properties are:

* ${(A^T)}^T = A$
* $(A \cdot B)^T = B^T \cdot A^T$
* $(A + B)^T = A^T + B^T$


2. **matrix multiplication**: while the summation of 2 matrixes is done element-wise. Matrix multiplication is done row-by-colum and requires matrixes of specific sizes. Given $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times p}$ it is possible to define $C = A \cdot B \in \mathbb{R}^{m \times p}$ s.t. $c_{i,j} = \sum_{k=1}^{n} a_{ik} b_{kj}$. In other words, $C$ is a linear combination of the row of $A$ and the colum of $B$.

$$
C = \left[\begin{array}{ccc}
  - & \mathbf{a}_{1:} & -\\
  - & \mathbf{a}_{2:} & -\\
     & \vdots &   \\
  - & \mathbf{a}_{m:} & -\\
\end{array}\right]
\left[\begin{array}{cccc}
  | & | & | & |\\
  \mathbf{b}_{:1} & \mathbf{b}_{:2} & \dots & \mathbf{b}_{:p}\\
  | & | & | & |\\
\end{array}\right]
= 
\left[\begin{array}{cccc}
  \mathbf{a}_{1:}\mathbf{b}_{:1} & \mathbf{a}_{1:}\mathbf{b}_{:2} & \dots & \mathbf{a}_{1:}\mathbf{b}_{:p}\\
  \mathbf{a}_{2:}\mathbf{b}_{:1} & \mathbf{a}_{2:}\mathbf{b}_{:2} & \dots & \mathbf{a}_{2:}\mathbf{b}_{:p}\\
     & \vdots & \vdots &   \\
  \mathbf{a}_{m:}\mathbf{b}_{:1} & \mathbf{a}_{m:}\mathbf{b}_{:2} & \dots & \mathbf{a}_{m:}\mathbf{b}_{:p}\\
\end{array}\right].
$$

The most important properties are:

* $(A \cdot B) \cdot C = A (B \cdot C)$
* $A \cdot (B + C) = (A \cdot B) + (A \cdot C)$
* $A \cdot B \neq B \cdot A $


3. **matrix inverse**: As for real numbers,  the inverso of a matrix $A$ is denoted as $A^{-1}$ and is defined as the matrix such that: $A A^{-1} = I$. Besites being easy to define computing the inverse of a matrix is an expencive operations. Moreover, $A^{-1}$ exists if and only if $det(A) \neq 0$.

The most important properties are:

* $(A^{-1})^{-1} = A$
* $(A \cdot B)^{-1} = B^{-1} \cdot A^{-1}$
* $(A^{-1})^{T} = (A^{T})^{-1} = A^{-T}$

# Basis Vectors

In linear algebra, a vector basis $B$ of a vector space $V$ is a set of vectors $\{\mathbf{b}_1, ..., \mathbf{b}_n\}$ that are linearly independent and allow to reconstruct every vector $\mathbf{v_i} \in V$ as a linear combination of $B$:

$$
\begin{align}
	\mathbf{v_i} & = a_1 \mathbf{b_1} + ... + a_n \mathbf{b_n} 
\end{align}
$$ {#eq-basis_vector}


For example, the vectors $\mathbf{i} = \left[\begin{array}{c}
  1\\
  0
\end{array}
\right]$ and $\mathbf{j} = \left[
\begin{array}{c}
  0\\
  1
\end{array}
\right]$ are the most common base vectors for the vectors space $\mathbb{R}^2$. Thus, it is possible to represent a vector $\mathbf{x} = \left[\begin{array}{c}
  3\\
  2
\end{array}\right]$ as $3 \mathbf{i} + 2 \mathbf{j}$.


::: {#fig-base-vector}
![](img/linear_algebra/basic_vectors.png){#fig-base-vectors}

Vector representation by the base vector $\mathbf{i}$ and $\mathbf{j}$.
:::


The ability to represent any vector in $V$ as a linear combination of the basis vectors is a powerful concept.
However, $\mathbf{i}$ and $\mathbf{j}$ are not the only possible basis vectors of $\mathbb{R}^2$.
For example, another possible basis could be formed by $\mathbf{v} = \left[\begin{array}{c}
  1\\
  2
\end{array}\right]$ and  $\mathbf{w} = \left[\begin{array}{c}
  3\\
  -1
\end{array}\right]$. However, the representation of $\mathbf{x}$ w.r.t. $\mathbf{v}$ and $\mathbf{w}$ would be different than the one w.r.t. $\mathbf{i}$ and $\mathbf{j}$.


# Span

The **Span** is defined as the set of all possible vectors that we can create given a basis set. Note that the number of basis vectors defines the dimension of our vector space.

# Linear Transformations

A **linear transformation** is equivalent to a function over vectors. That is, a linear transformation "move" an input vector to an output vector. While general transformations have complex features, linear transformations have some well-defined properties:

1. they maintain the origin of the vector space invariant
2. they map equally spaced lines to equally spaced lines (or points)

$$
\begin{align}
	L(a_1 \mathbf{i} + a_2 \mathbf{j}) & = a_1L(\mathbf{i}) + a_2L(\mathbf{j})
\end{align}
$${#eq-linear-transform}

::: {#fig-base-vector}
![](img/linear_algebra/transformations.png){#fig-base-vectors}

Examples of the most commont linear transformations. (Image taken from <a href="https://mathigon.org/course/linear-algebra/linear-transformations"> Samuel S. Watson</a>.
:::


Thanks to their properties, it is possible to linearly transform any vector by means to its basis. In other words, given a vector $\mathbf{x} = \left[\begin{array}{c} -1\\ 2 \end{array}\right]$ w.r.t. $\mathbf{i}$ and $\mathbf{j}$ and any lineart transformation $L$.
It is possible to represent $L(\mathbf{x})$ as a function of $L(\mathbf{i})$ and $L(\mathbf{j})$ (formally $L(\mathbf{x}) = -1 L(\mathbf{i}) + 2 L(\mathbf{j})$).

For example, assume $L = \left[\begin{array}{cc} 1 & 3\\ -2 & 0 \end{array}\right]$, then:

$$
\begin{align*}
L(\mathbf{i}) &= \left[\begin{array}{cc} 1 & 3\\ -2 & 0 \end{array}\right]  \left[\begin{array}{c} 1\\ 0 \end{array}\right] = \left[\begin{array}{c} 1\\ -2 \end{array}\right]\\
L(\mathbf{j}) &= \left[\begin{array}{cc} 1 & 3\\ -2 & 0 \end{array}\right]  \left[\begin{array}{c} 0\\ 1 \end{array}\right] = \left[\begin{array}{c} 3\\ 0 \end{array}\right] \\
L(\mathbf{x}) &= -1 L(\mathbf{i}) + 2 L(\mathbf{j}) \\
	&= -1 \left[\begin{array}{c} 1\\ -2 \end{array}\right] + 2 \left[\begin{array}{c} 3\\ 0 \end{array}\right] \\
	&= \left[\begin{array}{c} 5\\ 2 \end{array}\right]
\end{align*}
$$


::: {#fig-base-vector}
![](img/linear_algebra/linear-transform.png){#fig-linear-transform}

Visualization of the linear transformation appled to vector $\mathbf{x}$.
:::


Finally, as a linear transformation is represented by a matrix, it is possible to define the **composition of two or more linear transformations** as he left-to-right product of the transformation matrix:

$$
\begin{align}
L_2(L_1( \mathbf{x} )) =  L_2L_1(\mathbf{x})
\end{align}
$$

For example, if $L_1 = \left[\begin{array}{cc} 1 & -2\\ 1 & 0 \end{array}\right]$, $L_2 = \left[\begin{array}{cc} 0 & 2\\ 1 & 0 \end{array}\right]$ and $\mathbf{x} = \left[\begin{array}{c} x\\ y \end{array}\right]$. Then:

$$
\begin{align*}
L_2(L_1( \mathbf{x} )) &= \left[\begin{array}{cc} 0 & 2\\ 1 & 0 \end{array}\right] \Big ( \left[\begin{array}{cc} 1 & -2\\ 1 & 0 \end{array}\right] \left[\begin{array}{c} x\\ y \end{array}\right] \Big)
\\
L_2 L_1( \mathbf{x} ) &= \Big ( \left[\begin{array}{cc} 0 & 2\\ 1 & 0 \end{array}\right] \left[\begin{array}{cc} 1 & -2\\ 1 & 0 \end{array}\right] \Big) \left[\begin{array}{c} x\\ y \end{array}\right] \\
&= \left[\begin{array}{cc} 2 & 0\\ 1 & -2 \end{array}\right] \left[\begin{array}{c} x\\ y \end{array}\right] 
\end{align*} 
$$

Note that as matrix multiplication is equal to applying different linear transformations, the multiplication order does matter.

# Determinant
As linear transformations alter the original vector space, it is important to evaluate by how much the original space is expanded or contracted by a given linear transformation $L$. The **determinant** define how much the original unit surface is changed by $L$.

The determinant has some interesting properties:

1. A liner transantformation with 0 determinant ($det(L) = 0$) means that squash all the vectrs on a single line/plane. Moreover, it also means that $L$ has linearly dependents columns.
2. The determinant can be negative if it change orientation of the space.
3. Determinant is associative: $det(L) \cdot det(M) = det(L \cdot M)$.

<div>
<img src="{{site.baseurl}}/assets/img/linear_algebra/determinant.png" style="max-width: 85%">
<p style="font-size:small;">Figure 4: Visualization of the determinant for a initial vector space defined by $\mathbf{i}$ and $\mathbf{j}$ and the vector space obtained after applying the transformation $L$.)</p>
</div>


# System of Linear Equations
It is convininet to use inear algebra to represent a system of linear equations, e.g.


<table align="center">
<tr>
  <td>
    $\begin{cases} 
    2x + 5y + 3z = -3 \\ 
    4x + 8z = 0 \\
    x + 3y = 2
    \end{cases}$
  </td>
  <td>
    $
    \Rightarrow
    $
  </td>
  <td style="width: 5px">
    $
    \left[\begin{array}{ccc} 
    2 & 5 & 3\\
    4 & 0 & -2\\
    1 & 3 & 0
    \end{array}
    \right]
    $
  </td>
  <td style="width: 5px">
    $
    \left[\begin{array}{c} 
    x\\
    y\\
    z
    \end{array}
    \right]
    $
  </td>
  <td style="width: 3px">
    = 
  </td>
  <td style="width: 5px">
    $
    \left[\begin{array}{c} 
    -3\\
    0\\
    2
    \end{array}
    \right]
    $
  </td>
</tr>
<tr>
    <td></td>
    <td></td>
    <td>$A$</td>
    <td>$\rvx$</td>
    <td> = </td>
    <td>$\mathbf{b}$</td>
</tr>
</table>

Thus, any system of linear equations can be expressed as:

$$
\begin{equation}
A \mathbf{x} = \mathbf{b}
\end{equation}
$$

where $A \in \mathbb{R}^{m \times n}$ is a known linear transformation(a matrix), $\mathbf{b} \in \mathbb{R}^{m \times 1}$ is a known vector in the space of $A$, and $\rvx \in \mathbb{R}^{n \times 1}$ is an unkown vector that after the transformation $A$ lies over $\mathbf{b}$.


Note that the existence of such unkown vector is tightly related to the determinant of $A$:
* if $det(A) = 0$, in general, there is no such $\rvx$
* if $det(A) \neq 0$, in general, there is one-and-only-one $\rvx$ that satisfy $ A \rvx = \mathbf{b} $, namely $A^{-1}$.

Mathematically, the solution to $A \rvx = \mathbf{b}$ is $\rvx = A^{-1} \mathbf{b}$. However, computing $A^{-1}$ is a complex operation and is subject to numerical instabilities ([Don’t invert that matrix](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/)). Thus, mathematicians have develop multiple solvers for that same problam that does not require to compute the matrix invers and they leverage some specific property of matrix $A$.


# Change of Basis

Given a vector $\rvx = \left[\begin{array}{c} 3\\ 2\end{array}\right]$ imagine this vector represented in terms of the unit vectors $\rvi = \left[\begin{array}{c} 1\\ 0\end{array}\right]$ and $\rvj = \left[\begin{array}{c} 0\\ 1\end{array}\right]$, and, scale them by 3 and 2, i.e.


$\rvx = \left[\begin{array}{c} 3 \rvi\\ 2 \rvj \end{array}\right]$

However, as shown if Fig. 5, we can also represent $\rvx$ in terms of different basis vectors $\rvu = \left[\begin{array}{c} 2\\ 1\end{array}\right]$ and $\rvv = \left[\begin{array}{c} -1\\ 1\end{array}\right]$. That is, $\rvx$ can be represented as the linear combination of $\rvu$ $\rvj$:

$\rvx = \left[\begin{array}{c} \frac{5}{3} \rvu\\ \frac{1}{3} \rvv \end{array}\right]$

In other words, it is possible to represent $\rvx$ in two different languages: one according to basis $\rvi$ $\rvj$ the other according to basis $\rvu \rvv$.

<div>
<img src="{{site.baseurl}}/assets/img/linear_algebra/base_change.png" style="max-width: 85%">
<p style="font-size:small;">Figure 5: Visualization of the same vector $\rvx$ represented according to two different basis vectors.)</p>
</div>


As overstated, we can express $\rvu$ and $\rvv$ in terms of basis vectors $\rvi$ and $\rvj$ as:

$\rvu = \left[\begin{array}{c} 2\\ 1\end{array}\right] ~~~~~~ \rvv = \left[\begin{array}{c} -1\\ 1\end{array}\right]$

or in terms of $\rvu$ $\rvv$ it-self:

$\rvu = \left[\begin{array}{c} 1\\ 0\end{array}\right] ~~~~~~ \rvv = \left[\begin{array}{c} 0\\ 1\end{array}\right]$


Yet, the linear transformation $\rmU = [\rvu, \rvv]$ (composed by the collum vectors $\rvu$ and $\rvv$) allow to convert any vector written in terms of $\rvu$ $\rvv$ to its equivalent vector w.r.t. $\rvi$ and $\rvj$:

$ \left[\begin{array}{cc} 2 & -1\\ 1 & 1\end{array}\right] \cdot \left[\begin{array}{c} \frac{5}{3}\\ \frac{1}{3}\end{array}\right] = \left[\begin{array}{c} 3\\ 2 \end{array}\right]$

Similarly, we can use $\rmU^{-1}$ to convert any vector written in terms of $\rvi$ $\rvj$ to it equivalent representeation in $\rvu$ $\rvv$:

$
\left[\begin{array}{cc} \frac{1}{3} & \frac{1}{3}\\ -\frac{1}{3} & \frac{2}{3}\end{array}\right] \cdot \left[\begin{array}{c} 3\\ 2\end{array}\right] = \left[\begin{array}{c} \frac{5}{3}\\ \frac{1}{3}\end{array}\right]
$


More generaly, any transformation $A$ expressed in terms of the basis $\rvi$ and $\rvj$ can be applyed to any vectror $\rvx$ defined in temrs of basis $\rvu$ and $\rvv$ applying the change-of-basis equation:

$
\begin{equation}
\label{eq:cob}
[\rmU^{-1} A \rmU] \rvx
\end{equation}
$

where $\rmU^{-1} A \rmU$ express a sort of mathematical empathy between different reference systems; i.e., it converts a tranformation $A$ to a different reference systems.

# Eigenvectors and Eigenvalues

It is often convinient to study linear transformations, not on their matrix formulation, but ratehr on their base component.
Among the most common decomposition methods, **eigenvectors** and **eigenvalues** are the most common matrix decomposition thecnique.


Given a linear transformation $L = \left[\begin{array}{cc} 3 & 1\\ 0 & 2 \end{array}\right]$, most of the vectors $\mathbf{v}_i$ are rotated by $L$ away from their original span.
Instead some special vectors $\mathbf{e}_i$ are only streched or squished by $L$, but they remain on the original span. Moreover, every vector on the span of $\mathbf{e}_i$ is also only scaled by $L$.


<div>
<img src="{{site.baseurl}}/assets/img/linear_algebra/eigen_values.png" style="max-width: 85%">
<p style="font-size:small;">Figure 6: Visualization of one of the eigenvalue of $L$. Note that, $\mathbf{e}_1$ remain on its own span, while a random vector $\mathbf{v}$ is moved away from its original span.)</p>
</div>


Base on the intuition shown in Fig. 6 and on the "move from the span" consepts, we can formally define the eigenvalues of a squared matrix $A \in \mathbb{R}^{n \times n}$ as the non-zero vector $\mathbf{e}_i$:


$
\begin{align}
A \cdot \mathbf{e}_i = \lambda \mathbf{e}_i
\end{align}
$

where:
* $\lambda$ is known as the eigenvalue of the eigenvector $\mathbf{e}$.
* $\lambda \neq 0 $.
* if $\mathbf{e}_i$ is an eigenvectors of $A$, then any rescaled vector $c ~ \mathbf{e}_i $ for $ c \in \mathbb{R}, c \neq 0$ is also an eigenvectors of $A$. Thus, usually only the unit eigenvectors are considered.

There is an interesting connection between eigenvectors are determinant. According to the formal definition of eigenvectors, we are tring to map a matrix to a vector. Thus, we are tring to map a volume/surface to a single line/point; which is possible only if the determinant of the matrix is 0:

$
\begin{align}
\label{eq:eigenvectors}
A \cdot \mathbf{e}_i &= \lambda \mathbf{e}_i \\
A \cdot \mathbf{e}_i &= (\rmI \lambda) \mathbf{e}_i \nonumber \\
(A - \lambda \rmI) \mathbf{e}_i &= 0 \nonumber \\
& \Rightarrow det(A - \lambda \rmI) = 0 \nonumber
\end{align}
$


The most important properties are:
* the trance of a matrix is equal to the some of its eigenvalues: $tr(A) = \sum_{i=0}^{n-1} \lambda_i$
* the determinanto of $A$ is equal to the producto of its eigenvalues: $ det(A) = \prod_{i=0}^{n-1} \lambda_i$



# Matrix Decomposition

Similarly, to how it is conveninet to express $15$ as product of its factors $5 \cdot 3$; sometimes it is convenient to express a matrix $A$ as product of other matrixes.
There are multiple method to decpompose a matrix, but they are mostly used to eficiently solve systems of linear equation.

## Eigendecomposition 

Given a squared matrix $A \in \mathbb{R}^{n \times n}$, it is possible to rewrite Eq. \ref{eq:eigenvectors} in matrix form as:

$
\begin{equation}
\label{eq:eigenvectors_matrix}
A \rmU = \rmU \mathbf{\Lambda}
\end{equation}
$

Moreover, according to Eq. \ref{eq:cob}, using the eigenvectors of $A$ as new basis of $A$ will generate a diagonal matrix of eigenvalues:

$
\begin{equation}
\rmU^{-1} A \rmU = \mathbf{\Lambda}
\end{equation}
$

where $\rmU \in \mathbb{R}^{n \times n} = \left[\begin{array}{ccc}
  | & | & | \\
  \rve_1 & \dots & \rve_{n}\\
  | & | & | \\
\end{array}\right]$ is the matrix formed by the eigenvectors of $A$ and $\mathbf{\Lambda} \in \mathbb{R}^{n \times n} = \left[\begin{array}{ccc}
  \lambda_1 &  &  \\
   & \ddots & \\
   & & \lambda_n \\
\end{array}\right]$ is the diagonal matrix formed by the eigenvalues assogiated to the eigenvectors of $A$.


This process of expressing $A$ in terms of its eigenvalue and eigenvectors is know as **diagonalization**.
If the eigenvalues of $A$ are linearly indipendent, then the matrix $\rmU$ is invertible, thus, it is possible to **decompose** $A$ as:

$
\begin{equation}
\label{eq:eigendecomposition}
A = \rmU \mathbf{\Lambda} \rmU^{-1} .
\end{equation}
$

Moreover, if $A$ is real valued and symmetric then it can be shown that $\rmU$ is orthonormal, i.e., $\rvu^T_i \rvu_j = 0$ if $i \neq j$ and $\rvu^T_i \rvu_i = 1$ (or $\rmU^T\rmU = \rmU \rmU^T = \rmI$). Thus, we can futher symplify Eq. \ref{eq:eigendecomposition} $A$ as:

$
\begin{equation}
A = \rmU \mathbf{\Lambda} \rmU^T .
\end{equation}
$

As a final note, it is possible to leverage such eigendecomposition to easily compute the inverse of a matrix $A$. Since $\rmU^T = \rmU^{-1}$, we have:

$
\begin{equation}
A^{-1} = \rmU \mathbf{\Lambda}^{-1} \rmU^T .
\end{equation}
$


### Lagrangian Methods for Constrained Optimization
While eigen decomposition is commonly applied to solve systems of liear equations.
It is also a powerful method for optimization subject to linear constrains (constrained optimization).
That is, it can be used to solve quadratic constrained problems of the form:

$
\min_{\rvx} \rvx^T \rmH \rvx + d, ~~\text{subject to} ~~ \rvx^T \rvx - 1 = 0
$

where $\rmH \in \mathbb{R}^{n \times n}$ is symmetric.
Such problems are a specific instanche of the **Lagrangian method**, in which an augmented objective is created to ensure the constrain satisfability:

$
L(\rvx, \lambda) = \max_{\lambda} \min_{\rvx} \rvx^T \rmH \rvx + d - \lambda (\rvx^T \rvx - 1)
$

The optimal $\rvx^*$ that solve the problem, need to satisfy the zero-gradient condition:

$
\begin{align*}
\frac{\partial L(\rvx, \lambda)} {\partial \rvx} & = 0 \\
 & = \frac{ \partial } {\partial \rvx} \rvx^T \rmH \rvx   +  \frac{\partial}{\partial \rvx} d - \frac{\partial}{\partial \rvx} \lambda (\rvx^T \rvx - 1)  \\
 & = \rvx^T (\rmH + \rmH^T) + 0 - 2 \lambda \rvx^T  && { \small \rmH = \rmH^T \text{ since is symmetric.} }\\
 & = 2 \rvx^T \rmH - 2 \lambda \rvx^T \\ 
\frac{\partial L(\rvx, \lambda)} {\partial \lambda} & = 0  \\
 & =  \frac{ \partial }{ \partial \lambda } \rvx^T \rmH \rvx + \frac{ \partial }{ \partial \lambda } d - \frac{ \partial }{ \partial \lambda } \lambda (\rvx^T \rvx - 1) \\
 & = 0 + 0 - \rvx^T \rvx + 1 \\
 & = \rvx^T \rvx - 1
\end{align*}
$

which is equivalent to the eigenvector equation (Eq. \ref{eq:eigenvectors_matrix}) $ \rmH \rvx = \lambda \rvx$.




## Singular Value Decomposition (SVD)

While eigendecomposition require squared matrices, **SVD** allow the factorization of rectangular matrices into **singular vectors** and **singular values**. Given any $A \in \mathbb{R}^{m \times n}$, it is possible to depompose it as:

$
\begin{equation}
A = \rmU \rmS \rmV^T
\end{equation}
$

where $\rmU \in \mathbb{R}^{m \times m}$ is composed by orthonormal columns ($\rmU^T \rmU = \rmI$), $\rmV \in \mathbb{R}^{n \times n}$ is compesed by orthonormals rows and columns ($\rmV^T\rmV = \rmV \rmV^T = \rmI$), and $\rmS \in \mathbb{R}^{m \times n}$ is a diagonal matrix containing the **singular values** $\sigma_i \geq 0$.
$\rmU$ and $\rmV^T$ are respectively known as the **left singular vectors** and **right singular vectors** of $A$ and are obtained as the eigenvectors of $AA^T$ and $A^TA$. Similarly, $\rmS$ is composed by the squared root of the eigenvalues of $AA^T$ and $A^TA$ arranged in descending order.

For example, consider

$
A = 
\left[\begin{array}{cc}
  2 & 4 \\
  1 & 3 \\
  0 & 0 \\
  0 & 0 \\
\end{array}\right]
$

then we know that the columns of $\rmU$ are made by the eigenvalues of $A A^T$:

$
\begin{align*}
A A^T &= \left[\begin{array}{cccc}
  20 & 14 & 0 & 0 \\
  14 & 10 & 0 & 0 \\
  0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 \\
\end{array}\right]\\
\rmU &= \left[\begin{array}{cccc}
  0.82 & -0.58 & 0 & 0 \\
  0.58 & 0.82 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
\end{array}\right]
\end{align*}
$

similarly, the right singular vectors are obtained as eigenvalues of $A^T A$:

$
\begin{align*}
A^T A &= \left[\begin{array}{cc}
  5 & 11 \\
  11 & 25\\
\end{array}\right]\\
\rmV &= \left[\begin{array}{cc}
  0.4 & -0.91 \\
  0.91 & 0.4
\end{array}\right]
\end{align*}
$

instead, $\rmS$ is formed by the squared root of the eivenvectors of $\rmV$ or $\rmU$:

$
\rmS = \left[\begin{array}{cc}
  5.46 & 0 \\
  0 & 0.37 \\
  0 & 0 \\
  0 & 0 
\end{array}\right]
$


# References

::: {#refs}
:::
