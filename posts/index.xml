<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Sandro Cavallari</title>
<link>https://andompesta.github.io/posts/</link>
<atom:link href="https://andompesta.github.io/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website of Sandro Cavallari</description>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Mon, 21 Dec 2020 23:00:00 GMT</lastBuildDate>
<item>
  <title>Gradinet Descent and Backpropagation</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/gradient_descent_and_backprop/</link>
  <description><![CDATA[ 




<p>Most <strong>deep learning</strong> algorithm relay on the idea of <strong>learning</strong> some useful information from the data to solve a specific task. That is, instead of explicitly define every single instruction that a program has to perform, in machine learning, we specify an optimization routine that a program executes over a set of examples to improve its performances. By executing the optimization algorithm, a machine automatically navigates the solution space to find the best “program” that solve the given task starting from a random state: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D">. It is expectable that the initial program obtained based on the random state would not perform well on the chosen task, however, by iterating over the dataset, we can adjust <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D"> until we obtain an optimal solution.</p>
<section id="gradient-descent" class="level1">
<h1>Gradient Descent</h1>
<p>One of the most common learning algorithm is known as Gradient Descent or <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"><strong>Stochastic Gradient Descent</strong></a> (SGD) <span class="citation" data-cites="hanson1988comparing">[1]</span>. The core idea of SGD is to iteratively evaluate the difference between the obtained prediction of the model (<img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctheta%7D">), and, the desired output (<img src="https://latex.codecogs.com/png.latex?y">) utilizing a loss function <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y_%7B%5Cmathbf%7B%5Ctheta%7D%7D,%20y)">. Once the difference is known, it is possible to adjust <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to reduce the difference or prediction error.</p>
<p>Formally, SGD is composed of 3 main steps:</p>
<ol type="1">
<li>evaluate the loss function: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y_%7B%5Cmathbf%7B%5Ctheta%7D%7D,%20y)">,</li>
<li>compute the gradient of the loss function w.r.t. the model parameters: <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Cmathcal%7BL%7D_%7B%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D(y_%7B%5Ctheta%7D,%20y)%7D%7B%5Cpartial%20%5Ctheta%7D">,</li>
<li>update the model parameters (or solution) to decrease the loss function: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D%20=%20%5Cmathbf%7B%5Ctheta%7D%20-%20%5Ceta%20%5Cnabla%20%5Cmathcal%7BL%7D_%7B%5Ctheta%7D">.</li>
</ol>
<p>As it is possible to notice, such a learning algorithm requires a loss function that is continuous and differentiable; otherwise, it is not applicable. However, over the years, many efficient and effective loss functions have been proposed.</p>
</section>
<section id="backpropagation" class="level1">
<h1>Backpropagation</h1>
<p>Computing the analytical gradients for a deep learning algorithm might not be easy, and it is definitely an error-prone procedure. Luckily, over the years mathematicians manage to programmatically compute the derivate of most of the functions with a procedure known as <a href="https://en.wikipedia.org/wiki/Automatic_differentiation"><strong>algorithmic differentiation</strong></a>. The application of algorithmic differentiation to compute the SGD is known as <strong>backpropagation</strong>.</p>
<p>Supposing to have the current function <img src="https://latex.codecogs.com/png.latex?f(x,y,z)%20=%20(x%20+%20y)%20%5Ccdot%20z">. It is possible to simplify it’s computation defining an intermediate function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aq(x,%20y)%20=%20x%20+%20y%20%5CRightarrow%20f(q,%20z)%20=%20q%20%5Ccdot%20z.%0A"></p>
<p>Knowing that:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%201"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201"></li>
</ul>
<p>we can compute <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D"> by <strong>chain rule</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Ccdot%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D.%0A"></p>
<p>This operation can be seen even as a computational graph, where each node represent an operation ; and using backpropagation it is possible to compute the gradient of function <img src="https://latex.codecogs.com/png.latex?f"> w.r.t. its input variable <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">:</p>
<div id="fig-elephants" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<img src="https://andompesta.github.io/posts/gradient_descent_and_backprop/img/backprop.png" id="fig-back-prop" class="img-fluid figure-img">
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The forward and backward pass of the computational graph for the function $ f(x,y,z) = (x + y) z $. (Image taken from Andrej Karpathy slides, CS231n.)
</figcaption>
</figure>
</div>
<p>It has to be noted that, backpropagation is a local and global process. It is local since a gate, during the forward pass, can compute:</p>
<ol type="1">
<li>its output value: <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y%20=%203">,</li>
<li>as well as its local gradient (the gradient of its input w.r.t. its output): <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201">.</li>
</ol>
<p>It is global since, a gate need to know the gradient of its output node in order to evaluate the chain rules: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D">. The gradient of its ouput is known only during the backward pass, thus all the local computations need to be stored in memory; thus require a lot of memory.</p>
<p>The backward pass start by computing: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20=%201">. Then, knowing that <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%201%20%5Ccdot%20-4%20=%20-4">. Similarly, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%203">. Finally, our goal is to goal is to compute: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%20-4%20%5Ccdot%201%20=%20-4%0A"> and, <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%20-4%20%5Ccdot%201%20=%20-4%0A">.</p>
</section>
<section id="weight-decay" class="level1">
<h1>Weight Decay</h1>
<p>To achieve better generaliation performance it is well known that graient updates needs to be regularized so to have sparse or force small weights magnitude. The two most common regularizations for gradiens are L1-regularization or weight decay <span class="citation" data-cites="fastFastaiAdamW">[2]</span> (equivalent to the L2-regularization):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20-%20%5Calpha%20%5Cfrac%7B%5Cpartial%20f(x;%20%5Ctheta_t)%7D%7B%5Cpartial%20%5Ctheta_t%7D%20-%20%5Clambda%20%5Ctheta_t%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Ctheta_t"> stand for weight decay or L2-regularization. However, weight dacay and L2-regularization are equivalent only for SDG, but not for Adam or other adaptive optimizers. Instead of applying the same learning rate to all parameters, Adam apply a different learning rate to each parameters proportional to the update signals they recently recevied (a.k.a proportional to the recent gradients). As Adam uses a different learning rate per each parameters, it means that L2-regularization is not only affected by <img src="https://latex.codecogs.com/png.latex?%5Clambda"> but also from the learning rate and the momentum. Thus, Adam requires a bigger regularizer coefficent to achieve comparable performance as SGD.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-hanson1988comparing" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Hanson S, Pratt L (1988) Comparing biases for minimal network construction with back-propagation. Advances in neural information processing systems 1</div>
</div>
<div id="ref-fastFastaiAdamW" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Gugger S, Howard J Fast.ai - <span>A</span>dam<span>W</span> and <span>S</span>uper-convergence is now the fastest way to train neural nets — fast.ai</div>
</div>
</div>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://andompesta.github.io/posts/gradient_descent_and_backprop/</guid>
  <pubDate>Mon, 21 Dec 2020 23:00:00 GMT</pubDate>
</item>
</channel>
</rss>
