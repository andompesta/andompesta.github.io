<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Sandro Cavallari</title>
<link>https://andompesta.github.io/posts/</link>
<atom:link href="https://andompesta.github.io/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website of Sandro Cavallari</description>
<generator>quarto-1.4.550</generator>
<lastBuildDate>Mon, 15 Jan 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Linear Regression</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/linear_regression/</link>
  <description><![CDATA[ 




<p>Linear regression, as discussed by Montgomery and Runger <span class="citation" data-cites="montgomery2021introduction">[1]</span>, stands out as a widely employed and intuitive machine learning model. Its primary objective is to establish a linear connection between a dependent variable (plotted on the y-axis) and one or more independent variables (plotted on the x-axis). The foundational formula for linear regression is represented as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnewcommand%7B%5Crvepsilon%7D%7B%5Cmathbf%7B%5Cepsilon%7D%7D%0A%5Cnewcommand%7B%5Crvtheta%7D%7B%5Cmathbf%7B%5Ctheta%7D%7D%0A%5Cnewcommand%7B%5Crva%7D%7B%5Cmathbf%7Ba%7D%7D%0A%5Cnewcommand%7B%5Crvb%7D%7B%5Cmathbf%7Bb%7D%7D%0A%5Cnewcommand%7B%5Crvc%7D%7B%5Cmathbf%7Bc%7D%7D%0A%5Cnewcommand%7B%5Crvd%7D%7B%5Cmathbf%7Bd%7D%7D%0A%5Cnewcommand%7B%5Crve%7D%7B%5Cmathbf%7Be%7D%7D%0A%5Cnewcommand%7B%5Crvi%7D%7B%5Cmathbf%7Bi%7D%7D%0A%5Cnewcommand%7B%5Crvj%7D%7B%5Cmathbf%7Bj%7D%7D%0A%5Cnewcommand%7B%5Crvq%7D%7B%5Cmathbf%7Bq%7D%7D%0A%5Cnewcommand%7B%5Crvu%7D%7B%5Cmathbf%7Bu%7D%7D%0A%5Cnewcommand%7B%5Crvv%7D%7B%5Cmathbf%7Bv%7D%7D%0A%5Cnewcommand%7B%5Crvw%7D%7B%5Cmathbf%7Bw%7D%7D%0A%5Cnewcommand%7B%5Crvx%7D%7B%5Cmathbf%7Bx%7D%7D%0A%5Cnewcommand%7B%5Crvy%7D%7B%5Cmathbf%7By%7D%7D%0A%5Cnewcommand%7B%5CrmA%7D%7B%5Cmathbf%7BA%7D%7D%0A%5Cnewcommand%7B%5CrmB%7D%7B%5Cmathbf%7BB%7D%7D%0A%5Cnewcommand%7B%5CrmC%7D%7B%5Cmathbf%7BC%7D%7D%0A%5Cnewcommand%7B%5CrmH%7D%7B%5Cmathbf%7BH%7D%7D%0A%5Cnewcommand%7B%5CrmI%7D%7B%5Cmathbf%7BI%7D%7D%0A%5Cnewcommand%7B%5CrmM%7D%7B%5Cmathbf%7BM%7D%7D%0A%5Cnewcommand%7B%5CrmS%7D%7B%5Cmathbf%7BS%7D%7D%0A%5Cnewcommand%7B%5CrmU%7D%7B%5Cmathbf%7BU%7D%7D%0A%5Cnewcommand%7B%5CrmV%7D%7B%5Cmathbf%7BV%7D%7D%0A%5Cnewcommand%7B%5CrmW%7D%7B%5Cmathbf%7BW%7D%7D%0A%5Cnewcommand%7B%5CrmX%7D%7B%5Cmathbf%7BX%7D%7D%0A%5Cnewcommand%7B%5CrmY%7D%7B%5Cmathbf%7BY%7D%7D%0A%5Cnewcommand%7B%5Creal%7D%7B%5Cmathbb%7BR%7D%7D%0A"></p>
<p><span id="eq-linear-model"><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20w_0%20+%20w_1%20x_1%20+%20...%20+%20w_k%20x_k%20+%20%5Cepsilon%0A%5Ctag%7B1%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?y"> is the dependent variable, often referred to as the response, target, or outcome variable.</li>
<li><img src="https://latex.codecogs.com/png.latex?x_k"> denotes the independent variables, also recognized as predictors or features.</li>
<li><img src="https://latex.codecogs.com/png.latex?w_k"> are the model parameters that define the linear relationship between the independent variables and the target. These coefficients signify the anticipated change in the dependent variable for a one-unit change in the respective independent variable, with all other variables held constant.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> represents the error term, accounting for the variability in <img src="https://latex.codecogs.com/png.latex?y"> that can not be explained by linear model.</li>
</ul>
<p>Assuming that the sole source of error in the model is the difference between the predicted outcome (<img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D">) and the actual value (<img src="https://latex.codecogs.com/png.latex?y">) of the dependent variable, the learning process adjusts the parameters to minimize the error terms:</p>
<p><span id="eq-least-square"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Chat%7By%7D%20&amp;%20=%20w_0%20+%20w_1%20x_1%20+%20...%20+%20w_k%20x_k%20%5C%5C%0A%5Cepsilon%20&amp;=%20y%20-%20%5Chat%7By%7D.%0A%5Cend%7Balign%7D%0A%5Ctag%7B2%7D"></span></p>
<p>While direct minimization of <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is impractical, a common alternative is to optimize the following loss function:</p>
<p><span id="eq-sum-of-squared-residuals"><img src="https://latex.codecogs.com/png.latex?%0A%5Carg%20%5Cmin%20%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D;%20%5Crvw)%20=%20(y%20-%20%5Chat%7By%7D)%5E2.%0A%5Ctag%7B3%7D"></span></p>
<p>It’s important to note that the minimization described in Equation&nbsp;3 entails minimizing <strong>the vertical distance</strong> between <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> and <img src="https://latex.codecogs.com/png.latex?y">, also knonw as residuals. This implies an implicit assumption that the only possible error is related to the model not accurately predicting the actual value, while the measurements of the independent variable are assumed to be <strong>error-free</strong>. This model formulation is commonly known as the least-square model, where Equation&nbsp;3 is the sum-of-squared-residual loss.</p>
<div id="cell-fig-least-square" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb1-5"></span>
<span id="cb1-6">n_datapoints <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb1-7">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-8">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.random.rand(n_datapoints)</span>
<span id="cb1-9">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.random.randn(n_datapoints))</span>
<span id="cb1-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression(fit_intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-11">model.fit(x[:, np.newaxis], y)</span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get prediction</span></span>
<span id="cb1-13">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(x[:, np.newaxis])</span>
<span id="cb1-14"></span>
<span id="cb1-15">residual <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred</span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get prediction for best line fit</span></span>
<span id="cb1-18">xfit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb1-19">y_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(xfit[:, np.newaxis])</span>
<span id="cb1-20"></span>
<span id="cb1-21">fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">10.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb1-22"></span>
<span id="cb1-23">ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb1-24">ax.scatter(</span>
<span id="cb1-25">    x,</span>
<span id="cb1-26">    y,</span>
<span id="cb1-27">    label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Data points"</span>,</span>
<span id="cb1-28">    edgecolors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k"</span>,</span>
<span id="cb1-29">)</span>
<span id="cb1-30"></span>
<span id="cb1-31"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plot data</span></span>
<span id="cb1-32">ax.plot(</span>
<span id="cb1-33">    [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],</span>
<span id="cb1-34">    [y.mean(), y.mean()],</span>
<span id="cb1-35">    color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#ff7f0e"</span>,</span>
<span id="cb1-36">    label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Initial fit"</span>,</span>
<span id="cb1-37">)</span>
<span id="cb1-38"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x)):</span>
<span id="cb1-39">    ax.plot(</span>
<span id="cb1-40">        [x[i], x[i]],</span>
<span id="cb1-41">        [y[i], y.mean()],</span>
<span id="cb1-42">        color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gray"</span>,</span>
<span id="cb1-43">        linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--"</span>,</span>
<span id="cb1-44">    )</span>
<span id="cb1-45">ax.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-46">ax.set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"x"</span>)</span>
<span id="cb1-47">ax.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span>)</span>
<span id="cb1-48">ax.legend()</span>
<span id="cb1-49">ax.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Initial model"</span>)</span>
<span id="cb1-50"></span>
<span id="cb1-51">ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb1-52">ax.scatter(</span>
<span id="cb1-53">    x,</span>
<span id="cb1-54">    y,</span>
<span id="cb1-55">    label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Data points"</span>,</span>
<span id="cb1-56">    edgecolors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k"</span>,</span>
<span id="cb1-57">)</span>
<span id="cb1-58"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plot best line fit</span></span>
<span id="cb1-59">ax.plot(</span>
<span id="cb1-60">    xfit,</span>
<span id="cb1-61">    y_,</span>
<span id="cb1-62">    color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2ca02c"</span>,</span>
<span id="cb1-63">    label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Best fit"</span>,</span>
<span id="cb1-64">)</span>
<span id="cb1-65"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Optionally, plot residuals (errors)</span></span>
<span id="cb1-66"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x)):</span>
<span id="cb1-67">    ax.plot(</span>
<span id="cb1-68">        [x[i], x[i]],</span>
<span id="cb1-69">        [y[i], y_pred[i]],</span>
<span id="cb1-70">        color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gray"</span>,</span>
<span id="cb1-71">        linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"--"</span>,</span>
<span id="cb1-72">    )</span>
<span id="cb1-73">ax.scatter(</span>
<span id="cb1-74">    x, y_pred, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Predicted value"</span></span>
<span id="cb1-75">)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If you want to show where the predicted points lie on the line</span></span>
<span id="cb1-76"></span>
<span id="cb1-77">ax.annotate(</span>
<span id="cb1-78">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"residual"</span>,</span>
<span id="cb1-79">    xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb1-80">    xycoords<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data"</span>,</span>
<span id="cb1-81">    xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>),</span>
<span id="cb1-82">    textcoords<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"axes fraction"</span>,</span>
<span id="cb1-83">    va<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"top"</span>,</span>
<span id="cb1-84">    ha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"left"</span>,</span>
<span id="cb1-85">    fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,</span>
<span id="cb1-86">    arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(</span>
<span id="cb1-87">        arrowstyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-&gt;"</span>,</span>
<span id="cb1-88">        facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>,</span>
<span id="cb1-89">    ),</span>
<span id="cb1-90">)</span>
<span id="cb1-91"></span>
<span id="cb1-92">ax.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-93">ax.set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"x"</span>)</span>
<span id="cb1-94">ax.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span>)</span>
<span id="cb1-95">ax.set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Fited model"</span>)</span>
<span id="cb1-96">ax.legend()</span>
<span id="cb1-97">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-least-square" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-least-square-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_regression/index_files/figure-html/fig-least-square-output-1.png" width="856" height="376" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-least-square-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Linear model fitted to a given data distribution by means of least-square. Note that the differece between the predicted value and the given datapoint is known as residual.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Finally, it is crucial to highlight some key assumptions of this model type:</p>
<ol type="1">
<li>Linearity: There must be a linear relationship between the independent and dependent variables.</li>
<li>Independence: The predictors should be independent of each other. This is vital for the model’s stability and interpretability. Collinearity, or correlation between variables, can lead to significant changes in the outcome variable for minor alterations in the predictor variables, contradicting the assumption of a linear relationship. Additionally, a model with independent variables is easier to interpret as each variable contributes uniquely to the prediction.</li>
<li>Homoscedasticity: The residuals should be uniformly distributed with constant variance. Without this, it becomes challenging to ensure that the model is unbiased and to conduct accurate error analysis.</li>
</ol>
<section id="least-square-estimation" class="level3">
<h3 class="anchored" data-anchor-id="least-square-estimation">Least Square Estimation</h3>
<p>As illustrated in Figure&nbsp;1,a linear model seeks to find the line that minimizes the distance between the fitted line and the data points. While stochastic gradient descent is a viable optimization approach, a more concise solution based on linear algebra can be derived from Equation&nbsp;3. Linear algebra provides a more general formulation that is robust with respect to the number of independent variables. This allows for a unified solution applicable to both the bivariate and multivariate cases. Considering a dataset composed of <img src="https://latex.codecogs.com/png.latex?N"> elements, we can reformulate the loss function as follows:</p>
<p><span id="eq-derivation"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Carg%20%5Cmin%20%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D)%20&amp;=%20%5Csum_%7Bi=1%7D%5EN(y_i%20-%20%5Chat%7By%7D_i)%5E2%20%5C%5C%0A&amp;%20=%20%5Csum_%7Bi=1%7D%5EN%20%5Cbig%5B%20(%5Crvy%5ET%20-%20(%5CrmX%20%5Crvw%5ET))%5E2%20%5Cbig%5D_i%20&amp;%20%5Csubstack%7B%5Ctext%7BWith%20an%20abuse%20of%20notation%7D%20%5C%5C%20%5Ctext%7Bwe%20reduce%20over%20the%20row%20of%20this%20matrix%7D%7D%5C%5C%0A&amp;%20=%20%5Csum_%7Bi=1%7D%5EN%20%5Cbig%5B%20%5Crvd%5E2%20%5Cbig%5D%20%5C%5C%0A&amp;%20=%20%5Crvd%20%5Crvd%5ET%20&amp;%20%5Csubstack%7B%5Ctext%7BNote%20that%20row%20vector%20multiplied%20by%7D%20%5C%5C%20%5Ctext%7Ba%20column%20vector%20return%20a%20scalar%7D%7D%5C%5C%0A&amp;%20=%20%7C%7C%20%5Crvd%20%7C%7C%5E2%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Crvy%5ET"> is the collumn vector <img src="https://latex.codecogs.com/png.latex?%5Cleft%5B%5Cbegin%7Balign%7D%20y_1%20%5C%5C%20%5Cvdots%20%5C%5C%20y_N%20%5Cend%7Balign%7D%5Cright%5D">;</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CrmX"> is the <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20k+1"> matrix <img src="https://latex.codecogs.com/png.latex?%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%201%20&amp;%20x_%7B1,1%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bk,1%7D%20%5C%5C%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%201%20&amp;%20x_%7B1,N%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bk,N%7D%5Cend%7Barray%7D%5Cright%5D">;</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Crvw%5ET"> is the is the <img src="https://latex.codecogs.com/png.latex?k+1"> column vector <img src="https://latex.codecogs.com/png.latex?%5Cleft%5B%5Cbegin%7Balign%7D%20w_0%20%5C%5C%20w_1%20%5C%5C%20%5Cvdots%20%5C%5C%20w_k%20%5Cend%7Balign%7D%5Cright%5D">;</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Crvd"> is the row vector <img src="https://latex.codecogs.com/png.latex?%5Cleft%5By_1%20-%20%5Chat%7By%7D_1,%20...,%20%20y_N%20-%20%5Chat%7By%7D_N%5Cright%5D">.</li>
</ul>
<p>It is then possible to compute the optimal paramters by differenciating w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Crvw">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D)%7D%7B%5Cpartial%20%5Crvw%5ET%7D%20=%20&amp;%20%5Cfrac%7B%5Cpartial%20%7C%7C%20%5Crvy%5ET%20-%20%5CrmX%20%5Crvw%5ET%20%7C%7C%5E2%7D%7B%5Cpartial%20%5Crvw%5ET%7D%20%5C%5C%0A&amp;%20=%202%5CrmX%5ET%20(%5Crvy%5ET%20-%20%5CrmX%20%5Crvw%5ET)%20%5C%5C%0A&amp;%20=%202%20%5CrmX%5ET%5Crvy%5ET%20-%202%20%5CrmX%5ET%20%5CrmX%20%5Crvw%5ET%20%5C%5C%0A&amp;%202%20%5CrmX%5ET%20%5CrmX%20%5Crvw%5ET%20=%202%20%5CrmX%5ET%5Crvy%5ET%20&amp;%20%5Csubstack%7B%5Ctext%7BThe%20minumum%20of%20%7D%20%5Cmathcal%7BL%7D%20%5Ctext%7Bis%20given%20by%7D%20%5C%5C%20%5Cpartial%20%5Cmathcal%7BL%7D%20=%200%20%5Ctext%7B%20and%20solving%20by%20%7D%20%5Crvw%5ET%7D%5C%5C%0A&amp;%20%5Crvw%5ET%20=%20%5Cfrac%7B%5CrmX%5ET%5Crvy%5ET%7D%7B%5CrmX%5ET%20%5CrmX%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>Note that <img src="https://latex.codecogs.com/png.latex?%5CrmX%5ET%20%5CrmX"> is an <img src="https://latex.codecogs.com/png.latex?k+1%20%5Ctimes%20k+1"> matrix; thus could be invertible, <img src="https://latex.codecogs.com/png.latex?%5CrmX%5ET%5Crvy%5ET"> is an <img src="https://latex.codecogs.com/png.latex?k+1%20%5Ctimes%201"> column vector and <img src="https://latex.codecogs.com/png.latex?%5Crvw%5ET"> is the <img src="https://latex.codecogs.com/png.latex?k+1%20%5Ctimes%201"> column vector of unknown parameters. Finally, this solution, known as ordinary least squares, is applicable to any model with a linear formulation concerning the model’s parameters, allowing for polynomial independent variables.</p>
</section>
<section id="residual-plot" class="level2">
<h2 class="anchored" data-anchor-id="residual-plot">Residual Plot</h2>
<p>Residuals plots <span class="citation" data-cites="sefidianUnderstandingInterpreting">[2]</span> <span class="citation" data-cites="tsai1998examination">[3]</span> are one of the most common methods to validate the presence of Homoscedasticity. As shonw in Figure&nbsp;2, residuals plots disply the residual values of a regression as a function of the predicted values and helps in understanding if the model is a good fit for the given data. Namelly residuals plot can be used for:</p>
<ol type="1">
<li>Checking Homoscedasticity: A key assumption in linear regression is that the residuals have constant variance at every level of the independent variable(s) (homoscedasticity). If the residuals fan out or form a pattern as the predicted values increase, this indicates heteroscedasticity, which can lead to inefficient estimates and affect hypothesis testing.</li>
<li>Identifying Non-Linearity: If the relationship between the variables is not linear, the residuals will often display a systematic pattern. A residuals plot can help identify such non-linear relationships, suggesting that a different model or a transformation of variables might be more appropriate.</li>
<li>Detecting Outliers: Residuals plots can reveal outliers – points that have a much larger error than the other observations. These outliers can disproportionately influence the model fit and can be indicative of data entry errors, or that the outlier is from a different population.</li>
<li>Evaluating Model Fit: If a model is a good fit for the data, the residuals should be randomly scattered around zero. Patterns or trends in the residuals suggest that the model is not capturing some aspect of the data, which could lead to biased or inaccurate predictions.</li>
<li>Checking Independence of Residuals: For a good model, the residuals should be independent of each other. If there’s a pattern over time (in time series data) or a pattern associated with another variable, this is a sign that the model is missing a key piece of information.</li>
<li>Verifying Normal Distribution of Residuals: In many forms of regression analysis (like linear regression), it’s assumed that residuals are normally distributed. By plotting the residuals and visually checking their distribution (or using statistical tests), we can validate this assumption. A common method is to use a Q-Q plot (quantile-quantile plot) to compare the distribution of the residuals to a normal distribution.</li>
</ol>
<div id="cell-fig-residual-plot" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-2"></span>
<span id="cb2-3">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb2-4"></span>
<span id="cb2-5">ax.scatter(y_pred, residual, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"residual"</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gray"</span>, edgecolors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k"</span>)</span>
<span id="cb2-6">ax.hlines(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"k"</span>, linestyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dashed"</span>)</span>
<span id="cb2-7"></span>
<span id="cb2-8">z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.polyfit(y_pred, residual, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-9">residual_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (z[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> y_pred) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> z[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb2-10">ax.plot(y_pred, residual_pred, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"residual fit"</span>)</span>
<span id="cb2-11"></span>
<span id="cb2-12">ax.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-13">ax.set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prediction"</span>)</span>
<span id="cb2-14">ax.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"residual"</span>)</span>
<span id="cb2-15">ax.legend()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-residual-plot" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residual-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_regression/index_files/figure-html/fig-residual-plot-output-1.png" width="596" height="356" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residual-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Residual plot. As you can see, the residual are simmetrically randomly distributed and in general, there aren’t any clear patterns. Moreover, the best-fit line to the residual is almost identical to the x-axis (y=0) indicating independence bewteen residuals and predicted values.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- ## Total Least Squares



Total Least Squares [@markovsky2007overview] (a.k.a Orthogonal Distance Regression) are a variant of the linear regression model specifically designed to handle situations in which we have errors not only on the mesurment of $y$, but also in the dependent variable $x$. -->
</section>
<section id="robust-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="robust-linear-model">Robust Linear Model</h2>
<p>In the real world, datasets often suffer from multicollinearity among independent variables. One effective method to mitigate this issue is through <a href="https://www.youtube.com/watch?v=Q81RR3yKn30&amp;ab_channel=StatQuestwithJoshStarmer"><strong>Ridge Regression</strong></a>, described by the formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20%5Cepsilon%20+%20w_0%20+%20w_1%20x_1%20+%20...%20+%20w_k%20x_k%20+%20%5Clambda%20%5Csum_%7B1%20%5Cleq%20i%20%5Cleq%20k+1%7D%20w_i%5E2.%0A"></p>
<p>Ridge Regression is essentially linear regression augmented with a penalty term. This term comprises the squared coefficients of the model, effectively minimizing their magnitude. This added penalty increases the model’s bias but decreases prediction variance by imposing a normal distribution prior on the model parameters.</p>
<hr>
<p>On the other hand, <a href="https://www.youtube.com/watch?v=NGf0voTMlcs&amp;ab_channel=StatQuestwithJoshStarmer"><strong>Lasso Regression</strong></a> is another widely-used variation of linear regression. Similar to Ridge, Lasso adds a penalty to the loss function of the linear model. However, Lasso minimizes the absolute value of the coefficients rather than their square:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20%5Cepsilon%20+%20w_0%20+%20w_1%20x_1%20+%20...%20+%20w_k%20x_k%20+%20%5Clambda%20%5Csum_%7B1%20%5Cleq%20i%20%5Cleq%20k+1%7D%20%7Cw_i%7C.%0A"></p>
<p>The key distinction is that Lasso regression can reduce some coefficients to zero, producing a sparse model, whereas Ridge regression only reduces them to near zero.</p>
<p>In conclusion, both these regularization approaches generate a model that has a bigger bias, but better generalization capability.</p>
</section>
<section id="parameters-analysis" class="level2">
<h2 class="anchored" data-anchor-id="parameters-analysis">Parameter’s Analysis</h2>
<p>There are instances when we seek to comprehend how one of our predictors influences the dependent variable. Specifically, our interest lies in determining whether the parameter <img src="https://latex.codecogs.com/png.latex?w_i"> significantly affects the response variable <img src="https://latex.codecogs.com/png.latex?y"> - that is, whether including the predictor <img src="https://latex.codecogs.com/png.latex?x_i"> leads to a notable reduction in the model’s loss.</p>
<p>Formally, this involves testing the following hypotheses:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AH_0%20:%20w_i%20=%200%20%5C%5C%0AH_1%20:%20w_i%20%5Cneq%200.%0A%5Cend%7Balign*%7D%0A"></p>
<p>For clarity, let’s denote <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Crvw%5ET_%7B%5Cnot%20i%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Crvw%5ET)"> as the sum of squared residuals for models excluding and including the <img src="https://latex.codecogs.com/png.latex?i">-th predictor, respectively. Assuming independence and homoscedasticity of the model’s parameters, the significance of the <img src="https://latex.codecogs.com/png.latex?i">-th predictor can be assessed using the F-test:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AF%20=%20%5Cfrac%7B%20%5Cfrac%7B%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Crvw%5ET_%7B%5Cnot%20i%7D)%20-%20%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Crvw%5ET)%7D%7Bp_2%7D%20%7D%7B%20%5Cfrac%7B%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Crvw%5ET)%7D%7BN%20-%20p%7D%20%7D%20.%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?p"> and <img src="https://latex.codecogs.com/png.latex?p_2"> represent the degrees of freedom for the overall model and the model containing only the <img src="https://latex.codecogs.com/png.latex?i">-th predictor; while <img src="https://latex.codecogs.com/png.latex?N"> is the number of training examples. The numerator of <img src="https://latex.codecogs.com/png.latex?F">-test represents the reduction in the residual sum of squares per additional degree of freedom utilized. The denominator is an estimate of the residual variance, serving as a measure of the model’s inherent noise. An <img src="https://latex.codecogs.com/png.latex?F">-ratio of one suggests that the predictors merely contribute noise. A ratio greater than one implies meaningful contribution, or signal, from the predictors. Typically, we reject <img src="https://latex.codecogs.com/png.latex?H_0"> and conclude that the <img src="https://latex.codecogs.com/png.latex?i">-th variable significantly impacts the response if the <img src="https://latex.codecogs.com/png.latex?F">-statistic exceeds the 95th percentile of the <img src="https://latex.codecogs.com/png.latex?F">-distribution. A full derivation of this result is available <a href="https://grodri.github.io/glms/notes/c2s3">here</a>.</p>
</section>
<section id="r-squared" class="level2">
<h2 class="anchored" data-anchor-id="r-squared">R-squared</h2>
<p>The <img src="https://latex.codecogs.com/png.latex?R%5E2"> metric, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a regression model. It represents the proportion of the variance in the dependent variable that is predictable from the independent variables. In simpler terms, <img src="https://latex.codecogs.com/png.latex?R%5E2"> indicates how well the data fit the regression model (the closer the value of <img src="https://latex.codecogs.com/png.latex?R%5E2"> is to 1, the better the fit) and can be computed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR%5E2%20=%20%5Cfrac%7B%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Cvarnothing)%20-%20%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Crvw%5ET)%7D%7B%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Cvarnothing)%7D%0A"></p>
<p>here <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Cvarnothing)"> represent the sum of squared residuals of a model wihtout parameters, a.k.a a model that alwasy predict the mean of the response variable. Similarly, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y,%20%5Chat%7By%7D:%20%5Crvw%5ET)"> is the linear regression developed.</p>
<p>The <img src="https://latex.codecogs.com/png.latex?R%5E2"> value ranges from 0 to 1. A vale of 0 means that the model does not explain any of the variability of the response data around its mean. On the other hand, an <img src="https://latex.codecogs.com/png.latex?R%5E2"> of 1 indicates that the model explains all the variability of the response data around its mean. In conclusion <img src="https://latex.codecogs.com/png.latex?R%5E2"> is a simple to compute, yet informative metric to determin how much our variance our model is able to predict correctly.</p>
</section>
<section id="improvements" class="level2">
<h2 class="anchored" data-anchor-id="improvements">Improvements</h2>
<p>In general it is possible to rank the model performance in terms of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D">. Thus, here are useful methods to reduce the sum of squared residuals:</p>
<ol type="1">
<li>Feature Selection: Choose relevant and significant variables to include in the model.</li>
<li>Transformation of Variables: Apply transformations (like log, square root, or inverse) to make the relationship more linear.</li>
<li>Polynomial Regression: Use higher-order terms (quadratic, cubic, etc.) if the relationship is not purely linear.</li>
<li>Interaction Terms: Include interaction terms if the effect of one variable depends on another. An interaction variable in a linear model represents the combined effect of two or more variables on the dependent variable, where the effect of one variable depends on the level of another variable. In other words, it’s used to capture situations where the relationship between a predictor and the outcome changes based on the value of another predictor. Formally, an interaction variable in alinear model is defined as:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20%5Cepsilon%20+%20w_0%20+%20w_1%20x_1%20+%20...%20+%20w_n%20x_n%20(x_i%20%5Ccdot%20x_j)%0A"></p>
<ol start="5" type="1">
<li>Regularization Techniques: Methods like Ridge, Lasso, or Elastic Net can help in reducing overfitting and improving prediction.</li>
<li>Residual Plots: Use residual plots to check for non-linearity, unequal error variances, and outliers.</li>
<li>Influence Measures: Identify and investigate influential observations that might disproportionately affect the model’s performance.</li>
<li>Homoscedasticity Testing: Ensure that residuals have constant variance across different levels of predictors.</li>
</ol>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-montgomery2021introduction" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Montgomery DC, Peck EA, Vining GG (2021) Introduction to linear regression analysis. John Wiley &amp; Sons</div>
</div>
<div id="ref-sefidianUnderstandingInterpreting" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><span>U</span>nderstanding and interpreting <span>R</span>esiduals <span>P</span>lot for linear regression - <span>A</span>mir <span>M</span>asoud <span>S</span>efidian - <span>S</span>efidian <span>A</span>cademy — sefidian.com</div>
</div>
<div id="ref-tsai1998examination" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Tsai C-L, Cai Z, Wu X (1998) The examination of residual plots. Statistica Sinica 445–465</div>
</div>
</div>


</section>

 ]]></description>
  <category>Statistics</category>
  <guid>https://andompesta.github.io/posts/linear_regression/</guid>
  <pubDate>Mon, 15 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Normalizing Flows</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/normalization_flow/</link>
  <description><![CDATA[ 




<p>Normalizing Flows (NF) represent a potent technique that facilitates the learning and sampling from intricate probability distributions <span class="citation" data-cites="normalization-flow-review">[1]</span> <span class="citation" data-cites="dinh2014nice">[2]</span>. These models, categorized as generative models, enable the precise estimation of likelihood for continuous input data, denoted as <img src="https://latex.codecogs.com/png.latex?p(x)">. In contrast to methods such as variational inference that rely on approximations, normalizing flows function by transforming samples from a simple distribution, denoted as <img src="https://latex.codecogs.com/png.latex?z%20%5Csim%20p(z)">, into samples from a more complex distribution using the following transformation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax%20=%20f_%7B%5Ctheta%7D(z),%20~~%20z%20%5Csim%20p(z;%20%5Cpsi).%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D(%5Ccdot)"> is a mapping function from <img src="https://latex.codecogs.com/png.latex?z"> to <img src="https://latex.codecogs.com/png.latex?x">, parametrized by <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, and <img src="https://latex.codecogs.com/png.latex?p(z;%20%5Cpsi)"> is the base distribution (sometimes referred to as the prior distribution), parametrized by <img src="https://latex.codecogs.com/png.latex?%5Cpsi">, from which samples can be drawn. The essential properties defining a normalizing flow include:</p>
<p>The defining propertires of a normalizing flow are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D(%5Ccdot)"> must be invertible,</li>
<li>Both <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D(%5Ccdot)"> and <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D%5E%7B-1%7D(%5Ccdot)"> must be differentiable.</li>
</ul>
<p>Adhering to these constraints ensures the well-defined density of <img src="https://latex.codecogs.com/png.latex?x">, as established by the change-of-variable theorem <span class="citation" data-cites="change-of-variable">[3]</span>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cint%20p(x)%20%5Cpartial%20x%20&amp;=%20%5Cint%20p(z)%20%5Cpartial%20z%20=%201%20%5C%5C%0A%20%20%20%20%5Cimplies%20p(x)%20&amp;%20=%20p(z)%20%5Ccdot%20%7C%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%7C%20%5C%5C%0A%20%20%20%20&amp;%20=%20p%5Cbig(f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%5Cbig)%20%5Ccdot%20%7C%5Cfrac%7B%5Cpartial%20f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%7D%7B%5Cpartial%20x%7D%7C%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
<p>In its definition, <img src="https://latex.codecogs.com/png.latex?%5Cpartial%20x"> represents the width of an infinitesimally small rectangle with height <img src="https://latex.codecogs.com/png.latex?p(x)">. Consequently, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%7D%7B%5Cpartial%20x%7D"> denotes the ratio between the areas of rectangles defined in two distinct coordinate systems: one in terms of <img src="https://latex.codecogs.com/png.latex?x"> and the other in terms of <img src="https://latex.codecogs.com/png.latex?z">.<br>
For illustrative purposes, consider Figure&nbsp;1, which depicts how the affine transformation <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%20=%20(5%20%5Ccdot%20x)%20-%202"> maps the Normal distribution <img src="https://latex.codecogs.com/png.latex?p(x;%20%5Cmu=0,%20%5Csigma=1)"> to another Gaussian distribution <img src="https://latex.codecogs.com/png.latex?p(z;%20%5Cmu=-2,%20%5Csigma=5)">. With <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%20=%205">, the area <img src="https://latex.codecogs.com/png.latex?%5Cpartial%20x"> undergoes a stretching factor of 5 when transformed into the variable <img src="https://latex.codecogs.com/png.latex?z">. Consequently, <img src="https://latex.codecogs.com/png.latex?p(z)"> must be lowered by a factor of 5 to maintain its validity as a probability density function, satisfying the condition <img src="https://latex.codecogs.com/png.latex?%5Cint%20p(z)%20%5Cpartial%20z%20=%201">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(z)%20=%20%5Cfrac%7Bp(x)%7D%7B%5Cfrac%7B%5Cpartial%20f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%7D%7B%5Cpartial%20x%7D%7D%20=%20%5Cfrac%7Bp(x)%7D%7Bf_%7B%5Ctheta%7D%5E%7B-1'%7D(x)%7D.%0A"></p>
<div id="fig-change-of-variable" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-change-of-variable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/change-of-variable.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-change-of-variable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Illustration of a Change-of-Variable. The random variable <img src="https://latex.codecogs.com/png.latex?x"> undergoes a transformation into another variable <img src="https://latex.codecogs.com/png.latex?z"> through the affine function <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%20=%205x%20-%202">; equivalently, <img src="https://latex.codecogs.com/png.latex?z"> can be expressed as <img src="https://latex.codecogs.com/png.latex?5x-2">. Ensuring the validity of the density function <img src="https://latex.codecogs.com/png.latex?p(z)"> requires satisfying the condition <img src="https://latex.codecogs.com/png.latex?%5Cint%20p(z)%20%5Cpartial%20z%20=%201">. However, due to the stretching effect of the transformation <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D%5E%7B-1'%7D(x)"> by a factor of 5, the density must be adjusted accordingly.<br>
Take note of the disparity between the maximum values of <img src="https://latex.codecogs.com/png.latex?p(z)"> and <img src="https://latex.codecogs.com/png.latex?p(x)"> for a visual representation, as depicted in the lower-left image illustrating the stretching of <img src="https://latex.codecogs.com/png.latex?%5Cpartial%20x"> caused by the transformation <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D%5E%7B-1%7D(%5Ccdot)">.
</figcaption>
</figure>
</div>
<p>In the preceding paragraph, we introduced the concept of area-preserving transformations. Extending this notion to the multidimensional space involves considering <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D"> not as a simple derivative but as the <strong>Jacobian</strong> matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AJ_%7Bz%7D(x)%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%5Cfrac%7B%5Cpartial%20z_1%7D%7B%5Cpartial%20x_1%7D%20&amp;%20%5Cdots%20&amp;%20%5Cfrac%7B%5Cpartial%20z_1%7D%7B%5Cpartial%20x_D%7D%5C%5C%0A%20%20%20%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%20%5Cvdots%20%5C%5C%0A%20%20%20%20%5Cfrac%7B%5Cpartial%20z_D%7D%7B%5Cpartial%20x_1%7D%20&amp;%20%5Cdots%20&amp;%20%5Cfrac%7B%5Cpartial%20z_D%7D%7B%5Cpartial%20x_D%7D%0A%5Cend%7Bbmatrix%7D.%0A"></p>
<p>In the multidimensional setting, the difference in areas translates to a difference in volumes quantified by the determinant of the Jacobian matrix, denoted as <img src="https://latex.codecogs.com/png.latex?det(J_%7Bz%7D(x))%20%5Capprox%20%5Cfrac%7BVol(z)%7D%7BVol(x)%7D">. Consolidating these concepts, we can formalize a multidimensional normalization flow as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Ap(x)%20&amp;%20=%20p(z)%20%5Ccdot%20%7Cdet(%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D)%7C%20%5C%5C%0A&amp;%20=%20p(f_%7B%5Ctheta%7D%5E%7B-1%7D(x))%20%5Ccdot%20%7Cdet(%5Cfrac%7B%5Cpartial%20f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%7D%7B%5Cpartial%20x%7D)%7C%20%5C%5C%0A&amp;%20=%20p(f_%7B%5Ctheta%7D%5E%7B-1%7D(x))%20%5Ccdot%20%7Cdet(J_%7Bf_%7B%5Ctheta%7D%5E%7B-1%7D%7D(x))%7C.%0A%5Cend%7Balign*%7D%0A"></p>
<section id="generative-process-as-finate-composition-of-transformations" class="level2">
<h2 class="anchored" data-anchor-id="generative-process-as-finate-composition-of-transformations">Generative Process as Finate Composition of Transformations</h2>
<p>In the general case, the transformations <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D(%5Ccdot)"> and <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D%5E%7B-1%7D(%5Ccdot)"> are defined as finite compositions of simpler transformations <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta_i%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Ax%20&amp;%20=%20z_%7BK%7D%20=%20f_%7B%5Ctheta%7D(z_0)%20=%20f_%7B%5Ctheta_K%7D%20%5Cdots%20f_%7B%5Ctheta_2%7D%20%5Ccirc%20f_%7B%5Ctheta_1%7D(z_0)%20&amp;%20%5C%5C%0Ap(x)%20&amp;%20=%20p_K(z_%7Bk%7D)%20=%20p_%7BK-1%7D(f_%7B%5Ctheta_K%7D%5E%7B-1%7D(z_%7Bk%7D))%20%5Ccdot%20%5CBig%7C%20det%5CBig(J_%7Bf_%7B%5Ctheta_K%7D%5E%7B-1%7D%7D(z_%7Bk%7D)%5CBig)%5CBig%7C%20&amp;%20%5C%5C%0A&amp;%20=%20p_%7BK-1%7D(z_%7BK-1%7D)%20%5Ccdot%20%5CBig%7C%20det%5CBig(J_%7Bf_%7B%5Ctheta_K%7D%5E%7B-1%7D%7D(z_k)%5CBig)%5CBig%7C%20&amp;%20%5Ctext%7BDue%20to%20the%20definition%20of%20%7D%20f_%7B%5Ctheta_K%7D(z_%7BK-1%7D)%20=%20z_K%5C%5C%0A&amp;%20=%20p_%7BK-1%7D(z_%7BK-1%7D)%20%5Ccdot%20%5CBig%7C%20det%5CBig(%20J_%7Bf_%7B%5Ctheta_K%7D(z_%7BK-1%7D)%7D%20%5CBig)%5E%7B-1%7D%5CBig%7C%20&amp;%20%5Ctext%7BAs:%20%7D%20J_%7Bf_%7B%5Ctheta_K%7D%5E%7B-1%7D%7D(x)%20=%20%5Cfrac%7Bf_%7B%5Ctheta_K%7D%5E%7B-1%7D(z_k)%7D%7B%5Cpartial%20z_K%7D%20%5C%5C%0A&amp;%20&amp;%20=%20%5CBig(%5Cfrac%7B%5Cpartial%20z_K%7D%7B%5Cpartial%20f_%7B%5Ctheta_k%7D%5E%7B-1%7D(z_k)%7D%20%5CBig)%5E%7B-1%7D%20%5C%5C%0A&amp;%20&amp;%20=%20%5CBig(%5Cfrac%7B%5Cpartial%20f_%7B%5Ctheta_%7Bk%7D%7D(z_%7BK-1%7D)%7D%7B%5Cpartial%20z_%7BK-1%7D%7D%5CBig)%5E%7B-1%7D%20%5C%5C%0A&amp;%20&amp;%20=%20%5CBig(%20J_%7Bf_%7B%5Ctheta_K%7D(z_%7BK-1%7D)%7D%20%5CBig)%5E%7B-1%7D%20%5C%5C%0A&amp;%20=%20p_%7BK-1%7D(z_%7BK-1%7D)%20%5Ccdot%20%5CBig%7C%20det%5CBig(%20J_%7Bf_%7B%5Ctheta_K%7D%7D(z_%7BK-1%7D)%20%5CBig)%5CBig%7C%5E%7B-1%7D.%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
<p>By this process, <img src="https://latex.codecogs.com/png.latex?p(z_i)"> is fully described by <img src="https://latex.codecogs.com/png.latex?z_%7Bi-1%7D"> and <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta_i%7D">, allowing the extension of the previous reasoning to all i-steps of the overall generative process:</p>
<p><span id="eq-flow-generator"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0Ap(x)%20=%20p(z_0)%20%5Ccdot%20%5Cprod_%7Bi=1%7D%5Ek%20%5CBig%7C%20det%20%5Cbig(%20J_%7Bf_%7B%5Ctheta_i%7D%7D(z_%7Bi-1%7D)%20%5Cbig)%20%5CBig%7C%5E%7B-1%7D.%0A%5Cend%7Bequation%7D%0A%5Ctag%7B1%7D"></span></p>
<p>It is noteworthy that in the context of generative models, <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D"> is also referred to as a pushforward mapping from a simple density <img src="https://latex.codecogs.com/png.latex?p(z)"> to a more complex <img src="https://latex.codecogs.com/png.latex?p(x)">. On the other hand, the inverse transformation <img src="https://latex.codecogs.com/png.latex?f_%7B%5Ctheta%7D%5E%7B-1%7D"> is known as the normalization function, as it systematically “normalizes” a complex distribution into a simpler one, one step at a time.</p>
</section>
<section id="training-procedures" class="level2">
<h2 class="anchored" data-anchor-id="training-procedures">Training Procedures</h2>
<p>As previously mentioned, NFs serve as efficient models for both sampling from and learning complex distributions. The primary applications of NFs lie in density estimation and data generation.<br>
Density estimation proves valuable for computing statistical quantities over unseen data, as demonstrated in works such as <span class="citation" data-cites="dinh2016density">[4]</span> and <span class="citation" data-cites="grathwohl2018ffjord">[5]</span>, where NF models effectively estimate densities for tabular and image datasets. Additionally, NFs find application in anomaly detection <span class="citation" data-cites="hirschorn2023normalizing">[6]</span>, although requiring careful tuning for out-of-distribution detection <span class="citation" data-cites="kirichenko2020normalizing">[7]</span>.<br>
On the flip side, data generation stands out as the central application for NFs. As mentioned earlier, NFs, under mild assumptions, can sample new data points from a complex distribution <img src="https://latex.codecogs.com/png.latex?p(x)">. Exemplifying this, <span class="citation" data-cites="kingma2018glow">[8]</span> showcases NFs applied to image generation, while <span class="citation" data-cites="oord2016wavenet">[9]</span> and <span class="citation" data-cites="kim2018flowavenet">[10]</span> demonstrate successful learning of audio signals through NFs.</p>
<p>A key advantage of NFs over other probabilistic generative models lies in their ease of training, achieved by minimizing a divergence metric between <img src="https://latex.codecogs.com/png.latex?p(x;%20%5Ctheta)"> and the target distribution <img src="https://latex.codecogs.com/png.latex?p(x)">. In most cases, NFs are trained by minimizing the Kullback-Leibler (KL) divergence between these two distributions:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cmathcal%7BL%7D(%5Ctheta)%20&amp;%20=%20D_%7BKL%7D%5Bp(x)%20%7C%7C%20p(x;%20%5Ctheta)%5D%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%20=%20-%20%5Csum_%7Bx%20%5Csim%20p(x)%7D%20p(x)%20%5Ccdot%20%5Clog%20%5Cfrac%7Bp(x;%20%5Ctheta)%7D%7Bp(x)%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%20=%20-%20%5Csum_%7Bx%20%5Csim%20p(x)%7D%20p(x)%20%5Ccdot%20%5Clog%20p(x;%20%5Ctheta)%20+%20%5Csum_%7Bx%20%5Csim%20p(x)%7D%20p(x)%20%5Ccdot%20%5Clog%20p(x)%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%20=%20-%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20p(x)%7D%20%5CBig%5B%20%5Clog%20p(x;%20%5Ctheta)%20%5CBig%5D%20+%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20p(x)%7D%20%5CBig%5B%20%5Clog%20p(x)%20%5CBig%5D%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%20=%20-%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20p(x)%7D%5CBig%5B%5Clog%20p(x;%20%5Ctheta)%5CBig%5D%20+%20const.%20~~~%20%5Ctext%7BAs%20it%20does%20not%20depend%20on%20$%5Ctheta$%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%20=%20-%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20p(x)%7D%5CBig%5B%5Clog%20p%5Cbig(f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%5Cbig)%20+%20%5Csum_%7Bi=1%7D%5E%7BK%7D%20%5Clog%20%5CBig%7C%20det%5Cbig(%20J_%7Bf_%7B%5Ctheta_i%7D%5E%7B-1%7D%7D(z_%7Bi%7D)%20%5Cbig)%5CBig%7C%20%5CBig%5D%20+%20const.%0A%5Cend%7Balign*%7D%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?p(f_%7B%5Ctheta%7D%5E%7B-1%7D(x))%20=%20p(z_0)">, and <img src="https://latex.codecogs.com/png.latex?z_K"> is equal to <img src="https://latex.codecogs.com/png.latex?x">. For a fixed training set <img src="https://latex.codecogs.com/png.latex?X_N%20=%20%5C%5C%7B%20x_n%20%5C%5C%7D_%7Bn=1%7D%5EN">, the loss function is derived as the negative log-likelihood typically optimized using stochastic gradient descent:</p>
<p><span id="eq-flow-loss"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cmathcal%7BL%7D(%5Ctheta)%20=%20-%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bn=1%7D%5EN%20%5Clog%20p%5Cbig(f_%7B%5Ctheta%7D%5E%7B-1%7D(x)%5Cbig)%20+%20%5Csum_%7Bi=1%7D%5E%7BK%7D%20%5Clog%20%5CBig%7C%20det%5Cbig(%20J_%7Bf_%7B%5Ctheta_i%7D%5E%7B-1%7D%7D(z_%7Bi%7D)%20%5Cbig)%5CBig%7C.%0A%5Cend%7Bequation%7D%0A%5Ctag%7B2%7D"></span></p>
<p>It is important to note that the loss function (Equation&nbsp;2) is computed by starting from a datapoint <img src="https://latex.codecogs.com/png.latex?x"> and reversing it to a plausible latent variable <img src="https://latex.codecogs.com/png.latex?z_0">. Consequently, the structural formulation of <img src="https://latex.codecogs.com/png.latex?p(z_0)"> plays a critical role in defining the training signals: if <img src="https://latex.codecogs.com/png.latex?p(z_0)"> is too lax, the training process lacks substantial information; if it is too stringent, the training process may become overly challenging. Furthermore, the training process is the inverse of the generative process defined in Equation&nbsp;1, emphasizing the importance of the sum of determinants. Achieving computationally efficient training requires the efficient computation of determinants of <img src="https://latex.codecogs.com/png.latex?J_%7Bf_%7B%5Ctheta_i%7D%5E%7B-1%7D%7D">. While auto-diff libraries can compute gradients with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta_i"> of the Jacobian matrix and its determinant, such computations are computationally expensive (<img src="https://latex.codecogs.com/png.latex?O(n)%5E3">). Therefore, significant research efforts have focused on designing transformations with efficient Jacobian determinant formulations.</p>
<section id="training-example" class="level3">
<h3 class="anchored" data-anchor-id="training-example">Training Example</h3>
<p>As previously mentioned, the training process of a NF involves mapping a given input data <img src="https://latex.codecogs.com/png.latex?x"> to a specific base distribution <img src="https://latex.codecogs.com/png.latex?p(z_0)">. Typically, the base distribution is a well-known distribution such as a multivariate Gaussian, Uniform, or any other exponential distribution. Similarly, the mapping function is usually implemented as a neural network.</p>
<p>Starting from first principles, any NF model can be specified as comprising a base distribution and a series of flows that map <img src="https://latex.codecogs.com/png.latex?x"> to <img src="https://latex.codecogs.com/png.latex?z_0">. Here is a Python implementation:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> NormalizingFlow(nn.Module):</span>
<span id="cb1-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(</span>
<span id="cb1-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb1-4">        prior: Distribution,</span>
<span id="cb1-5">        flows: nn.ModuleList,</span>
<span id="cb1-6">    ):</span>
<span id="cb1-7">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb1-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prior <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prior</span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.flows <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> flows</span>
<span id="cb1-10"></span>
<span id="cb1-11">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x: Tensor):</span>
<span id="cb1-12">        bs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-13">        zs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [x]</span>
<span id="cb1-14">        sum_log_det <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(bs).to(x.device)</span>
<span id="cb1-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> flow <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.flows:</span>
<span id="cb1-16">            z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> flow(x)</span>
<span id="cb1-17">            log_det <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> flow.log_abs_det_jacobian(x, z)</span>
<span id="cb1-18">            zs.append(z)</span>
<span id="cb1-19">            sum_log_det <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> log_det</span>
<span id="cb1-20">            x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> z</span>
<span id="cb1-21"></span>
<span id="cb1-22">        prior_logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prior.log_prob(z).view(bs, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-23">        log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prior_logprob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> sum_log_det</span>
<span id="cb1-24"></span>
<span id="cb1-25">        intermediat_results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(</span>
<span id="cb1-26">            prior_logprob<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_logprob,</span>
<span id="cb1-27">            sum_log_det<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sum_log_det,</span>
<span id="cb1-28">            zs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>zs,</span>
<span id="cb1-29">        )</span>
<span id="cb1-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> log_prob, intermediat_results</span></code></pre></div>
<p>where the prior can be any base distribution inplemented in <a href="https://pytorch.org/docs/stable/distributions.html">torch.distributions</a>, and flows can be any module that statisfy the NF’s properties.</p>
<p>Suppose we are given a 1D dataset, as shown in Fig. [2.a]. We can fit an NF to the underlying probability distribution <img src="https://latex.codecogs.com/png.latex?p(x)"> of the dataset. To successfully learn the density of the dataset, we need a base distribution (let’s say a Beta distribution parameterized by <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%202"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%205">) and a functional definition for our flow. In this case, let’s use the cumulative distribution function of a Gaussian Mixture Model (GMM) with 4 different components:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NormalizingFlow(</span>
<span id="cb2-2">    prior<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Beta(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5.0</span>),</span>
<span id="cb2-3">    flows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>nn.ModuleList([</span>
<span id="cb2-4">        GMMFlow(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-5">    ]),</span>
<span id="cb2-6">)</span>
<span id="cb2-7">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.AdamW(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">5e-3</span>, weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-5</span>)</span></code></pre></div>
<p>With these ingredients, we can train the model by minimizing the negative log-likelihood using stochastic gradient descent (SGD):</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(epochs):</span>
<span id="cb3-2">    model.train()</span>
<span id="cb3-3"></span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> idx, (x, y) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dataloader):</span>
<span id="cb3-5">        optimizer.zero_grad()</span>
<span id="cb3-6">        log_prob, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(x)</span>
<span id="cb3-7">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>log_prob.mean()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># nll</span></span>
<span id="cb3-8">        loss.backward()</span>
<span id="cb3-9">        nn.utils.clip_grad_norm_(model.parameters(), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-10">        optimizer.step()</span></code></pre></div>
<p>Note that no labels are used for training, as the objective is to directly maximize the predicted density of the dataset.</p>
<p>Figure&nbsp;2 shows the learned density and how the 4 different components are used to correctly model <img src="https://latex.codecogs.com/png.latex?p(x)">. While the same results might be achieved by using only 2 components, in the general case, the minimum number of needed components is not known a priori; thus using a larger number of components is a good practice.</p>
<p>Finally, Figure&nbsp;4 demonstrates how the learned model is able to map a dataset coming from an unknown density to the Beta distributin over-defined.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-1d-dataset" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1d-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/1d/dataset.png" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1d-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Training dataset build by sampling 750 elements from two distinct gaussian distributions
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-1d-fit" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1d-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/1d/fit-model.png" class="img-fluid figure-img" width="420">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1d-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: A normalizing flow fitted to the given dataset to learn p(x). The normalizing flow is composed by a beta distributon as a prior and as a gaussian mixture model with 4 different component as a flow.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="fig-1d-learned-transform" class="quarto-figure quarto-figure-center quarto-float anchored" width="600px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1d-learned-transform-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/1d/learned-transformation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1d-learned-transform-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Learned normalizing flow from the unknown distribution <img src="https://latex.codecogs.com/png.latex?p(x)"> to the choosen prior <img src="https://latex.codecogs.com/png.latex?p(z)">.
</figcaption>
</figure>
</div>
<p>Full code is contained in the following <a href="https://github.com/andompesta/pytorch-normalizing-flows/blob/main/nf_demo.ipynb">notebook</a>.</p>
</section>
<section id="d-training-example" class="level3">
<h3 class="anchored" data-anchor-id="d-training-example">2D Training Example</h3>
<p>Consider a more intricate dataset, such as the famous 2 Moon dataset depicted in Fig. [3.a]. The objective here is to map samples from this dataset into a latent variable that conforms to a Gaussian distribution.</p>
<p>In this context, relying solely on the cumulative distribution function of a Gaussian Mixture model as NF formulation may not provide the necessary expressiveness. While Neural Networks serve as powerful function approximators, they do not inherently guarantee the conditions required by a normalizing flow. Furthermore, computing the determinant of a linear layer within a neural network is computationally expensive.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-2d-moon" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-width="300px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2d-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/2d/dataset.png" class="img-fluid figure-img" data-fig-width="300px">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2d-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 2D Moon dataset.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-2d-moon-flow" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-width="500px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2d-moon-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/2d/2d_moon_flow.gif" class="img-fluid figure-img" data-fig-width="500px">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2d-moon-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Gif of all the steps needed by the normalization flow to map the 2 Moon dataset into a Gaussian distribution.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>In recent years, <strong>Coupling layers</strong> <span class="citation" data-cites="dinh2016density">[4]</span> have emerged as effective solutions for Normalizing Flows. They prove efficient both during sampling and training, while delivering competitive performances. The fundamental idea involves splitting the input variables of the i-th layer into equally sized groups:</p>
<ul>
<li>The first group of input variables (<img src="https://latex.codecogs.com/png.latex?z_i%5B0%5D,%20...,%20z_i%5Bd%5D">) is considered constant during the i-th layer<sup>1</sup>.</li>
<li>The second group of parameters (<img src="https://latex.codecogs.com/png.latex?z_%7Bi%7D%5Bd+1%5D,%20...,%20z_%7Bi%7D%5BD%5D">) undergoes transformation by a Neural Network that depends solely on <img src="https://latex.codecogs.com/png.latex?z_%7Bi%7D%5B%5Cleq%20d%5D">.</li>
</ul>
<p>Mathematically, we can represent the transformation applied to all input variables in the i-th layer as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20z_%7Bi+1%7D%5B0%5D,%20...,%20z_%7Bi+1%7D%5Bd%5D%20&amp;%20=%20z_%7Bi%7D%5B0%5D,%20...,%20z_%7Bi%7D%5Bd%5D%20%5C%5C%0A%20%20%20%20d_%7Bi%7D%5Bd+1%5D,%20...,%20d_%7Bi%7D%5BD%5D,%20t_%7Bi%7D%5Bd+1%5D,%20...,%20t_%7Bi%7D%5BD%5D%20&amp;%20=%20f(z_%7Bi%7D%5B0%5D,%20...,%20z_%7Bi%7D%5Bd%5D;%20%5Ctheta_%7Bi%7D)%20%5C%5C%0A%20%20%20%20z_%7Bi+1%7D%5Bd+1%5D,%20...,%20z_%7Bi+1%7D%5BD%5D%20&amp;%20=%20(z_%7Bi%7D%5Bd+1%5D%20%5Ccdot%20d_%7Bi%7D%5Bd+1%5D)%20+%20t_%7Bi%7D%5Bd+1%5D,%20...,%20(z_%7Bi%7D%5BD%5D%20%5Ccdot%20d_%7Bi%7D%5BD%5D)%20+%20t_%7Bi%7D%5BD%5D%0A%5Cend%7Balign*%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f(%5Ccdot;%20%5Ctheta_i)"> is any neural network. Intuitively, a coupling layer is akin to an autoregressive layer, where the autoregressive mask only permits <img src="https://latex.codecogs.com/png.latex?z_%7Bi+1%7D%5B%3Ed%5D"> to depend on <img src="https://latex.codecogs.com/png.latex?z_%7Bi%7D%5B%5Cleq%20d%5D">.<br>
As shown in Figure&nbsp;7 and Figure&nbsp;8, the beauty of coupling layers lies in the ease of inverting their transformation. Given the initial conditiokn <img src="https://latex.codecogs.com/png.latex?z_%7Bi+1%7D%5B%5Cleq%20d%5D%20=%20z_%7Bi%7D%5B%5Cleq%20d%5D">, it is possible to derive the affine parameters <img src="https://latex.codecogs.com/png.latex?d_%7Bi%7D%5B%3E%20d%5D"> and <img src="https://latex.codecogs.com/png.latex?t_%7Bi%7D%5B%3E%20d%5D"> by directly applying <img src="https://latex.codecogs.com/png.latex?f(%5Ccdot;%20%5Ctheta_i)"> to <img src="https://latex.codecogs.com/png.latex?z_%7Bi+1%7D%5B%5Cleq%20d%5D">.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-coupling-forward" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coupling-forward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/coupling_layer-forward.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coupling-forward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Coupling layer forward pass.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-coupling-backward" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coupling-backward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/coupling_layer-backward.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coupling-backward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Coupling layer backward pass.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>By construction, the Jacobian matrix of any such layer is lower triangular, following the structure:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AJ_%7Bz_%7Bi+1%7D%7D(z_%7Bi%7D)%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%5Cmathbf%7BI%7D%20&amp;%20%5Cmathbf%7BO%7D%20%5C%5C%0A%20%20%20%20%5Cmathbf%7BA%7D%20&amp;%20%5Cmathbf%7BD%7D%20%5C%5C%0A%5Cend%7Bbmatrix%7D.%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D"> is an identity matrix of size <img src="https://latex.codecogs.com/png.latex?d%20%5Ctimes%20d">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BO%7D"> is a zeros matrix of size <img src="https://latex.codecogs.com/png.latex?d%20%5Ctimes%20(D-d)">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is a full matrix of size <img src="https://latex.codecogs.com/png.latex?(D-d)%20%5Ctimes%20d"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D"> is a diagonal matrix of shape <img src="https://latex.codecogs.com/png.latex?(D-d)%20%5Ctimes%20(D-d)">. The determinant of such a matrix is formed by the product of the diagonal elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D">, making it efficient to compute.</p>
<p>Figure&nbsp;9 illustrates the dynamics of a NF trained on a 2 Moon dataset. Note how the final latent space (step 4) conforms to a Gaussian distribution.</p>
<div id="fig-2d-normalizing-flow" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2d-normalizing-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/normalization_flow/img/2d/training-process.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2d-normalizing-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Normalizing flow from a 2 moon dataset to the guassian prior visualized step by step. Bottom right picture shows the distribution of the final latent variable extracted by the flow, demonstrating that it is clrearly Gaussian.
</figcaption>
</figure>
</div>
<p>Finally, a simple implementation of a coupling layer in pytorch is proviceded as follow:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> CouplingFlow(nn.Module):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(</span>
<span id="cb4-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb4-4">        dim: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb4-5">        hidden_dim: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb4-6">        mask: Tensor,</span>
<span id="cb4-7">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb4-8">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb4-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span> dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dim must be even"</span></span>
<span id="cb4-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span> dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> mask.size(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mask dimension must equal dim"</span></span>
<span id="cb4-11"></span>
<span id="cb4-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dim</span>
<span id="cb4-13">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hidden_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hidden_dim</span>
<span id="cb4-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.register_buffer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mask"</span>, mask)</span>
<span id="cb4-15"></span>
<span id="cb4-16">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb4-17">            nn.Linear(dim, hidden_dim),</span>
<span id="cb4-18">            nn.LeakyReLU(),</span>
<span id="cb4-19">            nn.Linear(hidden_dim, dim),</span>
<span id="cb4-20">        )</span>
<span id="cb4-21"></span>
<span id="cb4-22">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb4-23">        x_masked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask</span>
<span id="cb4-24">        output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net(x_masked)</span>
<span id="cb4-25">        log_d, t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output.chunk(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-26">        z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_masked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> torch.exp(log_d) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> t))</span>
<span id="cb4-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> z</span>
<span id="cb4-28"></span>
<span id="cb4-29">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> log_abs_det_jacobian(</span>
<span id="cb4-30">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb4-31">        x: Tensor,</span>
<span id="cb4-32">        z: Tensor,</span>
<span id="cb4-33">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb4-34">        x_masked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mask</span>
<span id="cb4-35">        log_d, t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net(x_masked).chunk(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> log_d.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</section>
</section>
<section id="credits" class="level1">
<h1>Credits</h1>
<p>The content of this post is based on the lectures and code of <a href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Pieter Abbeel</a>, <a href="https://groups.csail.mit.edu/gdpgroup/6838_spring_2021.html">Justin Solomon</a> and <a href="https://github.com/karpathy/pytorch-normalizing-flows">Karpathy’s</a> tutorial. Moreover, I want to credit <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lil’Long</a> and <a href="https://blog.evjang.com/2018/01/nf1.html">Eric Jang</a> for their amazing tutorials. For example, the pioneering work done by Dinh et. al. is the first to leverage transformations with triangular matrix for efficent determinatnt computation.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-normalization-flow-review" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Papamakarios G, Nalisnick E, Rezende DJ, Mohamed S, Lakshminarayanan B (2021) Normalizing flows for probabilistic modeling and inference. The Journal of Machine Learning Research 22(1):2617–2680</div>
</div>
<div id="ref-dinh2014nice" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Dinh L, Krueger D, Bengio Y (2014) Nice: Non-linear independent components estimation. arXiv preprint arXiv:14108516</div>
</div>
<div id="ref-change-of-variable" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><span>C</span>hange of <span>V</span>ariables <span>T</span>heorem – from <span>W</span>olfram <span>M</span>ath<span>W</span>orld — mathworld.wolfram.com</div>
</div>
<div id="ref-dinh2016density" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Dinh L, Sohl-Dickstein J, Bengio S (2016) Density estimation using real nvp. arXiv preprint arXiv:160508803</div>
</div>
<div id="ref-grathwohl2018ffjord" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Grathwohl W, Chen RT, Bettencourt J, Sutskever I, Duvenaud D (2018) Ffjord: Free-form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:181001367</div>
</div>
<div id="ref-hirschorn2023normalizing" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Hirschorn O, Avidan S (2023) Normalizing flows for human pose anomaly detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp 13545–13554</div>
</div>
<div id="ref-kirichenko2020normalizing" class="csl-entry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Kirichenko P, Izmailov P, Wilson AG (2020) Why normalizing flows fail to detect out-of-distribution data. Advances in neural information processing systems 33:20578–20589</div>
</div>
<div id="ref-kingma2018glow" class="csl-entry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Kingma DP, Dhariwal P (2018) Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems 31</div>
</div>
<div id="ref-oord2016wavenet" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Oord A van den, Dieleman S, Zen H, et al (2016) Wavenet: A generative model for raw audio. arXiv preprint arXiv:160903499</div>
</div>
<div id="ref-kim2018flowavenet" class="csl-entry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Kim S, Lee S, Song J, Kim J, Yoon S (2018) FloWaveNet: A generative flow for raw audio. arXiv preprint arXiv:181102155</div>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Here we introduce the notation <img src="https://latex.codecogs.com/png.latex?z_%7Bi%7D%5Bd%5D"> as indicating the <img src="https://latex.codecogs.com/png.latex?d"> dimention of the latent variable at the i-th layer of a flow (<img src="https://latex.codecogs.com/png.latex?z_%7Bi%7D">).↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Deep Learning</category>
  <guid>https://andompesta.github.io/posts/normalization_flow/</guid>
  <pubDate>Fri, 17 Nov 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Finetune Large Models</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/finetune/</link>
  <description><![CDATA[ 




<p>Nowadays, it is common practice to develop new machine learning projects starting from a large pre-trained model and fine-tuning it to the task at hand. <a href="https://youtu.be/WHoWGNQRXb0?t=170">Sam Altman</a>, at the time of writing the CEO of OpenAI, recently mentioned that he envisions a feature where the most valuable startups are the ones capable of adapting publicly available foundation models to specific domains; rather than training a proprietary model from scratch. Thus, tuning or fine-tuning large models is a key capability that machine learning practitioners need to learn as much as being able to train a deep neural network was a major skill that each scientist had perfected in the last few years.</p>
<div id="fig-intro-finetuning" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-finetuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/finetune/img/intro.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-finetuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Model finetuning.
</figcaption>
</figure>
</div>
<p>On the web, there are a plethora of <a href="http://karpathy.github.io/2019/04/25/recipe/">recipes for training neural networks</a> (thanks Karpathy you saved me multiple times). Instead, there are only limited amount of resources tackling the fine-tuning problem. To this end, this article aims at describing the strategy that I adopt when fine-tuning a large Transformer model.</p>
<section id="the-procedure" class="level1">
<h1>The Procedure</h1>
<p>The procedure here-described is built on the key principle that: <em>the fine-tuning step should be the continuation of the training phase but on a different dataset</em>. The main focus is on avoiding the introduction of any unnecessary difference between the original and fine-tuning tasks at every stage of the pipeline. Therefore we will minimise the number of parameters that need to be adapted: resulting in greater transferability between tasks.</p>
<section id="dataset-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="dataset-preprocessing">1. Dataset Preprocessing</h2>
<p>Often catastrophic forgettin is caused by a bad implementation of the preprocessing steps applied to the fine-tuning dataset.</p>
<p>On the one hand, for natural language processing tasks additional cares need to be taken during the tokenization of the input text. Specifically, always double-check that the correct tokenizer is applied, and that the correct PAD-token is adopted. <a href="https://huggingface.co/">HuggingFace</a> made a terrific effort in providing a bug-free implementation of most Transformers models, but sub-word tokenizers remain a challenging module to operate. Another error-prone transformation is the creation of the input mask: always assert that causal and padding masks are correctly combined and applied in the correct layer.</p>
<p>On the other hand, in computing vision tasks, ensure that the proper transformations are applied to the input images. Order of the input channel, and normalization and interpolation strategy for the resizing steps are among the most error-prone functionality to correctly re-create.</p>
<p>Finally, if you are working with graph structures, double-check how you uniformize to a fixed size a batch of nodes having a different amount of neighbours.</p>
</section>
<section id="optimizer" class="level2">
<h2 class="anchored" data-anchor-id="optimizer">2. Optimizer</h2>
<p>According to the key principle of this article, the fine-tuning procedure should use the same optimizer adopted during training. While this is not always possible, most of the foundation models <span class="citation" data-cites="bommasani2021opportunities">[1]</span> are trained by applying the AdamW optimize <span class="citation" data-cites="loshchilov2017decoupled">[2]</span>. AdamW is a variation of the well-known Adam algorithm that better generalize to unknown examples thanks to the decoupling of the main loss and the regularization term known as <em>weight decay</em>. Note that back in the day, PyTorch did not provide a proper implementation of the AdamW. Thus, multiple open-source projects provided their implementation. Across the many, the one provided by Meta’s <a href="https://github.com/facebookresearch/fairseq/blob/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/optim/adam.py#L110">fairseq</a> is highly reliable and I still use it nowadays.</p>
</section>
<section id="weight-decay" class="level2">
<h2 class="anchored" data-anchor-id="weight-decay">3. Weight Decay</h2>
<p>By default, AdamW will apply the weight decay mechanism to all models’ parameters, yet in most cases, weight decay is <strong>NOT applied</strong> to all weights. The weight decay objective is to regularise the training process toward learning models with weights smaller in magnitude. To this end, biases and gains are usually not included in the weight decay loss for multiple reasons:</p>
<ol type="1">
<li>Biases are used to shift the activation function of a neuron and do not typically have a significant impact on the overall complexity of the model. As a result, applying weight decay to the biases would not have a significant effect on the model’s ability to overfit.</li>
<li>Gains are commonly used in normalization layers to enable high expressivity while ensuring gaussian-like activations. Thus, biases do not typically have a significant impact on the model complexity. On the contrary, weight decay applied to gains might limit the expressivity of a model resulting in underfitting.</li>
<li>Overall, weight decay is typically applied only to the weights of a model, rather than the biases or gains, to encourage the model to use small weights and reduce the risk of overfitting. In PyTorch this is commonly achieved by creating two groups of parameters for the optimizers.</li>
</ol>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_group_params(</span>
<span id="cb1-2">    model: nn.Module,</span>
<span id="cb1-3">    weight_decay: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>,</span>
<span id="cb1-4">    no_decay_patterns: Optional[List[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bias"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ln_.+weight"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ln_.+bias"</span>],</span>
<span id="cb1-5">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, Any]]:</span>
<span id="cb1-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""function generating the appropriate group's parameters to optimize</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :param model: model to train</span></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :type model: nn.Module</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :param weight_decay: weight decay factor, defaults to 0.01 </span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :type weight_decay: float</span></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :param no_decay_patterns: regex patterns used to identify parameters for witch no decay is used, defaults to ["bias", "ln_.+weight", "ln_.+bias"]</span></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :type no_decay_patterns: Optional[List[str]], optional</span></span>
<span id="cb1-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :return: two groups of parameters to optimize</span></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    :rtype: list[dict[str, Any]]</span></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb1-17">    optimizer_grouped_parameters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb1-18">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(</span>
<span id="cb1-19">            params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb1-20">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># parameters with weight decay</span></span>
<span id="cb1-21">                p <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> n, p <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> model.named_parameters() <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(</span>
<span id="cb1-22">                    [re.search(pattern, n) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> pattern <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> no_decay_patterns]</span>
<span id="cb1-23">                )</span>
<span id="cb1-24">            ],</span>
<span id="cb1-25">            weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>weight_decay,</span>
<span id="cb1-26">        ),</span>
<span id="cb1-27">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(</span>
<span id="cb1-28">            params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb1-29">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># parameters without weight decay</span></span>
<span id="cb1-30">                p <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> n, p <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> model.named_parameters() <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(</span>
<span id="cb1-31">                    [re.search(pattern, n) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> pattern <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> no_decay_patterns]</span>
<span id="cb1-32">                )</span>
<span id="cb1-33">            ],</span>
<span id="cb1-34">            weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.</span>,</span>
<span id="cb1-35">        )</span>
<span id="cb1-36">    ]</span>
<span id="cb1-37">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> optimizer_grouped_parameters</span></code></pre></div>
<p>If you don’t like regex patterns, karpathy provided a nite alternative implementation in its <a href="https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L224">miniGPT</a> repository.</p>
</section>
<section id="learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate">4. Learning Rate</h2>
<p>While finetuning your models, you will be surprised but you will find out that the best-performing learning rate will be quite small. A sound starting point is a learning rate of <code>5e-5</code>. As overmentioned, such a learning rate is extremely small compared to when you train a neural network from scratch, but it will fit the new data well and prevent catastrophic forgetting.</p>
<p>Moreover, in many cases, it is beneficial to use a learning rate scheduler while fine-tuning your model. Across the many schedulers a linear decay scheduler with a warmup phase works well while being simple to implement.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.optim.lr_scheduler <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LambdaLR</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.optim.optimizer <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Optimizer</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_linear_scheduler_with_warmup(</span>
<span id="cb2-5">    optim: Optimizer,</span>
<span id="cb2-6">    num_warmup_step: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-7">    num_training_step: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb2-8">    last_epoch: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-9">):</span>
<span id="cb2-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    get a scheduler that linearly increase the learning data between [0, num_warmup_steps) and then linearly decrease</span></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    it between [num_warmup_steps, num_training_step).</span></span>
<span id="cb2-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb2-14"></span>
<span id="cb2-15">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> lr_lambda(current_step):</span>
<span id="cb2-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> current_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> num_warmup_step:</span>
<span id="cb2-17">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(current_step) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, num_warmup_step))</span>
<span id="cb2-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(</span>
<span id="cb2-19">            <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>,</span>
<span id="cb2-20">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(num_training_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_step) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span></span>
<span id="cb2-21">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, num_training_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> num_warmup_step)),</span>
<span id="cb2-22">        )</span>
<span id="cb2-23"></span>
<span id="cb2-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> LambdaLR(optim, lr_lambda, last_epoch)</span></code></pre></div>
<p>The assumption is to linearly increase the learning rate during the warmup phase (usually lasting for one epoch) to align the model’s parameters to the new task. Afterwards, the learning rate is reduced to allow for fine-grained adjustments of the weights. Figure&nbsp;2 shows how the learning rate is adjusted across different epochs.</p>
<p>Note that adopting a learning rate scheduler is beneficial, but it introduces many challenges for reproducibility. Schedulers adapt the learning rate according to the epoch and the dataset size. Thus, by simply modifiing the dataset you will likely change your scheduler. Be carefull !!!</p>
<div id="fig-scheduler" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scheduler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/finetune/img/scheduler.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scheduler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Example of linear scheduler with warmup.
</figcaption>
</figure>
</div>
</section>
<section id="parameter-feezing" class="level2">
<h2 class="anchored" data-anchor-id="parameter-feezing">5. Parameter feezing</h2>
<p>To achieve good transferability and avoiding chatastorfic forgetting it is key to implement a proper parameter freezing stragegy. As over-mentioned it is important to minimise the number of parameter to ajdust during the fine-tuining phase to guarantee transferability across tasks. To this end, <span class="citation" data-cites="zaken2021bitfit">[3]</span> proposed to only adjust bias terms to achieve a good tradeoff between learning speed and transferabilty performance. Similarly, in a multilingual setting it is common to keep frozen the embedding layers as well as the first few layers of a multilingual-BERT models (<span class="citation" data-cites="pires2019multilingual">[4]</span>) demonstrated that the first 8 layers need to be keep frozen, while the remaining are tasks-specific). Finally, the same effect is also observable in computer vision assignment: in <span class="citation" data-cites="kumar2022fine">[5]</span> it is reported how freezing the lowest layers is key to obtain good generalization.</p>
<p>Freezing parameters is extramly easy in PyTorch thanks to the <code>requires_grad</code> flag associated to each model’s parameter:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> unfreeze_layer_params(</span>
<span id="cb3-2">    model: nn.Module,</span>
<span id="cb3-3">    freeze_patterns: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".*bias$"</span>],</span>
<span id="cb3-4">):</span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> n, p <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> model.named_parameters():</span>
<span id="cb3-6">        </span>
<span id="cb3-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>([re.search(pattern, n) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> pattern <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> freeze_patterns]):</span>
<span id="cb3-8">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># unfreeze biases parameter</span></span>
<span id="cb3-9">            p.requires_grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb3-10">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"unfreeze -&gt; </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-11"></span>
<span id="cb3-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb3-13">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># freeze remaining parameters</span></span>
<span id="cb3-14">            p.requires_grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb3-15">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"FREEZE -&gt; </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</section>
<section id="overfit" class="level2">
<h2 class="anchored" data-anchor-id="overfit">6. Overfit</h2>
<p>Finally, it is recommended to evaluate the training scripts on a single batch before executing the fine-tuning procedure. The benefits of this exercise are twofold:</p>
<ol type="1">
<li>it enables us to detect errors or hardware limitations in the fine-tuning procedure at a low cost;</li>
<li>it ensures that our model will be able to fit the fine-tuning dataset.</li>
</ol>
<p>If the implementation is correct we expect to overfit the training batch in a few gradient updates: resulting in 0 training loss and 100 % training accuracy. Similarly, as the model keep overfitting a single training batch we expect the validation performances to degrade progressively. Figure&nbsp;3 displays a good example of how the metrics should look at this stage.</p>
<div id="fig-overfit" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-overfit" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-train" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-train-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/finetune/img/train_overfit.png" class="img-fluid figure-img" data-ref-parent="fig-overfit">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-train-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Training loss and accuracy for a single batch.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-overfit" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-val" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-val-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/finetune/img/eval_overfit.png" class="img-fluid figure-img" data-ref-parent="fig-overfit">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-val-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Validation loss and accuracy for a single batch.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3
</figcaption>
</figure>
</div>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-bommasani2021opportunities" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Bommasani R, Hudson DA, Adeli E, et al (2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:210807258</div>
</div>
<div id="ref-loshchilov2017decoupled" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Loshchilov I, Hutter F (2017) Decoupled weight decay regularization. arXiv preprint arXiv:171105101</div>
</div>
<div id="ref-zaken2021bitfit" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Zaken EB, Ravfogel S, Goldberg Y (2021) Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:210610199</div>
</div>
<div id="ref-pires2019multilingual" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Pires T, Schlinger E, Garrette D (2019) How multilingual is multilingual BERT? arXiv preprint arXiv:190601502</div>
</div>
<div id="ref-kumar2022fine" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Kumar A, Shen R, Bubeck S, Gunasekar S (2022) How to fine-tune vision models with sgd. arXiv preprint arXiv:221109359</div>
</div>
</div>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://andompesta.github.io/posts/finetune/</guid>
  <pubDate>Thu, 05 Jan 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Efficient and Scalable Machine Learning Pipelines</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/data_preprocessing/</link>
  <description><![CDATA[ 




<p>Jobs related to machine learning usually require managing massive datasets. A well-established rule of thumb that applies to most machine learning projects is that the larger and cleaner the dataset, the better the performance. Thus, the problem of preprocessing large amounts of data and efficiently feeding the produced dataset into the training pipeline emerges. While developing fancy models is a fun task for which limitless resources are available on the web, the ML community needs to cover better the topic of streamlining data preprocessing and ingestion pipelines. Backed by the fast iteration philosophy, this document aims to find the most efficient training process to minimise the cost of experimentation as more experiment results in better performance: as <a href="https://www.youtube.com/embed/E7MQb9Y4FAE?start=330&amp;autoplay=1">Elon Musk says</a>, “high production rate solve many ills”.</p>
<p>To be as general as possible, this article follow the work done by <span class="citation" data-cites="chia2022contrastive">[1]</span> and will focus on finetuning a <a href="https://openai.com/blog/clip/">CLIP-like model</a> on the <a href="https://eval.ai/web/challenges/challenge-page/1721/overview">farfetch dataset</a>. This task’s choice enables us to preprocess a large number of images as well as text, which are the most common data-type currently used in machine learning.</p>
<section id="data-preprocessing" class="level1">
<h1>Data Preprocessing</h1>
<p>It is not a secret that the fuel of machine learning applications is data. For example, online advertising leverage behavioural data to personalise the displayed products, translation services leverage parallel documents, and research engine use users’ feedback to learn better query-document rankings. However, generating these datasets from the raw events is a challenging task. It is common knowledge that the larger the company, the messier the data; thus, carefully crafted transformation jobs are needed. Data preprocessing is the tedious step of collecting and integrating events from raw data sources and producing a well-formatted dataset that a training script can consume. As such, it is usually divided into two stages:</p>
<ul>
<li><strong>Data engineering</strong> is the process of combining different data sources into a prepared dataset. During this process, we want to ensure that data sources are appropriately integrated and aggregated to the right granularity, possible errors are carefully handled, noise examples are removed, and the resulting dataset conforms to a well-defined schema.</li>
<li><strong>Feature engineering</strong> aims at converting all the columns of the generated dataset to the appropriate numerical format needed by the model. As such, this process is sometimes postponed to the data loading and augmentation process.</li>
</ul>
<p>Nowadays, there is a multitude of libraries that can be used for data preprocessing. <a href="https://spark.apache.org/">Apache Spark</a> appears to be the best tool for data preprocessing jobs. It provides a Pandas-like interface; it is distributed so it can scale vertically and horizontally, integrated with the most common cloud environments, and supports batch and streaming data sources. While there a multiple approaches to speed up a preprocessing job, this article only focuses on comparing the different runtime engines supported by Spark:</p>
<ul>
<li>base engine designed to run on CPU clusters used as the baseline,</li>
<li><a href="https://www.databricks.com/product/photon">photon</a> engigne: engine: a C++ implementation of Spark that leverages vectorization operation to reduce the computation time,</li>
<li><a href="https://www.nvidia.com/en-us/deep-learning-ai/software/rapids/">rapids</a> engine: an emerging GPU-based environment provided by NVIDIA.</li>
</ul>
<p>To compare the different engines, the text preprocessing task is used for benchmarking. As above mentioned, the dataset is the farfetch dataset that contains about 400K products, but about 300K are used for this test due to missing meaningful descriptions. Text preprocessing aims to clean, tokenize, pad and add special tokens to each product description. For consistency, we adopted the same tokenizer used in the original <a href="https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py">CLIP implementation</a>. However, a pyspark UDF is used to apply the preprocessing to the original dataframe:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> encode(value):</span>
<span id="cb1-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Petastorm specific enconding function for numpy array datatype.</span></span>
<span id="cb1-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Read following section for a better explanation.</span></span>
<span id="cb1-4">    memfile <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> io.BytesIO()</span>
<span id="cb1-5">    np.save(memfile, value)</span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bytearray</span>(memfile.getvalue())</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_preprocess_data_fn(</span>
<span id="cb1-9">        vocab_path: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb1-10">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Callable[[Iterator[pd.DataFrame]], Iterator[pd.DataFrame]]:</span>
<span id="cb1-11"></span>
<span id="cb1-12">    tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SimpleTokenizer(</span>
<span id="cb1-13">        vocab_path,</span>
<span id="cb1-14">        dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb1-15">    )</span>
<span id="cb1-16"></span>
<span id="cb1-17">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> preprocess_data(</span>
<span id="cb1-18">        dataframe_batch_iterator: Iterator[pd.DataFrame]</span>
<span id="cb1-19">    ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Iterator[pd.DataFrame]:</span>
<span id="cb1-20">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        UDF function to tokenize the items descriptions. Resulting</span></span>
<span id="cb1-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        numpy array is encoded according to petestorm configuration.</span></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb1-24"></span>
<span id="cb1-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> dataframe_batch <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> dataframe_batch_iterator:</span>
<span id="cb1-26">            product_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-27">            descriptions_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-28">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> row <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> dataframe_batch.itertuples():</span>
<span id="cb1-29">                product_ids.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(row.product_id))</span>
<span id="cb1-30"></span>
<span id="cb1-31">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># tokenize description</span></span>
<span id="cb1-32">                description_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(row.description)</span>
<span id="cb1-33">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># description_ids = description_ids.astype(np.int64)</span></span>
<span id="cb1-34">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># enconde numpy array as byte-array</span></span>
<span id="cb1-35">                descriptions_ids.append(encode(description_ids))</span>
<span id="cb1-36"></span>
<span id="cb1-37">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">yield</span> pd.DataFrame({</span>
<span id="cb1-38">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"product_id"</span>: product_ids,</span>
<span id="cb1-39">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"description_ids"</span>: descriptions_ids,</span>
<span id="cb1-40">            })</span>
<span id="cb1-41"></span>
<span id="cb1-42">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> preprocess_data</span>
<span id="cb1-43"></span>
<span id="cb1-44"></span>
<span id="cb1-45"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> main():</span>
<span id="cb1-46">    ...</span>
<span id="cb1-47">    preprocess_data_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_preprocess_data_fn(</span>
<span id="cb1-48">    path<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>vocab_path</span>
<span id="cb1-49">    )</span>
<span id="cb1-50"></span>
<span id="cb1-51">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># generate text encoding and image preprocessing</span></span>
<span id="cb1-52">    farfetch_description <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> farfetch_description.mapInPandas(</span>
<span id="cb1-53">        preprocess_data_fn,</span>
<span id="cb1-54">        StructType([</span>
<span id="cb1-55">            StructField(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"product_id"</span>, IntegerType()),</span>
<span id="cb1-56">            StructField(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"description_ids"</span>, BinaryType()),</span>
<span id="cb1-57">        ])</span>
<span id="cb1-58">    )</span>
<span id="cb1-59">    ...</span></code></pre></div>
<p>Cluster configuration and experiment results are reported in Table&nbsp;1 and Figure&nbsp;1.</p>
<div id="tbl-setup" class="hover quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Clstuer definitions used for data preprocessing benchmarking.
</figcaption>
<div aria-describedby="tbl-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table">
<thead>
<tr class="header">
<th>CPU</th>
<th>Memory</th>
<th>GPU</th>
<th>Num Workers</th>
<th>Price (DBU/h)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>16</td>
<td>64 GB</td>
<td>0</td>
<td>8</td>
<td>13.7</td>
</tr>
<tr class="even">
<td>Photon-accelerated</td>
<td>16</td>
<td>64 GB</td>
<td>0</td>
<td>8</td>
</tr>
<tr class="odd">
<td>GPU-accelerated</td>
<td>16</td>
<td>64 GB</td>
<td>NVIDIA T4</td>
<td>8</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The experiment result shows that a reduction of 31.5 to 49% in processing time achievable just by adopting an accelerated engine. Rapids is the most promising option as it provides a 2x speedup at a competitive price w.r.t. the CPU baseline. However, not that in this test, the dataset size is small compared to the available cluster memory, and special care is usually required when working with GPUs as it is pretty common to get out-of-bound memory errors.</p>
<div id="fig-data-procseeing" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-procseeing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/data_preprocessing/img/text_preprocessing_benchmark.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-procseeing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Data preprocessing benchmark.
</figcaption>
</figure>
</div>
</section>
<section id="data-ingestion" class="level1">
<h1>Data Ingestion</h1>
<p>Data ingestion is another crucial component for any successful ML project. Too little credit has been given to the developers of <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">Tensorflow Dataset</a> and <a href="https://pytorch.org/docs/stable/data.html">PyTorch DataLoaders</a> as they provide common APIs for loading and manipulating the created datasets. However, these data-loaders utilities are limited when it comes to processing large datasets that do not fit the memory and that need to operate in a distributed environment. On the one hand, Tensorflow Dataset is extremely efficient when working with TFRecords, but does not support parquet data format and requires a significant amount of boilerplate code to be parsed. On the other hand, PyTorch Dataloaders only focuses on sampling and batching the data, leaving the scope of loading the dataset into the memory for rapid access indexed access to the user.</p>
<p>Due to these limitations, multiple third-party solutions arose. As shown in Figure&nbsp;2 in machine learning there are two main type of datasets: - datasets composed by raw complex data formats such as images, texts or audios are common in deep learning; - tabular dataset are composed by multiple columns of scalar values representing handcrafted features are usually used in reccomandation applications. As this two datatypes present different challences, distinct benchmarks are conducted to understand which are the best practices to follow in differnet scenarios.</p>
<div id="fig-datasets-type" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-datasets-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/data_preprocessing/img/datasets-type.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-datasets-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Visual representations of the different dataset types. Note that for NLP or CV tasks the inputs are in a raw data format, but the models are complex. Instead, RecSys systems are limited by the I/O operations needed to process the large amount of user-items pairs; rather than the model complexity. Figure adapted from <a href="https://medium.com/nvidia-merlin/why-isnt-your-recommender-system-training-faster-on-gpu-and-what-can-you-do-about-it-6cb44a711ad4">Why isn’t your recommender system training faster on GPU? (And what can you do about it?)</a>
</figcaption>
</figure>
</div>
<section id="complex-raw-datasets" class="level2">
<h2 class="anchored" data-anchor-id="complex-raw-datasets">Complex Raw Datasets</h2>
<p>These types of datasets are composed of a mixture of texts, images and audio in the form of large multi-dimensional arrays. The large input space requires deep models to learn good embeddings of the data. Thus, it is assumed that the forward and backward passes of the model is the main bottelneck of the training phase due to the model’s complexity. Instead, the data loading process is assumed to be comparatively less expensive.</p>
<p>Based on these assumptions, a common pattern emerged across the dataloaders solutions: spawning thread-based or process-based workers to ingest the data while the GPUs are used for training. Among the others, <a href="https://petastorm.readthedocs.io/en/latest/index.html">Petastorm</a>, a general-purpose solution provided by Uber that easily integrates with Databricks, follows exactly this pattern. At the core of Petastorm, there is the <a href="https://petastorm.readthedocs.io/en/latest/_modules/petastorm/codecs.html?highlight=Codec">Codecs</a> concept, an API that specifies methods to encode and decode custom datatypes. For example, numpy arrays and images, two types not supported by Spark, are encoded by Petastorm into a Spark DataFrames as BinaryType and decoded at training time. As above mentioned, when a new column containing a non-native datatype is added to the DataFrame, the <a href="https://github.com/uber/petastorm/blob/170b22a18ee1c0346d2b289f096804e34a0c5d25/petastorm/codecs.py#L136">encode</a> function is applied to every row.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> encode(value)</span>
<span id="cb2-2">    memfile <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> BytesIO()</span>
<span id="cb2-3">    np.save(memfile, value)</span>
<span id="cb2-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bytearray</span>(memfile.getvalue())</span></code></pre></div>
<p>Similarly, once the dataset is stored on any distributed file system, the Petastorm <a href="https://github.com/uber/petastorm/blob/170b22a18ee1c0346d2b289f096804e34a0c5d25/petastorm/reader.py#L344">Reader</a> decodes each row of the dataset while feeding the data into the training pipeline.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> decode(value)</span>
<span id="cb3-2">    memfile <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> BytesIO(value)</span>
<span id="cb3-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.load(memfile)</span></code></pre></div>
<p>Yet, performing random access of distributed datasets containing large arrays is costly due to the multiple I/O operations involved. Thus, an evaluation of Petastorm dataloaders, on the dataset previously prepared is reported. The dataset consists of about 300K image/text pairs. Images are represented as <code>3 x 224 x 224</code> arrays, while text by a list of <code>77</code> elements. Each batch is composed of 64 examples. The objective is to find the best worker-type and number of worker combinations possible. Thus, a grid search is reported in [fig-dataloader-cn-nlp-bnc], where thread-based and process-based workers are compared with a setting that uses 5, 10 and 20 workers.</p>
<div id="fig-dataloader-cn-nlp-bnc" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dataloader-cn-nlp-bnc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/data_preprocessing/img/dataloader-cn-nlp-bnc.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dataloader-cn-nlp-bnc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Bencharking of dataloaders for complex raw datatype. Upper figure represent the overall execution time for a single epoch, while bottom figure shows the amount of batch per seconds (BpS) processed.
</figcaption>
</figure>
</div>
<p>The results show that a setting with 5 processes is the fastest as it can process <code>74</code> batches per second (BpS), which is a <code>172 %</code> improvement over the default configuration (threaded with 10 workers). <code>74</code> batches per second might seem like a bad result, but the computational cost of a deep model would likely be an order of magnitude larger, even if working in a data-parallel setting. Thus, most of the research focuses on speeding up the model computational time with strategies like model pruning, mixed-precision, ecc.</p>
</section>
<section id="tabular-datasets" class="level2">
<h2 class="anchored" data-anchor-id="tabular-datasets">Tabular Datasets</h2>
<p>Tabular datasets are commonly found in recommendation systems (RecSys) applications where the objective is to score (user, item) or (query, document) pairs. RecSys have key differences w.r.t. other deep learning applications:</p>
<ul>
<li>many recommendation applications need to perform in a real-time environment; thus the models need to satisfy tight latency constraints;</li>
<li>the datasets are usually large since the collection of (weakly) labelled examples is inexpensive;</li>
<li>the inputs are composed of a large set of handcrafted features.</li>
</ul>
<p>In the RecSys settings, efficient data-loading pipelines are an extremely important component of the training phase as the computational cost of the model is relatively small w.r.t. the loading operations. Note that, this is the exact opposite of the traditional deep learning environment, thus the multi-worker solution might perform poorly. To this end, custom-designed solutions for tabular datasets such as <a href="https://github.com/NVIDIA-Merlin">NVIDIA Merlin</a> emerged. Merlin is a complete toolkit for hardware-accelerated RecSys systems built on top of Dask, cuDF and Rapids. <a href="https://github.com/NVIDIA-Merlin/dataloader">Merlin dataloader</a> is a package specifcally built for the RecSys usecase; it leverages cuDF to efficiently load data into the GPUs and <a href="https://github.com/dmlc/dlpack">DLPack</a> to transfer the data to the appropriate backend framework (usually Tensorflow, PyTorch or JAX).</p>
<p>To evaluate the importance of having efficient data-loading solutions in this setting, a benchmark of Petastorm, Tensorflow Datasets with TFRecords and Merlin dataloader is conducted. The dataset used for the experiment is composed of ~8 M examples. Each row is composed of 100 columns containing only scalar values. A batch is 64 examples is loaded at each step.</p>
<p>The following setups are used to fine-tune each framework:</p>
<ul>
<li>Petastorm uses 5 or 10 workers in a thread or process-based solutions;</li>
<li>Tensorflow uses 5 or 10 workers for reading and parsing the TFRecords;</li>
<li>Merlin uses the default configuration.</li>
</ul>
<div id="fig-dataloader-tabular-bnc" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dataloader-tabular-bnc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/data_preprocessing/img/dataloader-tablular-bnc.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dataloader-tabular-bnc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Benchmarking of dataloaders for tabular datasets. The results demonstrate how the custom-design proposed by Merlin achieves 5x to 1000x better performance against other deep learning solutions.
</figcaption>
</figure>
</div>
<p>Figure&nbsp;4 highlights how a general-purpose solution like Petastorm does not fit the RecSys settings as it is more than <code>1000x</code> slower than Merlin. Tensorflow Datasets are showing decent performances, but handling TFRecords is challenging as they consume a large amount of disk space and need to know the dataset schema at parsing time. Without many surprises, Merlin demonstrates astonishing performances being more than <code>4000</code> batches per second and it is 10 times faster than Tensorflow while being almost a plug-and-play solution if the datasets are stored in a parquet format. Unfortunately, Merlin does not support any other datatype than numerical values; thus datasets containing strings and multi-dimensional arrays are not supported.</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-chia2022contrastive" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Chia PJ, Attanasio G, Bianchi F, et al (2022) Contrastive language and vision learning of general fashion concepts. Scientific Reports 12(1):18958</div>
</div>
</div>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://andompesta.github.io/posts/data_preprocessing/</guid>
  <pubDate>Sun, 13 Nov 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Data Structures</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/datastructure/</link>
  <description><![CDATA[ 




<p>Data scrtucures are efficent memory construct used to sotre and organize data in an efficent manner. Adopting the right data structure and having efficent access to the needed information is a fundamentala to build usable and scalable products.</p>
<section id="big-o-notation-and-asymptotic-analysis" class="level1">
<h1>Big-O Notation and asymptotic Analysis</h1>
<p>To evaluate the efficency of a data structure we need to evaluate the <strong>time</strong> and <strong>memory consumption</strong> requred to execute the algorithm. As the run-time depends on the input size, we will focus on the performance of the data structure when the inputs are infinitly large. The asymptotic notations is the mathematical tool used to perform this analysis, specifically we will focus on the <strong>Big-O</strong> notation that studies the behaviout of each algorithm in the worst-case scenarious. Thus, it indicates the complexity of an algirithm assuming inputs of size N with <img src="https://latex.codecogs.com/png.latex?%5Clim%20N%5Cto%5Cinfty">. Under this context constant factors are ignored as are dominated by N.</p>
<table class="table-striped table-hover table">
<caption>Time complexities</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th>Notation</th>
<th style="text-align: center;">Name</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="https://latex.codecogs.com/png.latex?O(1)"></td>
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Time_complexity#Constant_time">constant</a></td>
<td>Determining if a binary number is even or odd; Using a constant-size <a href="https://en.wikipedia.org/wiki/Lookup_table">lookup table</a></td>
</tr>
<tr class="even">
<td><img src="https://latex.codecogs.com/png.latex?O(%5Clog%20N)"></td>
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Logarithmic_time">logarithmic</a></td>
<td>Finding an item in a sorted array with a binary search or a balanced search tree as well as all operations in a binomial heap.</td>
</tr>
<tr class="odd">
<td><img src="https://latex.codecogs.com/png.latex?O(N)"></td>
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Linear_time">linear</a></td>
<td>Finding an item in an unsorted list or in an unsorted array; adding two <em>n</em>-bit integers by ripple carry.</td>
</tr>
<tr class="even">
<td><img src="https://latex.codecogs.com/png.latex?O(N%5Clog%20N)"></td>
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Linearithmic_time">linearithmic</a></td>
<td>Performing a <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">Fast Fourier Transform</a> or <a href="&quot;https://en.wikipedia.org/wiki/Merge_sort">Merge sort</a>.</td>
</tr>
<tr class="odd">
<td><img src="https://latex.codecogs.com/png.latex?O(N%5E%7B2%7D)"></td>
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Quadratic_time">quadratic</a></td>
<td>Simple sorting algorithms such as <a href="https://en.wikipedia.org/wiki/Bubble_sort">Bubble Sort</a> or <a href="https://en.wikipedia.org/wiki/Selection_sort">Select Sort</a>.</td>
</tr>
<tr class="even">
<td><img src="https://latex.codecogs.com/png.latex?O(N!)"></td>
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Factorial">factorial</a></td>
<td>Solving the <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem">Travelling Salesman problem</a> via brute-force.</td>
</tr>
</tbody>
</table>
</section>
<section id="data-structures" class="level1">
<h1>Data Structures</h1>
<section id="array" class="level2">
<h2 class="anchored" data-anchor-id="array">Array</h2>
<p>Arrays are collections of items stored at a contiguous memory locations. Such property makes array easy to traverse and genearlly it provides random access to its element in constant complexity.</p>
<p>Genearally speaking arrays have fixed size and new element can’t be added if the array is already full. However, it is possible to implement dynamic arrays at the expences of a memory overhead (unused memory is reserved for new items that will be added later on). Dynamic arrays achieve constant time complexity when it comes to append and delete operation in the general case, but if resize is needed then a new copy of the current array has to be create; thus requireing high memory and time complexity. The dynamic structure is obtained by creating a new array double size of the original array and copy all element from the previous array to the new array.</p>
<table class="table-hover table">
<caption>Array complexities</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Append</td>
<td>Add an element to the end of the array</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"> (in ammortized time)</td>
</tr>
<tr class="even">
<td>Insert</td>
<td>Insert an element to the i-th position of the array</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(N)"></td>
</tr>
<tr class="odd">
<td>Remove</td>
<td>Remove the i-th element of the array</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(N)"></td>
</tr>
<tr class="even">
<td>Remove Last</td>
<td>Remove the last element of the array</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"> (in ammortized time)</td>
</tr>
<tr class="odd">
<td>Search</td>
<td>Check if an element is present in the list</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="even">
<td>Get</td>
<td>Get the i-th element in the list</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(1)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="odd">
<td>Sort</td>
<td>Sort element in the list</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(N%20%5Clog%20N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(N)"></td>
</tr>
</tbody>
</table>
</section>
<section id="hash-tables" class="level2">
<h2 class="anchored" data-anchor-id="hash-tables">Hash Tables</h2>
<p>Hash tabels are one of the most importat data strcutre build uppon arrays. By organising data in (key, values) pairs it allows for fast insertion, lookup and access to data. It is composed by an array and the position of each key in this array is determined by the function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aidx%20=%20hash(key)%20%5C%25%20size(hash%5C_table)%0A">.</p>
<p>Python provide a native implementation of hash table under the <code>dict</code> class.</p>
<table class="table-hover table">
<caption>Hash table complexities</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Insert</td>
<td>Add an element to the dictionary</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"> (in ammortized time)</td>
</tr>
<tr class="even">
<td>Remove</td>
<td>Remove a key from the dictonary</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(1)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="odd">
<td>Search</td>
<td>Check if a key is present in the dictionary</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(1)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="even">
<td>Get</td>
<td>Get a given key in the dictionary</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(1)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="odd">
<td>Iterate</td>
<td>Iterate over all element of the dictionary</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
</tbody>
</table>
</section>
<section id="linked-list" class="level2">
<h2 class="anchored" data-anchor-id="linked-list">Linked List</h2>
<p>A linked list is a linear data structure that includes a series of connected nodes. Usually every nodes is composed by a data filed that contains some value and a pointer to the next element (if there is). While arrays are contiguous in memory, linked lists allows for a dynamic memory management where nodes can be scattered across the memory and simply point to each other. Linked lists are the fundamental backbone for other data structure as stacks and queue.</p>
<table class="table-hover table">
<caption>Linked list complexities</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Insert</td>
<td>Add an element to the list</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="even">
<td>Remove</td>
<td>Remove an element from the list</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="odd">
<td>Search</td>
<td>Search an element in the list</td>
<td>Time: <img src="https://latex.codecogs.com/png.latex?O(N)"> Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Node:</span>
<span id="cb1-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb1-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> value</span>
<span id="cb1-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> LinkedList:</span>
<span id="cb1-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initializing a stack.</span></span>
<span id="cb1-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use a dummy node, which is</span></span>
<span id="cb1-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># easier for handling edge cases.</span></span>
<span id="cb1-10">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb1-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb1-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb1-13"></span>
<span id="cb1-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check if the stack is empty</span></span>
<span id="cb1-15">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> isEmpty(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb1-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb1-17"></span>
<span id="cb1-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Insert at the end</span></span>
<span id="cb1-19">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> insert(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb1-20">        node <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Node(value)</span>
<span id="cb1-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.isEmpty():</span>
<span id="cb1-22">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node</span>
<span id="cb1-23">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node</span>
<span id="cb1-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb1-25">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node</span>
<span id="cb1-26">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span></span>
<span id="cb1-27"></span>
<span id="cb1-28">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># remove from the beginning.</span></span>
<span id="cb1-29">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> remove(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb1-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.isEmpty():</span>
<span id="cb1-31">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Remove from an empty list"</span>)</span>
<span id="cb1-32"></span>
<span id="cb1-33">        node <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head</span>
<span id="cb1-34">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail:</span>
<span id="cb1-35">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tail.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span></span>
<span id="cb1-36">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span></span>
<span id="cb1-37">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> node</span>
<span id="cb1-38"></span>
<span id="cb1-39">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sarch the node with a given value</span></span>
<span id="cb1-40">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> search(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb1-41">        node <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head</span>
<span id="cb1-42">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> node <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb1-43">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> node.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> value:</span>
<span id="cb1-44">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">break</span></span>
<span id="cb1-45">            node <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span></span>
<span id="cb1-46">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> node</span></code></pre></div>
<section id="floyds-cycle-finding-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="floyds-cycle-finding-algorithm">Floyd’s Cycle Finding Algorithm</h4>
<p>One of the most famous algorithm for LinkedList is the so called Floyd’s finding algorithm. This algorithm is used to find a loop in a linked list. It uses two pointers one moving twice as fast as the other one. The faster one is called the faster pointer and the other one is called the slow pointer. While traversing the linked list one of these things will occur:</p>
<ul>
<li>the fast pointer may reach the end (NULL) this shows that there is no loop in the linked list.</li>
<li>the fast pointer again catches the slow pointer at some time therefore a loop exists in the linked list.</li>
</ul>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> detectLoop(llist):</span>
<span id="cb2-2">    slow_pointer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> llist.head</span>
<span id="cb2-3">    fast_pointer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> llist.head</span>
<span id="cb2-4"></span>
<span id="cb2-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> (slow_pointer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb2-6">           <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> fast_pointer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb2-7">           <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> fast_pointer.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb2-8">        slow_pointer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> slow_pointer.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span></span>
<span id="cb2-9">        fast_pointer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fast_pointer.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span></span>
<span id="cb2-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> (slow_pointer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> fast_pointer):</span>
<span id="cb2-11">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-12"></span>
<span id="cb2-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span></code></pre></div>
</section>
<section id="stack" class="level3">
<h3 class="anchored" data-anchor-id="stack">Stack</h3>
<p>The Stack is a special kind of linked list that follows the <strong>LIFO</strong> principle. Intuitivly it’s a deck of cards where the top card of the deck (the last added element added) is the first card to picked (the first element to remove next).</p>
<table class="table-hover table">
<caption>Stack complexities</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Push</td>
<td>Add an element to the top of a stack</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="even">
<td>Pop</td>
<td>Remove an element from the top of a stack</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="odd">
<td>Peek</td>
<td>Get the value of the top element without removing it</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="even">
<td>IsEmpty</td>
<td>Check if the stack is empty</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Node:</span>
<span id="cb3-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb3-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> value</span>
<span id="cb3-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-6"></span>
<span id="cb3-7"></span>
<span id="cb3-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Stack:</span>
<span id="cb3-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initializing a stack.</span></span>
<span id="cb3-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use a dummy node, which is</span></span>
<span id="cb3-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># easier for handling edge cases.</span></span>
<span id="cb3-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-13">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-15"></span>
<span id="cb3-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the current size of the stack</span></span>
<span id="cb3-17">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> getSize(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.size</span>
<span id="cb3-19"></span>
<span id="cb3-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Check if the stack is empty</span></span>
<span id="cb3-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> isEmpty(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-23"></span>
<span id="cb3-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the top item of the stack</span></span>
<span id="cb3-25">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> peek(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Sanitary check to see if we</span></span>
<span id="cb3-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># are peeking an empty stack.</span></span>
<span id="cb3-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.isEmpty():</span>
<span id="cb3-29">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Peeking from an empty stack"</span>)</span>
<span id="cb3-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head.value</span>
<span id="cb3-31"></span>
<span id="cb3-32">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Push a value into the stack.</span></span>
<span id="cb3-33">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> push(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb3-34">        node <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Node(value)</span>
<span id="cb3-35">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-36">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node</span>
<span id="cb3-37">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb3-38">            node.prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head</span>
<span id="cb3-39">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node</span>
<span id="cb3-40">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node</span>
<span id="cb3-41">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-42"></span>
<span id="cb3-43">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove a value from the stack and return.</span></span>
<span id="cb3-44">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> pop(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-45">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.isEmpty():</span>
<span id="cb3-46">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Popping from an empty stack"</span>)</span>
<span id="cb3-47">        remove <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head</span>
<span id="cb3-48">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head.prev</span>
<span id="cb3-49">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.head.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-50">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-51">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> remove.value</span></code></pre></div>
</section>
<section id="queue" class="level3">
<h3 class="anchored" data-anchor-id="queue">Queue</h3>
<p>Queues are an implementation of LinkedList that follows the <strong>FIFO</strong> principle. Similarly to ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket.</p>
<table class="table-hover table">
<caption>Queue complexities</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Enqueue</td>
<td>Add an element to the end of the queue</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="even">
<td>Dequeue</td>
<td>Remove an element from the front of the queue</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="odd">
<td>Peek</td>
<td>Get the value of the front of the queue without removing it</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
<tr class="even">
<td>IsEmpty</td>
<td>Check if the queue is empty</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(1)"></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="binary-trees" class="level2">
<h2 class="anchored" data-anchor-id="binary-trees">Binary Trees</h2>
<p>A binary tree is a tree data structure in which each parent node can have at most two children. Each node of a binary tree consists of three items:</p>
<ul>
<li>value of the node,</li>
<li>the address to the left child,</li>
<li>the address to the right child.</li>
</ul>
<table class="table-hover table">
<caption>Binary trees complexities</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Construct</td>
<td>Construct a binary tree</td>
<td>Time and Space: <img src="https://latex.codecogs.com/png.latex?O(N)"></td>
</tr>
<tr class="even">
<td>Travers</td>
<td>Traverse a binary tree</td>
<td>Time <img src="https://latex.codecogs.com/png.latex?O(N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(height)"></td>
</tr>
</tbody>
</table>
<p>Binary trees are generaly represetned by linked nodes structures.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Node:</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb4-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> value</span>
<span id="cb4-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb4-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> pre_order_traverse(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb4-8">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value)</span>
<span id="cb4-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left:</span>
<span id="cb4-10">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left.pre_order_traverse()</span>
<span id="cb4-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right:</span>
<span id="cb4-12">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right.pre_order_traverse()</span>
<span id="cb4-13"></span>
<span id="cb4-14">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> in_order_traverse(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb4-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left:</span>
<span id="cb4-16">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left.in_order_traverse()</span>
<span id="cb4-17"></span>
<span id="cb4-18">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value)</span>
<span id="cb4-19"></span>
<span id="cb4-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right:</span>
<span id="cb4-21">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right.in_order_traverse()</span>
<span id="cb4-22"></span>
<span id="cb4-23">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> post_order_traverse(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb4-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left:</span>
<span id="cb4-25">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left.post_order_traverse()</span>
<span id="cb4-26"></span>
<span id="cb4-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right:</span>
<span id="cb4-28">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right.post_order_traverse()</span>
<span id="cb4-29"></span>
<span id="cb4-30">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value)</span>
<span id="cb4-31"></span>
<span id="cb4-32"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> build(array):</span>
<span id="cb4-33">    root <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb4-34">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(array)</span>
<span id="cb4-35"></span>
<span id="cb4-36">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> add_child(idx):</span>
<span id="cb4-37">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> n:</span>
<span id="cb4-38">            node <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Node(array[idx])</span>
<span id="cb4-39"></span>
<span id="cb4-40">            node.left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_child(idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-41"></span>
<span id="cb4-42">            node.right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_child(idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb4-43"></span>
<span id="cb4-44">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> node</span>
<span id="cb4-45"></span>
<span id="cb4-46">    root <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_child(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb4-47">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> root</span></code></pre></div>
<p>However, binary trees can be also represented using arrays in which:</p>
<ul>
<li>root node is stored at index 0,</li>
<li>left child is stored at index <img src="https://latex.codecogs.com/png.latex?(i%20%5Ccdot%202)%20+%201"> where, i is the index of the parent,</li>
<li>right child is stored at index <img src="https://latex.codecogs.com/png.latex?(i%20%5Ccdot%202)%20+%201">, i is the index of the parent.</li>
</ul>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Tree:</span>
<span id="cb5-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, array):</span>
<span id="cb5-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> array</span>
<span id="cb5-4"></span>
<span id="cb5-5">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> left(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, parent_idx):</span>
<span id="cb5-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> (parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array):</span>
<span id="cb5-7">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array[(parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb5-8"></span>
<span id="cb5-9">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> right(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, parent_idx):</span>
<span id="cb5-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> (parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array):</span>
<span id="cb5-11">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array[(parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span>
<span id="cb5-12"></span>
<span id="cb5-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> set_left(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, val, parent_idx):</span>
<span id="cb5-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array[(parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val</span>
<span id="cb5-15"></span>
<span id="cb5-16">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> set_right(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, val, parent_idx):</span>
<span id="cb5-17">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array[(parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val</span>
<span id="cb5-18"></span>
<span id="cb5-19">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> in_order(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, parent_idx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>):</span>
<span id="cb5-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left(parent_idx):</span>
<span id="cb5-21">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.in_order((parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-22"></span>
<span id="cb5-23">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.array[parent_idx])</span>
<span id="cb5-24"></span>
<span id="cb5-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right(parent_idx):</span>
<span id="cb5-26">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.in_order((parent_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<section id="binary-search-tree" class="level3">
<h3 class="anchored" data-anchor-id="binary-search-tree">Binary Search Tree</h3>
<p>Binary search tree is a data structure that quickly allows us to maintain a sorted list of numbers and search trought it.</p>
<p>The properties that separate a binary search tree from a regular binary tree are:</p>
<ul>
<li>all nodes of left subtree are less than the root node;</li>
<li>all nodes of right subtree are more than the root node;</li>
<li>both subtrees of each node are also BSTs i.e.&nbsp;they have the above two properties.</li>
</ul>
<p>Searching is extreamly efficent as can be done in <img src="https://latex.codecogs.com/png.latex?O(%5Clog%20N)"> time and constant space. Intuitively, searching is so efficent as we can analys only one of the two subtrees based on the relation between the current node’s value and the looked for value. Thus, at every step we half the searching space.</p>
<table class="table-hover table">
<caption>Binary Search trees complexities</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Insertion</td>
<td>Insert a node to the tree</td>
<td>Time <img src="https://latex.codecogs.com/png.latex?O(%5Clog%20N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(N)"></td>
</tr>
<tr class="even">
<td>Search</td>
<td>Search for an element in the tree</td>
<td>Time <img src="https://latex.codecogs.com/png.latex?O(%5Clog%20N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(N)"></td>
</tr>
<tr class="odd">
<td>Deletion</td>
<td>Remove an element from the tree</td>
<td>Time <img src="https://latex.codecogs.com/png.latex?O(%5Clog%20N)"> and Space: <img src="https://latex.codecogs.com/png.latex?O(N)"></td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Node:</span>
<span id="cb6-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb6-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> value</span>
<span id="cb6-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.left, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb6-5"></span>
<span id="cb6-6">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> search(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, value):</span>
<span id="cb6-7"></span>
<span id="cb6-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> value:</span>
<span id="cb6-9">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb6-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value.left <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-11">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value.left.search(value)</span>
<span id="cb6-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value.right <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.value.right.search(value)</span>
<span id="cb6-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb6-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb6-16">    </span>
<span id="cb6-17">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb6-18">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> insert(node, value):</span>
<span id="cb6-19">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> node <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-20">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> Node(value)</span>
<span id="cb6-21"></span>
<span id="cb6-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> node.value:</span>
<span id="cb6-23">            node.left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Node.insert(node.left, value)</span>
<span id="cb6-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb6-25">            node.right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Node.insert(node.right, value)</span>
<span id="cb6-26"></span>
<span id="cb6-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> node</span>
<span id="cb6-28"></span>
<span id="cb6-29">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@staticmethod</span></span>
<span id="cb6-30">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> remove(node, value):</span>
<span id="cb6-31">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> node.value:</span>
<span id="cb6-32">            node.left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Node.remove(node.left, value)</span>
<span id="cb6-33">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> node.value:</span>
<span id="cb6-34">            node.right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Node.remove(node.right, value)</span>
<span id="cb6-35"></span>
<span id="cb6-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb6-37">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># case 1: it is a leaf node</span></span>
<span id="cb6-38">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> node.left <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> node.right <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-39">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb6-40"></span>
<span id="cb6-41">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># case 2: there is only 1 child</span></span>
<span id="cb6-42">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> node.left <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> node.right <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-43">                node.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node.left.value</span>
<span id="cb6-44">                node.left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb6-45">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">elif</span> node.left <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> node.right <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-46">                node.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node.right.value</span>
<span id="cb6-47">                node.right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb6-48"></span>
<span id="cb6-49">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># case 3: take as new node, the right child or the left node of the right child if it exist</span></span>
<span id="cb6-50">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb6-51">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get new min value from right</span></span>
<span id="cb6-52">                temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node.right</span>
<span id="cb6-53">                prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> node</span>
<span id="cb6-54">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> temp.left <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-55">                    prev, temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp, temp.left</span>
<span id="cb6-56"></span>
<span id="cb6-57">                node.value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp.value</span>
<span id="cb6-58"></span>
<span id="cb6-59">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> node:</span>
<span id="cb6-60">                    prev.left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp.right</span>
<span id="cb6-61">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb6-62">                    node.right <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp.right</span>
<span id="cb6-63">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> node</span>
<span id="cb6-64">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> node</span></code></pre></div>


</section>
</section>
</section>

 ]]></description>
  <category>Coding</category>
  <guid>https://andompesta.github.io/posts/datastructure/</guid>
  <pubDate>Sun, 09 Oct 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Evaluation Metrics for Ads Ranking Systems</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/metrics/</link>
  <description><![CDATA[ 




<p>Ads ranking systems are the barebone of many modern business and became one of the main success story of machine learning applied to real-world problems. Given an user/client, the main goal of such ranking systems is to order a set of candidates ads according to their click or conversion score. As such it is commonly modelled as a binary classification task where the positive class (<img src="https://latex.codecogs.com/png.latex?y%5E+">) represents clicks or conversions, and the negative class (<img src="https://latex.codecogs.com/png.latex?y%5E-">) represents examples without interaction. In the most common case, the adopted classifier is a probabilistic classifier, which does not provide a class label, but rather the predicted probability of the positive class <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D(%20y%20=%20y%5E+)">. According to the application, it is possible to obtain a predicted label by applying a threshold <img src="https://latex.codecogs.com/png.latex?t"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D(%20y%20=%20y%5E+)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7By%7D%20=%20%5Cbegin%7Bcases%7D%0A1%20&amp;%20%5Ctext%7Bif%7D%20~%20%5Cmathbf%7BP%7D(%20y%20=%20y%5E+)%20%5Cgeq%20t%20%5C%5C%0A0%20&amp;%20%5Ctext%7Bo.w.%7D%0A%5Cend%7Bcases%7D%0A"></p>
<p>In most classification problem a reasonable threshold is <img src="https://latex.codecogs.com/png.latex?t=0.5">. However, in many domains the datasets are not balanced, models are not perfectly calibrated, and different use-cases have different <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">sensitivity vs specificity</a> tradeoffs. For example, many real-world applications are dominated by negative examples; thus a more conservative classifier might be prefered. Conservative models produce positive classifications only with strong evidence; thus they are identified by a low False Positive Rate. Probabbilistic classifiers can be made more or less conservative by adjusting the threshold <img src="https://latex.codecogs.com/png.latex?t">, and many experiments are required to detect the best trade-off. To this end, common evaliation metric that relay on the predicted labels, such as Accuracy or F1-score, are not suited in these domains as they fails to capture some important aspect of the model performance. To overcome this problem, it is desireable to have metric capable to identify model perforamnces based on the predicted probability rather than the predicted label and that are robust to unballanced datasets.</p>
<div style="text-align:center;">
<table id="tab:confiusion-matrix" style="border:none; background:transparent; text-align:center;" align="center">
<tbody>
<tr>
<td style="border:none;" rowspan="2">
</td>
<td style="border:none;">
</td>
<td style="background:#bbeeee;" colspan="2">
<b>Predicted condition</b>
</td>
</tr>
<tr>
<td style="background:#eeeeee;">
<b>Total population </b> <br> P + N
</td>
<td style="background:#ccffff;">
<b> Positive</b> <br> PP
</td>
<td style="background:#aadddd;">
<b> Negative</b> <br> PN
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td rowspan="2" class="nowrap unsortable" style="line-height:99%;vertical-align:middle;padding:.4em .4em .2em;background-position:50% .4em !important;overflow:hidden;background:#eeeebb;">
<b>Actual condition</b>
</td>
<td style="background:#ffffcc;">
<b>Positive <br>P</b>
</td>
<td style="background:#ccffcc;">
<b>True&nbsp;positive <br> <span style="font-size:85%;">hit</span></b> <br>TP
</td>
<td style="background:#ffdddd;">
<b>False&nbsp;negative <br> <span style="font-size:85%;">miss</span></b> <br>FN
</td>
<td style="background:#eeffee;">
<b>True positive rate <br> <span style="font-size:85%;"> Recall&nbsp;/&nbsp;sensitivity</span></b> <br> TPR = TP/P = 1 - FNR
</td>
<td style="background:#ffeeee;">
<b>False&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;">miss&nbsp;rate</span></b> <br> FNR = FN/P = 1 - TPR
</td>
</tr>
<tr>
<td style="background:#ddddaa;">
<b>Negative</b> <br> N
</td>
<td style="background:#ffcccc;">
<b>False&nbsp;positive <br> <span style="font-size:85%;">type&nbsp;I&nbsp;error / false&nbsp;alarm</span> </b> <br> FP
</td>
<td style="background:#bbeebb;">
<b>True&nbsp;negative <br> <span style="font-size:85%;">correct&nbsp;rejection</span></b> <br> TN
</td>
<td style="background:#eedddd;">
<b>False&nbsp;positive&nbsp;rate <br> <span style="font-size:85%;">probability&nbsp;of&nbsp;false&nbsp;alarm</span> </b> <br> FPR = FP/N = 1 − TNR
</td>
<td style="background:#ddeedd;">
<b>True&nbsp;negative&nbsp;rate <br> <span style="font-size:85%;"> specificity / selectivity </span></b> <br> TNR = TN/N = 1 − FPR
</td>
</tr>
<tr>
<td style="border:none;">
</td>
<td style="border:none;">
</td>
<td style="background:#eeffee;">
<b>Precision<br> <span style="font-size:85%;">positive&nbsp;predictive&nbsp;value</span> </b> <br> PR = TP
</td>
<td style="background:#ffeeee;">
<b>Fale&nbsp;omission&nbsp;rate </b> <br> FOR = FN
</td>
</tr>
</tbody>
</table>
<p style="font-size:small;">
Table 1: definition of all the metric reported and build on to of a Confusion matrix, credito to <a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="blank">wikipedia</a>
</p>
</div>
<section id="roc-auc" class="level2">
<h2 class="anchored" data-anchor-id="roc-auc">ROC-AUC</h2>
<p>The Receiver Operating Characteristic (ROC) is an analysis tool initially proposed by Provost et al. to compare classifiers’ performance. The ROC curve is built as the interpolation between the <strong>False Positive Rate</strong> (FPR) on the x-axe, and the <strong>True Positive Rate</strong> (TPR) on the y-axe, computed at different thresholds. Intuitively, the TPR represents how well a classifier can identify all the positive examples. In contrast the FPR indicate how likely an error will occur in the negative class (for a formal definition of TPR and FPR, consult the confusion matrix reported in Tab. 1).</p>
<div id="fig-roc-auc" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/metrics/img/roc-auc.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Example of an ROC-AUC curve.
</figcaption>
</figure>
</div>
<p>As shown in Figure&nbsp;1, a model that have higher ROC cureve is deemed to have better performances. However, there is the need to summarise the knowledge captured by a ROC curve in a single scalar value to facilitate the comparison of different models. To this end, the area under the ROC curve is used as a summary statistic representative of the classifier performances. Thus, ROC-AUC is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BROC-AUC%7D%20=%20%5Cint_0%5E1%20TPR%20~%20%5Cdelta%20%5Csmall%20FPR.%0A"></p>
<p>Note that ROC-AUC exhibits the following properties <span class="citation" data-cites="flach2015precision">[1]</span>:</p>
<ol type="1">
<li>It can be interpreted as “the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one” <span class="citation" data-cites="fernandez2018learning">[2]</span> <span class="citation" data-cites="fawcett2006introduction">[3]</span>. In other words, the ROC curve shows the ability of the classifier to rank the positive instances relativeto the negative instance only.</li>
<li>A universal baseline is always available, as a random classifier will have ROC-AUC of 0.5.</li>
<li>It cares about the ranking obtained from the predictions but does not consider the actual predicted value.</li>
<li>A classifier do not need to produce accurate, calibrated probability estimates; it need only to produce relative accurate scores that serve to discriminate positive and negative instances <span class="citation" data-cites="fawcett2004roc">[4]</span>.</li>
<li>The perfect model is represented by the point <img src="https://latex.codecogs.com/png.latex?(0,%201)"> and has an AUC of 1.</li>
<li>The point <img src="https://latex.codecogs.com/png.latex?(0,%200)"> identifies a model that never issues a positive prediction.</li>
<li>The point <img src="https://latex.codecogs.com/png.latex?(1,%201)"> implements the opposite strategy: only positive predictions are made.</li>
<li>ROC-AUC is a linear space, thus allowing for easy interpolation and visual interpretation.</li>
</ol>
<section id="highly-imbalanced-dataset" class="level3">
<h3 class="anchored" data-anchor-id="highly-imbalanced-dataset">Highly Imbalanced Dataset</h3>
<p>In the general case, one of the most attractive properties of the ROC curve is its robustness to changes in the class distribution. This propertly derives from the fact that, ROC curves are defined as a ratios of quantities computed w.r.t. only the positive or only the negative class rather than a mix of the two. Thus, we expect a model to generate similar ROC curves regardless of the number of positive and negative examples present in the dataset. In so doing, ROC curves are a great tools to compare model across different datasets; for example dataset generated across different day.</p>
<p>However, ROC curves are known to be “over-optimistic” at scoring model performances when the datasets are highly skewed, and there is a high interest in evaluating the model w.r.t. the positive class. For example, consider the case where you have two datasets; the former is composed of 100 negative points and 50 positive samples, while the latter is composed of 100 negative examples and 25 positive examples. As shown in Figure&nbsp;2 (a) , let’s assume that the negative examples overlap with the positive ones according to a uniform distribution.</p>
<div id="fig-init" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-init-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-init" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-dataset" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/metrics/img/datasets.png" class="img-fluid figure-img" style="width:80.0%" data-ref-parent="fig-init">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Two datasets with same label distribution but different amount of positive examples. Dataset 1 contains 50 positive examples while Dataset 2 contains only 25 positive examples. In both cases there are 100 negative lables.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-init" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-datasets-auc" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-datasets-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/metrics/img/datasets-auc.png" class="img-fluid figure-img" style="width:80.0%" data-ref-parent="fig-init">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-datasets-auc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) ROC-AUC computed on D1 and D2.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-init-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2
</figcaption>
</figure>
</div>
<p>Let’s try to compute the AUC for both of the datasets:</p>
<ul>
<li>then for D1, as the threshold (<img src="https://latex.codecogs.com/png.latex?t">) moves from <img src="https://latex.codecogs.com/png.latex?0"> to <img src="https://latex.codecogs.com/png.latex?0.5"> the True Positive Rate remains constant at 1; instead for <img src="https://latex.codecogs.com/png.latex?t%20%3E%200.5">, both TPR and FPR decrease linearly since positive and negative examples start to be homogeneously mixed.</li>
<li>similarly, for D2, when <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%200.75"> then <img src="https://latex.codecogs.com/png.latex?TPR%20=%201">; while for <img src="https://latex.codecogs.com/png.latex?t%3E0.75"> TRP and FPR decreases linearnly.</li>
</ul>
<p>A graphical representation is provided in Figure&nbsp;2 (b) showing how the ROC-AUC of the second dataset is more significant than the first dataset even if the models have the same maximum F1-score (maximum F1-score on D1 is achieved with <img src="https://latex.codecogs.com/png.latex?t=0.5"> while on D2 the best threshold is <img src="https://latex.codecogs.com/png.latex?t=0.5">). A deeper analysis suggests that this over-estimation problem arises when a significant change in the false positive leads only to a slight shift in the false positive rate since the dataset is dominated by the negative class <span class="citation" data-cites="davis2006relationship">[5]</span>.</p>
</section>
</section>
<section id="precision-recall-curve" class="level2">
<h2 class="anchored" data-anchor-id="precision-recall-curve">Precision-Recall curve</h2>
<p>When the main objective is to evaluate a model on the positive class, PR-curves are more informative. Figure&nbsp;3 shows how PR-curves are built by plotting the <strong>Precision</strong> as a function of the True Positive Rate (or Recall). By inspecting Tab. 1, it is visible how PR-curves effectively consider only statistics related to the positive class; thus, they are inherently robust to highly skewed datasets <span class="citation" data-cites="paula2016survey">[6]</span>. Perhaps motivated by the similarity with ROC-curves, PR-curves became a popular alternative to analysis models on highly skewed datasets.</p>
<div id="fig-pr-curve" class="quarto-figure quarto-figure-center quarto-float anchored" width="80%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pr-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/metrics/img/pr-curve.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pr-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Precision-Recall curve and PR-AUC.
</figcaption>
</figure>
</div>
<p>As for ROC curves, the PR-AUC is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BPR-AUC%7D%20=%20%5Cint_0%5E1%20PR%20~%20%5Cdelta%20%5Csmall%20TPR.%0A"></p>
<p>Overall, we can distinguish the following properties for the PR-AUC:</p>
<ol type="1">
<li>PR-AUC has no intrinsic meaning except the geometric one.</li>
<li>While ROC-AUC has a baseline value of <img src="https://latex.codecogs.com/png.latex?0.5">, in PR-AUC there is no universal baseline to compare with.</li>
<li>PR-AUC is not directly connected with the F1-score; thus is unrelated to the calibration score of the model.</li>
<li>The perfect model is represented by the point <img src="https://latex.codecogs.com/png.latex?(1,%201)"> and has an area of 1.</li>
<li>PR space is an hyperbolic space; thus more difficult to inspect visually and find similar performing models.</li>
</ol>
</section>
<section id="logloss" class="level2">
<h2 class="anchored" data-anchor-id="logloss">LogLoss</h2>
<p>One of the limitations of the previous metrics is their focus on the ranking obtained from the model output, but they ignore the predicted value itself. This is not an issue in most recommendation systems, but most real-time ad allocation systems require well-calibrated and accurate prediction values to implement an optimal bidding strategy. In this problem instance, choosing a threshold t is not important. Rather there is high interest in having predictions that, on average, <em>reliability</em> resemble the ground-true. That is: “the probability associated with the predicted class label should reflect its ground truth correctness likelihood” <span class="citation" data-cites="guo2017calibration">[7]</span>. For example, consider a dataset composed of 10 samples and assume that the model assigns a probability of 0.6 to every instance. Then, if the model is <strong>calibrated</strong>, we expect 6 examples to belong to the positive class.</p>
<p>Note that:</p>
<ol type="1">
<li>Calibration is a measure of <strong>uncertainty</strong>, not accuracy.</li>
<li>A calibrated model allows for higher interpretability as the output probabilities have an intrinsic meaning.</li>
</ol>
<p>A standard metric used to monitor the model’s calibration is the negative <strong>LogLoss</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%20=%20-%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5CBig(%20y%20%5Clog(%5Chat%7Bp%7D)%20+%20(1%20-%20y)%20%5Clog(1%20-%20%5Chat%7Bp%7D)%20%5CBig);%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y"> represents the ground-true label, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D"> is the predicted probability, and <img src="https://latex.codecogs.com/png.latex?N"> is the size of the dataset. The LogLoss has an extended application history as training loss for (deep) logistic regressions, yet here a claim is made to adopt it as a validation and test metric. Modern deep neural networks are highly accurate but overconfident, showing poor uncertainty estimation <span class="citation" data-cites="sensoy2018evidential">[8]</span>. Thus, it is handy to have a scalar metric to summarise a model’s calbration characteristics and compare it to the prediction error.</p>
</section>
<section id="brier-score" class="level2">
<h2 class="anchored" data-anchor-id="brier-score">Brier Score</h2>
<p>Another popular method to capture the model calibration in a scalar value is by computing the Brier Score<span class="citation" data-cites="hernandez2011brier">[9]</span>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BBS%7D%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5CBig(%20y_i%20-%20%5Chat%7Bp%7D_i%20%5CBig)%5E2.%0A"></p>
<p>The Brier Score is the mean-squared-error between the ground ture and the predicted probabilities; thus the lower the value the better.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-flach2015precision" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Flach P, Kull M (2015) Precision-recall-gain curves: PR analysis done right. Advances in neural information processing systems 28</div>
</div>
<div id="ref-fernandez2018learning" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Fernández A, Garcı́a S, Galar M, Prati RC, Krawczyk B, Herrera F (2018) Learning from imbalanced data sets. Springer</div>
</div>
<div id="ref-fawcett2006introduction" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Fawcett T (2006) An introduction to ROC analysis. Pattern recognition letters 27(8):861–874</div>
</div>
<div id="ref-fawcett2004roc" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Fawcett T (2004) ROC graphs: Notes and practical considerations for researchers. Machine learning 31(1):1–38</div>
</div>
<div id="ref-davis2006relationship" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Davis J, Goadrich M (2006) The relationship between precision-recall and ROC curves. In: Proceedings of the 23rd international conference on machine learning. pp 233–240</div>
</div>
<div id="ref-paula2016survey" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Paula Branco L, Ribeiro R (2016) A survey of predictive modelling under imbalanced distributions. ACM Comput Surveys 49:31–48</div>
</div>
<div id="ref-guo2017calibration" class="csl-entry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Guo C, Pleiss G, Sun Y, Weinberger KQ (2017) On calibration of modern neural networks. In: International conference on machine learning. PMLR, pp 1321–1330</div>
</div>
<div id="ref-sensoy2018evidential" class="csl-entry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Sensoy M, Kaplan L, Kandemir M (2018) Evidential deep learning to quantify classification uncertainty. Advances in neural information processing systems 31</div>
</div>
<div id="ref-hernandez2011brier" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Hernández-Orallo J, Flach PA, Ramirez CF (2011) Brier curves: A new cost-based visualisation of classifier performance. In: Icml. pp 585–592</div>
</div>
</div>


</section>

 ]]></description>
  <category>Statistics</category>
  <guid>https://andompesta.github.io/posts/metrics/</guid>
  <pubDate>Fri, 15 Jul 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Variational Autoencoders</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/vea/</link>
  <description><![CDATA[ 




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In this article, I will delve into the theoretical foundations of Variational Autoencoders (VAE). You can find the code used for both Convolutional Neural Network (CNN) and normal feedforward autoencoder trained on the MNIST dataset on my <a href="https://github.com/andompesta/variational-autoencoder/tree/master">GitHub repository</a>.</p>
<p>VAEs are generative models grounded in Bayesian inference theory and variational inference. The underlying concept involves generating data points from a given latent variable that encodes the characteristics of the desired data. To illustrate, consider the task of dwaring an animal. Initially, we conceptualize the animal with specific criteria, such as having four legs and the ability to swim. With these criteria, we can draw the animal by sampling from the animal kingdom.</p>
<p>Let use define some notation:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x"> represents a data point.</li>
<li><img src="https://latex.codecogs.com/png.latex?z"> is a latent variable.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(x)"> denotes the probability distribution of the data.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(z)"> signifies the probability of the latent variable indicating the type of generated data.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(x%7Cz)"> represents the distribution of the generated data based on a latent variable. Analogously, it is akin to transforming imagination into reality.</li>
<li><img src="https://latex.codecogs.com/png.latex?D_%7BKL%7D%5Cbig(p(X)%20%5Cparallel%20q(X)%5Cbig)%20=%20%5Csum_%7Bx_i%20%5Cin%20X%7D%20p(x_i)%20%5Clog%20%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D%20=%20-%20%5Csum_%7Bx_i%20%5Cin%20X%7D%20p(x_i)%20%5Clog%20%5Cfrac%7Bq(x_i)%7D%7Bp(x_i)%7D"> is the Kullback-Leibler (KL) divergence between two discrete distributions.</li>
</ul>
<p>KL divergence possesses notable properties: firstly, <img src="https://latex.codecogs.com/png.latex?D_%7BKL%7D%5Cbig(p(x)%20%5Cparallel%20q(x)%5Cbig)"> is not equal to <img src="https://latex.codecogs.com/png.latex?D_%7BKL%7D%5Cbig(q(x)%20%5Cparallel%20p(x)%5Cbig)">, indicating its asymmetric nature. Secondly, <img src="https://latex.codecogs.com/png.latex?D_%7BKL%7D%20%3E%200">.</p>
</section>
<section id="variationa-autoencoders" class="level1">
<h1>Variationa Autoencoders</h1>
<p>Variational Autoencoders function as generative models, enabling the sampling of new data points from such a model. In general, generative models learn a functional form of <img src="https://latex.codecogs.com/png.latex?p(x)"> that allows for sampling. However, <img src="https://latex.codecogs.com/png.latex?p(x)">is often unknown, and only a dataset <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D%20=%20(x_i)%5EN_%7Bi=1%7D"> comprising some samples from <img src="https://latex.codecogs.com/png.latex?p(x)"> is provided.</p>
<p>VAEs overcome this challenge by leveraging the concept that high-dimensional data is generated based on a low-dimensional latent variable <img src="https://latex.codecogs.com/png.latex?z">; thus, the joint distribution can be factorized as <img src="https://latex.codecogs.com/png.latex?p(x,z)=p(x%E2%88%A3z)p(z)">. Ultimately, through marginalization, we can define <img src="https://latex.codecogs.com/png.latex?p(x)"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(x)%20=%20%5Cint%20p(x%7Cz)%20p(z)%20%5Cpartial%20z.%0A"></p>
<p>In our earlier analogy, <img src="https://latex.codecogs.com/png.latex?z"> represents the imagined concept, while <img src="https://latex.codecogs.com/png.latex?x"> is the realization of all the selected concepts. As mentioned before, during the training phase of VAEs, access is neither given to <img src="https://latex.codecogs.com/png.latex?p(x)"> nor to the latent variable <img src="https://latex.codecogs.com/png.latex?z"> used to generate the dataset. However, throughout the training process, a reasonable posterior distribution <img src="https://latex.codecogs.com/png.latex?p(z%E2%88%A3x)"> is learned. This approach makes sense, as the goal is to make the latent variable likely under the observed data, thereby generating plausible data.</p>
<p>According to Bayesian theory:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(z%7Cx)%20=%20%5Cfrac%7Bp(x%7Cz)%5Ccdot%20p(z)%7D%7Bp(x)%7D%20=%20%5Cfrac%7Bp(x,%20z)%7D%7Bp(x)%7D.%0A"></p>
<p>As mentioned earlier, <img src="https://latex.codecogs.com/png.latex?p(x)"> can be expressed through marginalization over <img src="https://latex.codecogs.com/png.latex?z">; however, such computation is typically intractable as it involves integrating over all latent dimensions:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(x)%20%5Cint%20...%20%5Cint%20%5Cint%20p(x%7Cz)%5Ccdot%20p(z)%20%5Cpartial%20z_i.%0A"></p>
<p>To overcome this computational challenge, variational inference suggests approximating <img src="https://latex.codecogs.com/png.latex?p(z%E2%88%A3x)"> with a simpler distribution <img src="https://latex.codecogs.com/png.latex?q(z%E2%88%A3x)">. By assigning a tractable form to <img src="https://latex.codecogs.com/png.latex?q(z%E2%88%A3x)">, such as a Gaussian distribution, and adjusting its parameters to closely match <img src="https://latex.codecogs.com/png.latex?p(z%E2%88%A3x)">, we can overcome the intractability issue.</p>
<p>Formally, we can rewrite our goal as: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmin%20D_%7BKL%7D%5Cbig(q(z%7Cx)%20%7C%7C%20p(z%7Cx)%5Cbig)%20&amp;%20=%20-%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z%7Cx)%7D%20%20%5C%5C%0A&amp;%20=%20-%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5CBig(%5Cfrac%7Bp(x,%20z)%7D%7Bq(z%7Cx)%7D%20%20%5Ccdot%20%5Cfrac%7B1%7D%7Bp(x)%7D%20%5CBig)%20%5C%5C%0A&amp;%20=%20-%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5CBig(%5Cfrac%7Bp(x,%20z)%7D%7Bq(z%7Cx)%7D%20%20-%20%5Clog%20p(x)%20%5CBig)%20%20%5C%5C%0A&amp;%20=%20-%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bp(x,%20z)%7D%7Bq(z%7Cx)%7D%20+%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20p(x)%20%5C%5C%0A&amp;%20=%20%5Clog%20p(x)%20-%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bp(x,%20z)%7D%7Bq(z%7Cx)%7D%20~~~~%20%5Csmall%7B%5Ctext%7B:as%20$%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20=%201$%20and%20$p(x)$%20do%20not%20depend%20on%20$z$%7D%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>By rearranging the above equation we can state that: <img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20p(x)%20=%20D_%7BKL%7D%5Cbig(q(z%7Cx)%20%7C%7C%20p(z%7Cx)%5Cbig)%20+%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bp(x,%20z)%7D%7Bq(z%7Cx)%7D.%0A"></p>
<p>However, <img src="https://latex.codecogs.com/png.latex?p(x)"> is constant for a given dataset <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">, thus minimizing <img src="https://latex.codecogs.com/png.latex?D_%7BKL%7D%5Cbig(q(z%7Cx)%20%7C%7C%20p(z%7Cx)%5Cbig)"> is equivalent to maximise <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bp(x,%20z)%7D%7Bq(z%7Cx)%7D"> up to a constant factor. Such formulation of the KL-divergenve is also known as the Evidence Lower Bound (ELBO) and it is tractable:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bp(x,%20z)%7D%7Bq(z%7Cx)%7D%20&amp;%20=%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cbig(%20%5Cfrac%7Bp(x%7Cz)%20p(z)%7D%7Bq(z%7Cx)%7D%5Cbig)%20%20%5C%5C%0A&amp;%20=%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5CBig(%5Clog%20p(x%7Cz)%20+%20%5Clog%20%5Cfrac%7Bp(z)%7D%7Bq(z%7Cx)%7D%20%5CBig)%20%20%5C%5C%0A&amp;%20=%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20p(x%7Cz)%20+%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bp(z)%7D%7Bq(z%7Cx)%7D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7Bz%20%5Csim%20q(z%7Cx)%7D%20%5Cbig%5B%20%5Clog%20p(x%7Cz)%20%5Cbig%5D%20-%20%5Csum_%7Bx%20%5Cin%20%5Chat%7BX%7D%7D%20q(z%7Cx)%20%5Clog%20%5Cfrac%7Bq(z%7Cx)%7D%7Bp(z)%7D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7Bz%20%5Csim%20q(z%7Cx)%7D%20%5Cbig%5B%20%5Clog%20p(x%7Cz)%20%5Cbig%5D%20-%20%5Cmathbb%7BE%7D_%7Bz%20%5Csim%20q(z%7Cx)%7D%20%5Cbig%5B%20%5Clog%20q(z%7Cx)%20-%20%5Clog%20p(z)%20%5Cbig%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbb%7BE%7D_%7Bz%20%5Csim%20q(z%7Cx)%7D%20%5Cbig%5B%20%5Clog%20p(x%7Cz)%20%5Cbig%5D%20-%20D_%7BKL%7D%5Cbig(%20q(z%7Cx)%20%5Cparallel%20p(z)%20%5Cbig).%0A%5Cend%7Balign*%7D%0A"></p>
<p>The initial component of the ELBO, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7Bz%20%5Csim%20q(z%7Cx)%7D%20%5Cbig%5B%20%5Clog%20p(x%7Cz)%20%5Cbig%5D"> , is commonly known as the (negative) reconstruction error. This is because it involves encoding <img src="https://latex.codecogs.com/png.latex?x"> into <img src="https://latex.codecogs.com/png.latex?z"> and then decoding it back. The second segment, <img src="https://latex.codecogs.com/png.latex?D_%7BKL%7D%5Cbig(%20q(z%7Cx)%5Cparallel%20p(z)%20%5Cbig)"> can be viewed as a regularization term that imposes a specific distribution on <img src="https://latex.codecogs.com/png.latex?q">.</p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>Based on the code, we have trained a CNN-based Variational Autoencoder on the MNIST dataset. Figure&nbsp;1 report the training loss, while Figure&nbsp;2 shows us some generated example. As it is possible to see, there are still some artifact. Maybe a better activation function would provide better results.</p>
<div id="fig-loss" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/vea/img/loss.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Training loss of a CNN based VAE on the MNIST dataset.
</figcaption>
</figure>
</div>
<div id="fig-new-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-new-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/vea/img/cnn_variational_autoencoder_pred.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-new-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Generated examples.
</figcaption>
</figure>
</div>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://andompesta.github.io/posts/vea/</guid>
  <pubDate>Sun, 10 Jan 2021 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Basic Principles of Linear Algebra</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/linear_algebra/</link>
  <description><![CDATA[ 




<p>Linear algebra is the branch of math and statistics that is devoted to the study of matrices and vectors. As such, it is broadly used to model real-world problems in phisitcs and machine learning. Such post is a collections of my notes obtained from the 3Blue1Brown series on linear-algebra <span class="citation" data-cites="youtubeEssenceLinear">[1]</span> and Murphy’s new book <span class="citation" data-cites="murphy2022probabilistic">[2]</span>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnewcommand%7B%5Crvepsilon%7D%7B%5Cmathbf%7B%5Cepsilon%7D%7D%0A%5Cnewcommand%7B%5Crvtheta%7D%7B%5Cmathbf%7B%5Ctheta%7D%7D%0A%5Cnewcommand%7B%5Crva%7D%7B%5Cmathbf%7Ba%7D%7D%0A%5Cnewcommand%7B%5Crvb%7D%7B%5Cmathbf%7Bb%7D%7D%0A%5Cnewcommand%7B%5Crvc%7D%7B%5Cmathbf%7Bc%7D%7D%0A%5Cnewcommand%7B%5Crve%7D%7B%5Cmathbf%7Be%7D%7D%0A%5Cnewcommand%7B%5Crvi%7D%7B%5Cmathbf%7Bi%7D%7D%0A%5Cnewcommand%7B%5Crvj%7D%7B%5Cmathbf%7Bj%7D%7D%0A%5Cnewcommand%7B%5Crvu%7D%7B%5Cmathbf%7Bu%7D%7D%0A%5Cnewcommand%7B%5Crvv%7D%7B%5Cmathbf%7Bv%7D%7D%0A%5Cnewcommand%7B%5Crvx%7D%7B%5Cmathbf%7Bx%7D%7D%0A%5Cnewcommand%7B%5CrmA%7D%7B%5Cmathbf%7BA%7D%7D%0A%5Cnewcommand%7B%5CrmB%7D%7B%5Cmathbf%7BB%7D%7D%0A%5Cnewcommand%7B%5CrmC%7D%7B%5Cmathbf%7BC%7D%7D%0A%5Cnewcommand%7B%5CrmH%7D%7B%5Cmathbf%7BH%7D%7D%0A%5Cnewcommand%7B%5CrmI%7D%7B%5Cmathbf%7BI%7D%7D%0A%5Cnewcommand%7B%5CrmM%7D%7B%5Cmathbf%7BM%7D%7D%0A%5Cnewcommand%7B%5CrmS%7D%7B%5Cmathbf%7BS%7D%7D%0A%5Cnewcommand%7B%5CrmU%7D%7B%5Cmathbf%7BU%7D%7D%0A%5Cnewcommand%7B%5CrmV%7D%7B%5Cmathbf%7BV%7D%7D%0A%5Cnewcommand%7B%5CrmX%7D%7B%5Cmathbf%7BX%7D%7D%0A%5Cnewcommand%7B%5CrmY%7D%7B%5Cmathbf%7BY%7D%7D%0A%5Cnewcommand%7B%5Creal%7D%7B%5Cmathbb%7BR%7D%7D%0A"></p>
<section id="basic-matrix-operations" class="level1">
<h1>Basic Matrix Operations</h1>
<ol type="1">
<li><strong>transpose</strong>: given a matrix <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%20n%7D">, its transpose <img src="https://latex.codecogs.com/png.latex?%5CrmA%5ET"> is obtained ‘’flipping’’ the rows and colums</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CrmA%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20...%20&amp;%20a_%7B1n%7D%5C%5C%0A%20%20a_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20...%20&amp;%20a_%7B2n%7D%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20...%20&amp;%20a_%7Bmn%7D%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A%5CRightarrow%0A%5CrmA%5ET%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B21%7D%20&amp;%20...%20&amp;%20a_%7Bm1%7D%5C%5C%0A%20%20a_%7B12%7D%20&amp;%20a_%7B22%7D%20&amp;%20...%20&amp;%20a_%7Bm2%7D%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7B1n%7D%20&amp;%20a_%7B2n%7D%20&amp;%20...%20&amp;%20a_%7Bmn%7D%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
<p>The most important properties are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%7B(%5CrmA%5ET)%7D%5ET%20=%20%5CrmA"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5CrmA%20%5CrmB)%5ET%20=%20%5CrmB%5ET%20%5CrmA%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(%5CrmA%20+%20%5CrmB)%5ET%20=%20%5CrmA%5ET%20%5CrmB%5ET"></li>
</ul>
<ol start="2" type="1">
<li><strong>matrix multiplication</strong>: while the summation of 2 matrixes is done element-wise. Matrix multiplication is done row-by-colum and requires matrixes of specific sizes. Given <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?%5CrmB%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%20p%7D"> it is possible to define <img src="https://latex.codecogs.com/png.latex?%5CrmC%20=%20%5CrmA%20%5CrmB%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%20p%7D"> s.t. <img src="https://latex.codecogs.com/png.latex?c_%7Bi,j%7D%20=%20%5Csum_%7Bk=1%7D%5E%7Bn%7D%20a_%7Bik%7D%20b_%7Bkj%7D">. In other words, <img src="https://latex.codecogs.com/png.latex?%5CrmC"> is a linear combination of the row of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> and the colum of <img src="https://latex.codecogs.com/png.latex?%5CrmB">.</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CrmC%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%20%20-%20&amp;%20%5Crva_%7B1:%7D%20&amp;%20-%5C%5C%0A%20%20-%20&amp;%20%5Crva_%7B2:%7D%20&amp;%20-%5C%5C%0A%20%20%20%20%20&amp;%20%5Cvdots%20&amp;%20%20%20%5C%5C%0A%20%20-%20&amp;%20%5Crva_%7Bm:%7D%20&amp;%20-%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20&amp;%20%7C%5C%5C%0A%20%20%5Crvb_%7B:1%7D%20&amp;%20%5Crvb_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Crvb_%7B:p%7D%5C%5C%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20&amp;%20%7C%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A=%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20%5Crva_%7B1:%7D%5Crvb_%7B:1%7D%20&amp;%20%5Crva_%7B1:%7D%5Crvb_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Crva_%7B1:%7D%5Crvb_%7B:p%7D%5C%5C%0A%20%20%5Crva_%7B2:%7D%5Crvb_%7B:1%7D%20&amp;%20%5Crva_%7B2:%7D%5Crvb_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Crva_%7B2:%7D%5Crvb_%7B:p%7D%5C%5C%0A%20%20%20%20%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%20%20%5C%5C%0A%20%20%5Crva_%7Bm:%7D%5Crvb_%7B:1%7D%20&amp;%20%5Crva_%7Bm:%7D%5Crvb_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Crva_%7Bm:%7D%5Crvb_%7B:p%7D%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
<p>The most important properties are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(%5CrmA%20%5CrmB)%20%5CrmC%20=%20%5CrmA%20(%5CrmB%20%5CrmC)">,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CrmA(%5CrmB%20+%20%5CrmC)%20=%20(%5CrmA%20%5CrmB)%20+%20(%5CrmA%20%5CrmC)">,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5CrmB%20%5Cneq%20%5CrmB%20%5CrmA">.</li>
</ul>
<ol start="3" type="1">
<li><strong>matrix inverse</strong>: As for real numbers, the inverso of a matrix <img src="https://latex.codecogs.com/png.latex?%5CrmA"> is denoted as <img src="https://latex.codecogs.com/png.latex?%5CrmA%5E%7B-1%7D"> and is defined as the matrix such that: <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5CrmA%5E%7B-1%7D%20=%20%5CrmI">. Besites being easy to define computing the inverse of a matrix is an expencive operations. Moreover, <img src="https://latex.codecogs.com/png.latex?%5CrmA%5E%7B-1%7D"> exists if and only if <img src="https://latex.codecogs.com/png.latex?det(%5CrmA)%20%5Cneq%200">.</li>
</ol>
<p>The most important properties are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(%5CrmA%5E%7B-1%7D)%5E%7B-1%7D%20=%20%5CrmA">,</li>
<li><img src="https://latex.codecogs.com/png.latex?(%5CrmA%20%5CrmB)%5E%7B-1%7D%20=%20%5CrmB%5E%7B-1%7D%20%5CrmA%5E%7B-1%7D">,</li>
<li><img src="https://latex.codecogs.com/png.latex?(%5CrmA%5E%7B-1%7D)%5E%7BT%7D%20=%20(%5CrmA%5E%7BT%7D)%5E%7B-1%7D%20=%20%5CrmA%5E%7B-T%7D">.</li>
</ul>
</section>
<section id="basis-vectors" class="level1">
<h1>Basis Vectors</h1>
<p>In linear algebra, a vector basis <img src="https://latex.codecogs.com/png.latex?%5CrmB"> of a vector space <img src="https://latex.codecogs.com/png.latex?%5CrmV"> is a set of vectors <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Crvb_1,%20...,%20%5Crvb_n%5C%7D"> that are linearly independent and allow to reconstruct every vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_i%7D%20%5Cin%20V"> as a linear combination of <img src="https://latex.codecogs.com/png.latex?%5CrmB">:</p>
<p><span id="eq-basis-vector"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%20%20%5Cmathbf%7Bv_i%7D%20&amp;%20=%20a_1%20%5Cmathbf%7Bb_1%7D%20+%20...%20+%20a_n%20%5Cmathbf%7Bb_n%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B1%7D"></span></p>
<p>For example, the vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D%20=%20%5Cleft%5B%0A%5Cbegin%7Barray%7D%7Bc%7D%0A%20%201%5C%5C%0A%20%200%0A%5Cend%7Barray%7D%0A%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D%20=%20%5Cleft%5B%0A%5Cbegin%7Barray%7D%7Bc%7D%0A%20%200%5C%5C%0A%20%201%0A%5Cend%7Barray%7D%0A%5Cright%5D"> are the most common base vectors for the vectors space <img src="https://latex.codecogs.com/png.latex?%5Creal%5E2">. Thus, it is possible to represent a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%203%5C%5C%0A%20%202%0A%5Cend%7Barray%7D%5Cright%5D"> as <img src="https://latex.codecogs.com/png.latex?3%20%5Cmathbf%7Bi%7D%20+%202%20%5Cmathbf%7Bj%7D">.</p>
<div id="fig-base-vector" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/basic_vectors.png" id="fig-base-vector" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Vector representation by the base vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D">.
</figcaption>
</figure>
</div>
<p>The ability to represent any vector in <img src="https://latex.codecogs.com/png.latex?V"> as a linear combination of the basis vectors is a powerful concept. However, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D"> are not the only possible basis vectors of <img src="https://latex.codecogs.com/png.latex?%5Creal%5E2">. For example, another possible basis could be formed by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%201%5C%5C%0A%20%202%0A%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%203%5C%5C%0A%20%20-1%0A%5Cend%7Barray%7D%5Cright%5D">. However, the representation of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> would be different than the one w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D">.</p>
</section>
<section id="span" class="level1">
<h1>Span</h1>
<p>The <strong>Span</strong> is defined as the set of all possible vectors that we can create given a basis set. Note that the number of basis vectors defines the dimension of our vector space.</p>
</section>
<section id="linear-transformations" class="level1">
<h1>Linear Transformations</h1>
<p>A <strong>linear transformation</strong> is equivalent to a function over vectors. That is, a linear transformation “move” an input vector to an output vector. While general transformations have complex features, linear transformations have some well-defined properties:</p>
<ol type="1">
<li>they maintain the origin of the vector space invariant,</li>
<li>they map equally spaced lines to equally spaced lines (or points).</li>
</ol>
<p><span id="eq-linear-transform"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%20%20L(a_1%20%5Cmathbf%7Bi%7D%20+%20a_2%20%5Cmathbf%7Bj%7D)%20&amp;%20=%20a_1L(%5Cmathbf%7Bi%7D)%20+%20a_2L(%5Cmathbf%7Bj%7D)%0A%5Cend%7Balign%7D%0A%5Ctag%7B2%7D"></span></p>
<div id="fig-transformations" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/transformations.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples of the most commont linear transformations. (Image taken from <a href="https://mathigon.org/course/linear-algebra/linear-transformations"> Samuel S. Watson</a>).
</figcaption>
</figure>
</div>
<p>Thanks to their properties, it is possible to linearly transform any vector by means to its basis. In other words, given a vector <img src="https://latex.codecogs.com/png.latex?%5Crvx%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20-1%5C%5C%202%20%5Cend%7Barray%7D%5Cright%5D"> w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj"> and any lineart transformation <img src="https://latex.codecogs.com/png.latex?L">. It is possible to represent <img src="https://latex.codecogs.com/png.latex?L(%5Crvx)"> as a function of <img src="https://latex.codecogs.com/png.latex?L(%5Crvi)"> and <img src="https://latex.codecogs.com/png.latex?L(%5Crvj)"> (formally <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7Bx%7D)%20=%20-1%20L(%5Cmathbf%7Bi%7D)%20+%202%20L(%5Cmathbf%7Bj%7D)">).</p>
<p>For example, assume <img src="https://latex.codecogs.com/png.latex?L%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%203%5C%5C%20-2%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D">, then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AL(%5Cmathbf%7Bi%7D)%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%203%5C%5C%20-2%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%200%20%5Cend%7Barray%7D%5Cright%5D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%20-2%20%5Cend%7Barray%7D%5Cright%5D%5C%5C%0AL(%5Cmathbf%7Bj%7D)%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%203%5C%5C%20-2%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%200%5C%5C%201%20%5Cend%7Barray%7D%5Cright%5D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%200%20%5Cend%7Barray%7D%5Cright%5D%20%5C%5C%0AL(%5Cmathbf%7Bx%7D)%20&amp;=%20-1%20L(%5Cmathbf%7Bi%7D)%20+%202%20L(%5Cmathbf%7Bj%7D)%20%5C%5C%0A%20%20%20%20&amp;=%20-1%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%20-2%20%5Cend%7Barray%7D%5Cright%5D%20+%202%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%200%20%5Cend%7Barray%7D%5Cright%5D%20%5C%5C%0A%20%20%20%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%205%5C%5C%202%20%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<div id="fig-base-vector" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/linear-transform.png" id="fig-linear-transform" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Visualization of the linear transformation appled to vector <img src="https://latex.codecogs.com/png.latex?%5Crvx">.
</figcaption>
</figure>
</div>
<p>Finally, as a linear transformation is represented by a matrix, it is possible to define the <strong>composition of two or more linear transformations</strong> as he left-to-right product of the transformation matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AL_2(L_1(%20%5Cmathbf%7Bx%7D%20))%20=%20%20L_2L_1(%5Cmathbf%7Bx%7D)%0A%5Cend%7Balign%7D%0A"></p>
<p>For example, if <img src="https://latex.codecogs.com/png.latex?L_1%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%20-2%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D">, <img src="https://latex.codecogs.com/png.latex?L_2%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%200%20&amp;%202%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D">. Then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AL_2(L_1(%20%5Cmathbf%7Bx%7D%20))%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%200%20&amp;%202%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5CBig%20(%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%20-2%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D%20%5CBig)%0A%5C%5C%0AL_2%20L_1(%20%5Cmathbf%7Bx%7D%20)%20&amp;=%20%5CBig%20(%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%200%20&amp;%202%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%20-2%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5CBig)%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%202%20&amp;%200%5C%5C%201%20&amp;%20-2%20%5Cend%7Barray%7D%5Cright%5D%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Note that as matrix multiplication is equal to applying different linear transformations, the multiplication order does matter.</p>
</section>
<section id="determinant" class="level1">
<h1>Determinant</h1>
<p>As linear transformations alter the original vector space, it is important to evaluate by how much the original space is expanded or contracted by a given linear transformation <img src="https://latex.codecogs.com/png.latex?L">. As shown in Figure&nbsp;4, the <strong>determinant</strong> define how much the original unit surface is changed by <img src="https://latex.codecogs.com/png.latex?L">.</p>
<p>The determinant has some interesting properties:</p>
<ol type="1">
<li>A liner transantformation with 0 determinant ($ det(L) = 0$) means that squash all the vectrs on a single line/plane. Moreover, it also means that <img src="https://latex.codecogs.com/png.latex?L"> has linearly dependents columns.</li>
<li>The determinant can be negative if it change orientation of the space.</li>
<li>Determinant is associative: <img src="https://latex.codecogs.com/png.latex?det(L)%20%5Ccdot%20det(M)%20=%20det(L%20%5Ccdot%20M)">.</li>
</ol>
<div id="fig-determinant" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-determinant-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/determinant.png" id="fig-determinant" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-determinant-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Visualization of the determinant for a initial vector space defined by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D"> and the vector space obtained after applying the transformation <img src="https://latex.codecogs.com/png.latex?L">.
</figcaption>
</figure>
</div>
</section>
<section id="system-of-linear-equations" class="level1">
<h1>System of Linear Equations</h1>
<p>It is convininet to use inear algebra to represent a system of linear equations, e.g.</p>
<table align="center">
<tbody><tr>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bcases%7D%0A%20%20%20%202x%20+%205y%20+%203z%20=%20-3%20%5C%5C%0A%20%20%20%204x%20+%208z%20=%200%20%5C%5C%0A%20%20%20%20x%20+%203y%20=%202%0A%20%20%20%20%5Cend%7Bcases%7D">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5CRightarrow%0A%20%20%20%20">
</td>
<td style="width: 5px">
<img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%20%20%20%202%20&amp;%205%20&amp;%203%5C%5C%0A%20%20%20%204%20&amp;%200%20&amp;%20-2%5C%5C%0A%20%20%20%201%20&amp;%203%20&amp;%200%0A%20%20%20%20%5Cend%7Barray%7D%0A%20%20%20%20%5Cright%5D%0A%20%20%20%20">
</td>
<td style="width: 5px">
<img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%20%20%20x%5C%5C%0A%20%20%20%20y%5C%5C%0A%20%20%20%20z%0A%20%20%20%20%5Cend%7Barray%7D%0A%20%20%20%20%5Cright%5D%0A%20%20%20%20">
</td>
<td style="width: 3px">
=
</td>
<td style="width: 5px">
<img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%20%20%20-3%5C%5C%0A%20%20%20%200%5C%5C%0A%20%20%20%202%0A%20%20%20%20%5Cend%7Barray%7D%0A%20%20%20%20%5Cright%5D%0A%20%20%20%20">
</td>
</tr>
<tr>
<td>
</td>
<td>
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5CrmA">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Crvx">
</td>
<td>
=
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Crvb">
</td>
</tr>
</tbody></table>
<p>Thus, any system of linear equations can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5CrmA%20%5Crvx%20=%20%5Crvb%0A%5Cend%7Bequation%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%20n%7D"> is a known linear transformation(a matrix), <img src="https://latex.codecogs.com/png.latex?%5Crvb%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%201%7D"> is a known vector in the space of <img src="https://latex.codecogs.com/png.latex?%5CrmA">, and <img src="https://latex.codecogs.com/png.latex?%5Crvx%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%201%7D"> is an unkown vector that after the transformation <img src="https://latex.codecogs.com/png.latex?%5CrmA"> lies over <img src="https://latex.codecogs.com/png.latex?%5Crvb">.</p>
<p>Note that the existence of such unkown vector is tightly related to the determinant of <img src="https://latex.codecogs.com/png.latex?%5CrmA">:</p>
<ul>
<li>if <img src="https://latex.codecogs.com/png.latex?det(A)%20=%200">, in general, there is no such <img src="https://latex.codecogs.com/png.latex?%5Crvx">,</li>
<li>if <img src="https://latex.codecogs.com/png.latex?det(A)%20%5Cneq%200">, in general, there is one-and-only-one <img src="https://latex.codecogs.com/png.latex?%5Crvx"> that satisfy <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Crvx%20=%20%5Crvb">, namely <img src="https://latex.codecogs.com/png.latex?%5CrmA%5E%7B-1%7D">.</li>
</ul>
<p>Mathematically, the solution to <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Crvx%20=%20%5Crvb"> is <img src="https://latex.codecogs.com/png.latex?%5Crvx%20=%20%5CrmA%5E%7B-1%7D%20%5Crvb">. However, computing <img src="https://latex.codecogs.com/png.latex?%5CrmA%5E%7B-1%7D"> is a complex operation and is subject to numerical instabilities (<a href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">Don’t invert that matrix</a>). Thus, mathematicians have develop multiple solvers for that same problam that does not require to compute the matrix invers and they leverage some specific property of matrix <img src="https://latex.codecogs.com/png.latex?%5CrmA">.</p>
</section>
<section id="change-of-basis" class="level1">
<h1>Change of Basis</h1>
<p>Given a vector <img src="https://latex.codecogs.com/png.latex?%5Crvx%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%202%5Cend%7Barray%7D%5Cright%5D"> imagine this vector represented in terms of the unit vectors <img src="https://latex.codecogs.com/png.latex?%5Crvi%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%200%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%200%5C%5C%201%5Cend%7Barray%7D%5Cright%5D">, and, scale them by 3 and 2, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crvx%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%20%5Crvi%5C%5C%202%20%5Crvj%20%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
<p>However, as shown if Figure&nbsp;5, we can also represent <img src="https://latex.codecogs.com/png.latex?%5Crvx"> in terms of different basis vectors <img src="https://latex.codecogs.com/png.latex?%5Crvu%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%202%5C%5C%201%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20-1%5C%5C%201%5Cend%7Barray%7D%5Cright%5D">. That is, <img src="https://latex.codecogs.com/png.latex?%5Crvx"> can be represented as the linear combination of <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crvx%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20%5Cfrac%7B5%7D%7B3%7D%20%5Crvu%5C%5C%20%5Cfrac%7B1%7D%7B3%7D%20%5Crvv%20%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>In other words, it is possible to represent <img src="https://latex.codecogs.com/png.latex?%5Crvx"> in two different languages: one according to basis <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj">; the other according to basis <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv">.</p>
<div id="fig-base-change" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-base-change-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/base_change.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-base-change-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5
</figcaption>
</figure>
</div>
<p>As overstated, we can express <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv"> in terms of basis vectors <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crvu%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%202%5C%5C%201%5Cend%7Barray%7D%5Cright%5D%20~~~~~~%20%5Crvv%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20-1%5C%5C%201%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>or in terms of <img src="https://latex.codecogs.com/png.latex?%5Crvu"> <img src="https://latex.codecogs.com/png.latex?%5Crvv"> it-self:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crvu%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%200%5Cend%7Barray%7D%5Cright%5D%20~~~~~~%20%5Crvv%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%200%5C%5C%201%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
<p>Yet, the linear transformation <img src="https://latex.codecogs.com/png.latex?%5CrmU%20=%20%5B%5Crvu,%20%5Crvv%5D"> (composed by the collum vectors <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv">) allow to convert any vector written in terms of <img src="https://latex.codecogs.com/png.latex?%5Crvu"> <img src="https://latex.codecogs.com/png.latex?%5Crvv"> to its equivalent vector w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%202%20&amp;%20-1%5C%5C%201%20&amp;%201%5Cend%7Barray%7D%5Cright%5D%20%5Ccdot%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20%5Cfrac%7B5%7D%7B3%7D%5C%5C%20%5Cfrac%7B1%7D%7B3%7D%5Cend%7Barray%7D%5Cright%5D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%202%20%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>Similarly, we can use <img src="https://latex.codecogs.com/png.latex?%5CrmU%5E%7B-1%7D"> to convert any vector written in terms of <img src="https://latex.codecogs.com/png.latex?%5Crvi"> <img src="https://latex.codecogs.com/png.latex?%5Crvj"> to it equivalent representeation in <img src="https://latex.codecogs.com/png.latex?%5Crvu"> <img src="https://latex.codecogs.com/png.latex?%5Crvv">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%5C%5C%20-%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B2%7D%7B3%7D%5Cend%7Barray%7D%5Cright%5D%20%5Ccdot%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%202%5Cend%7Barray%7D%5Cright%5D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20%5Cfrac%7B5%7D%7B3%7D%5C%5C%20%5Cfrac%7B1%7D%7B3%7D%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
<p>More generaly, any transformation <img src="https://latex.codecogs.com/png.latex?%5CrmA"> expressed in terms of the basis <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj"> can be applyed to any vectror <img src="https://latex.codecogs.com/png.latex?%5Crvx"> defined in temrs of basis <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv"> applying the change-of-basis equation:</p>
<p><span id="eq-cob"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5B%5CrmU%5E%7B-1%7D%20%5CrmA%20%5CrmU%5D%20%5Crvx%0A%5Cend%7Bequation%7D%0A%5Ctag%7B3%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmU%5E%7B-1%7D%20%5CrmA%20%5CrmU"> express a sort of mathematical empathy between different reference systems; i.e., it converts a tranformation <img src="https://latex.codecogs.com/png.latex?%5CrmA"> to a different reference systems.</p>
</section>
<section id="eigenvectors-and-eigenvalues" class="level1">
<h1>Eigenvectors and Eigenvalues</h1>
<p>It is often convinient to study linear transformations, not on their matrix formulation, but ratehr on their base component. Among the most common decomposition methods, <strong>eigenvectors</strong> and <strong>eigenvalues</strong> are the most common matrix decomposition thecnique.</p>
<p>Given a linear transformation <img src="https://latex.codecogs.com/png.latex?L%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%203%20&amp;%201%5C%5C%200%20&amp;%202%20%5Cend%7Barray%7D%5Cright%5D">, most of the vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_i"> are rotated by <img src="https://latex.codecogs.com/png.latex?L"> away from their original span. Instead some special vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i"> are only streched or squished by <img src="https://latex.codecogs.com/png.latex?L">, but they remain on the original span. Moreover, every vector on the span of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i"> is also only scaled by <img src="https://latex.codecogs.com/png.latex?L">.</p>
<div id="fig-eigen-values" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-eigen-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/eigen_values.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-eigen-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Visualization of one of the eigenvalue of <img src="https://latex.codecogs.com/png.latex?L">. Note that, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_1"> remain on its own span, while a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is moved away from its original span.
</figcaption>
</figure>
</div>
<p>Base on the intuition shown in Figure&nbsp;6 and on the “move from the span” consepts, we can formally define the eigenvalues of a squared matrix <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%20n%7D"> as the non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5CrmA%20%5Ccdot%20%5Cmathbf%7Be%7D_i%20=%20%5Clambda%20%5Cmathbf%7Be%7D_i%0A%5Cend%7Balign%7D%0A"></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Clambda"> is known as the eigenvalue of the eigenvector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D">,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Cneq%200">,</li>
<li>if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i"> is an eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmA">, then any rescaled vector <img src="https://latex.codecogs.com/png.latex?c%20~%20%5Cmathbf%7Be%7D_i"> for <img src="https://latex.codecogs.com/png.latex?c%20%5Cin%20%5Creal,%20c%20%5Cneq%200"> is also an eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> . Thus, usually only the unit eigenvectors are considered.</li>
</ul>
<p>There is an interesting connection between eigenvectors are determinant. According to the formal definition of eigenvectors, we are tring to map a matrix to a vector. Thus, we are tring to map a volume/surface to a single line/point; which is possible only if the determinant of the matrix is 0:</p>
<p><span id="eq-eigenvectors"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5CrmA%20%5Ccdot%20%5Cmathbf%7Be%7D_i%20&amp;=%20%5Clambda%20%5Cmathbf%7Be%7D_i%20%5C%5C%0A%5CrmA%20%5Ccdot%20%5Cmathbf%7Be%7D_i%20&amp;=%20(%5CrmI%20%5Clambda)%20%5Cmathbf%7Be%7D_i%20%5Cnonumber%20%5C%5C%0A(%5CrmA%20-%20%5Clambda%20%5CrmI)%20%5Cmathbf%7Be%7D_i%20&amp;=%200%20%5Cnonumber%20%5C%5C%0A&amp;%20%5CRightarrow%20det(%5CrmA%20-%20%5Clambda%20%5CrmI)%20=%200%20%5Cnonumber%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
<p>The most important properties are:</p>
<ul>
<li>the trance of a matrix is equal to the some of its eigenvalues: <img src="https://latex.codecogs.com/png.latex?tr(%5CrmA)%20=%20%5Csum_%7Bi=0%7D%5E%7Bn-1%7D%20%5Clambda_i">,</li>
<li>the determinanto of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> is equal to the producto of its eigenvalues: <img src="https://latex.codecogs.com/png.latex?det(%5CrmA)%20=%20%5Cprod_%7Bi=0%7D%5E%7Bn-1%7D%20%5Clambda_i">.</li>
</ul>
</section>
<section id="matrix-decomposition" class="level1">
<h1>Matrix Decomposition</h1>
<p>Similarly, to how it is conveninet to express <img src="https://latex.codecogs.com/png.latex?15"> as product of its factors <img src="https://latex.codecogs.com/png.latex?5%20%5Ccdot%203">; sometimes it is convenient to express a matrix <img src="https://latex.codecogs.com/png.latex?%5CrmA"> as product of other matrixes. There are multiple method to decpompose a matrix, but they are mostly used to eficiently solve systems of linear equation.</p>
<section id="eigendecomposition" class="level2">
<h2 class="anchored" data-anchor-id="eigendecomposition">Eigendecomposition</h2>
<p>Given a squared matrix <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%20n%7D">, it is possible to rewrite Equation&nbsp;4 in matrix form as:</p>
<p><span id="eq-eigenvectors-matrix"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5CrmA%20%5CrmU%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D.%0A%5Cend%7Bequation%7D%0A%5Ctag%7B5%7D"></span></p>
<p>Moreover, according to Equation&nbsp;3, using the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> as new basis of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> will generate a diagonal matrix of eigenvalues:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5CrmU%5E%7B-1%7D%20%5CrmA%20%5CrmU%20=%20%5Cmathbf%7B%5CLambda%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%0A%5CrmU%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%20n%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20%5C%5C%0A%20%20%5Crve_1%20&amp;%20%5Cdots%20&amp;%20%5Crve_%7Bn%7D%5C%5C%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D"> is the matrix formed by the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> and</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CLambda%7D%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%20n%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%20%20%5Clambda_1%20&amp;%20%20&amp;%20%20%5C%5C%0A%20%20%20&amp;%20%5Cddots%20&amp;%20%5C%5C%0A%20%20%20&amp;%20&amp;%20%5Clambda_n%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D"> is the diagonal matrix formed by the eigenvalues assogiated to the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmA">.</p>
<p>This process of expressing <img src="https://latex.codecogs.com/png.latex?%5CrmA"> in terms of its eigenvalue and eigenvectors is know as <strong>diagonalization</strong>. If the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> are linearly indipendent, then the matrix <img src="https://latex.codecogs.com/png.latex?%5CrmU"> is invertible, thus, it is possible to <strong>decompose</strong> <img src="https://latex.codecogs.com/png.latex?%5CrmA"> as:</p>
<p><span id="eq-eigendecomposition"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5CrmA%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D%20%5CrmU%5E%7B-1%7D%20.%0A%5Cend%7Bequation%7D%0A%5Ctag%7B6%7D"></span></p>
<p>Moreover, if <img src="https://latex.codecogs.com/png.latex?%5CrmA"> is real valued and symmetric then it can be shown that <img src="https://latex.codecogs.com/png.latex?%5CrmU"> is orthonormal, i.e., <img src="https://latex.codecogs.com/png.latex?%5Crvu%5ET_i%20%5Crvu_j%20=%200"> if <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j"> and <img src="https://latex.codecogs.com/png.latex?%5Crvu%5ET_i%20%5Crvu_i%20=%201"> (or <img src="https://latex.codecogs.com/png.latex?%5CrmU%5ET%5CrmU%20=%20%5CrmU%20%5CrmU%5ET%20=%20%5CrmI">). Thus, we can futher symplify Equation&nbsp;6 as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5CrmA%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D%20%5CrmU%5ET.%0A%5Cend%7Bequation%7D%0A"></p>
<p>As a final note, it is possible to leverage such eigendecomposition to easily compute the inverse of a matrix <img src="https://latex.codecogs.com/png.latex?%5CrmA">. Since <img src="https://latex.codecogs.com/png.latex?%5CrmU%5ET%20=%20%5CrmU%5E%7B-1%7D">, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5CrmA%5E%7B-1%7D%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D%5E%7B-1%7D%20%5CrmU%5ET%20.%0A%5Cend%7Bequation%7D%0A"></p>
<section id="lagrangian-methods-for-constrained-optimization" class="level3">
<h3 class="anchored" data-anchor-id="lagrangian-methods-for-constrained-optimization">Lagrangian Methods for Constrained Optimization</h3>
<p>While eigen decomposition is commonly applied to solve systems of liear equations. It is also a powerful method for optimization subject to linear constrains (constrained optimization). That is, it can be used to solve quadratic constrained problems of the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmin_%7B%5Crvx%7D%20%5Crvx%5ET%20%5CrmH%20%5Crvx%20+%20d,%20~~%5Ctext%7Bsubject%20to%7D%20~~%20%5Crvx%5ET%20%5Crvx%20-%201%20=%200%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmH%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%20n%7D"> is symmetric. Such problems are a specific instanche of the <strong>Lagrangian method</strong>, in which an augmented objective is created to ensure the constrain satisfability:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(%5Crvx,%20%5Clambda)%20=%20%5Cmax_%7B%5Clambda%7D%20%5Cmin_%7B%5Crvx%7D%20%5Crvx%5ET%20%5CrmH%20%5Crvx%20+%20d%20-%20%5Clambda%20(%5Crvx%5ET%20%5Crvx%20-%201).%0A"></p>
<p>The optimal <img src="https://latex.codecogs.com/png.latex?%5Crvx%5E*"> that solve the problem, need to satisfy the zero-gradient condition:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cfrac%7B%5Cpartial%20L(%5Crvx,%20%5Clambda)%7D%20%7B%5Cpartial%20%5Crvx%7D%20&amp;%20=%200%20%5C%5C%0A&amp;%20=%20%5Cfrac%7B%20%5Cpartial%20%7D%20%7B%5Cpartial%20%5Crvx%7D%20%5Crvx%5ET%20%5CrmH%20%5Crvx%20%20%20+%20%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Crvx%7D%20d%20-%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Crvx%7D%20%5Clambda%20(%5Crvx%5ET%20%5Crvx%20-%201)%20%20%5C%5C%0A&amp;%20=%20%5Crvx%5ET%20(%5CrmH%20+%20%5CrmH%5ET)%20+%200%20-%202%20%5Clambda%20%5Crvx%5ET%20%20&amp;&amp;%20%7B%20%5Csmall%20%5CrmH%20=%20%5CrmH%5ET%20%5Ctext%7B%20since%20is%20symmetric.%7D%20%7D%5C%5C%0A&amp;%20=%202%20%5Crvx%5ET%20%5CrmH%20-%202%20%5Clambda%20%5Crvx%5ET%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20L(%5Crvx,%20%5Clambda)%7D%20%7B%5Cpartial%20%5Clambda%7D%20&amp;%20=%200%20%20%5C%5C%0A&amp;%20=%20%20%5Cfrac%7B%20%5Cpartial%20%7D%7B%20%5Cpartial%20%5Clambda%20%7D%20%5Crvx%5ET%20%5CrmH%20%5Crvx%20+%20%5Cfrac%7B%20%5Cpartial%20%7D%7B%20%5Cpartial%20%5Clambda%20%7D%20d%20-%20%5Cfrac%7B%20%5Cpartial%20%7D%7B%20%5Cpartial%20%5Clambda%20%7D%20%5Clambda%20(%5Crvx%5ET%20%5Crvx%20-%201)%20%5C%5C%0A&amp;%20=%200%20+%200%20-%20%5Crvx%5ET%20%5Crvx%20+%201%20%5C%5C%0A&amp;%20=%20%5Crvx%5ET%20%5Crvx%20-%201%0A%5Cend%7Balign*%7D%0A"></p>
<p>which is equivalent to the eigenvector equation Equation&nbsp;5 <img src="https://latex.codecogs.com/png.latex?%5CrmH%20%5Crvx%20=%20%5Clambda%20%5Crvx">.</p>
</section>
</section>
<section id="singular-value-decomposition-svd" class="level2">
<h2 class="anchored" data-anchor-id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h2>
<p>While eigendecomposition require squared matrices, <strong>SVD</strong> allow the factorization of rectangular matrices into <strong>singular vectors</strong> and <strong>singular values</strong>. Given any <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%20n%7D">, it is possible to depompose it as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5CrmA%20=%20%5CrmU%20%5CrmS%20%5CrmV%5ET%0A%5Cend%7Bequation%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmU%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%20m%7D"> is composed by orthonormal columns (<img src="https://latex.codecogs.com/png.latex?%5CrmU%5ET%20%5CrmU%20=%20%5CrmI">), <img src="https://latex.codecogs.com/png.latex?%5CrmV%20%5Cin%20%5Creal%5E%7Bn%20%5Ctimes%20n%7D"> is compesed by orthonormals rows and columns (<img src="https://latex.codecogs.com/png.latex?%5CrmV%5ET%5CrmV%20=%20%5CrmV%20%5CrmV%5ET%20=%20%5CrmI">), and <img src="https://latex.codecogs.com/png.latex?%5CrmS%20%5Cin%20%5Creal%5E%7Bm%20%5Ctimes%20n%7D"> is a diagonal matrix containing the <strong>singular values</strong> <img src="https://latex.codecogs.com/png.latex?%5Csigma_i%20%5Cgeq%200">. <img src="https://latex.codecogs.com/png.latex?%5CrmU"> and <img src="https://latex.codecogs.com/png.latex?%5CrmV%5ET"> are respectively known as the <strong>left singular vectors</strong> and <strong>right singular vectors</strong> of <img src="https://latex.codecogs.com/png.latex?%5CrmA"> and are obtained as the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmA%5CrmA%5ET"> and <img src="https://latex.codecogs.com/png.latex?%5CrmA%5ET%5CrmA">. Similarly, <img src="https://latex.codecogs.com/png.latex?%5CrmS"> is composed by the squared root of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5CrmA%5CrmA%5ET"> and <img src="https://latex.codecogs.com/png.latex?%5CrmA%5ET%5CrmA"> arranged in descending order.</p>
<p>For example, consider</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CrmA%20=%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%20%202%20&amp;%204%20%5C%5C%0A%20%201%20&amp;%203%20%5C%5C%0A%20%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>then we know that the columns of <img src="https://latex.codecogs.com/png.latex?%5CrmU"> are made by the eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5CrmA%20%5CrmA%5ET">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5CrmA%20%5CrmA%5ET%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%2020%20&amp;%2014%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%2014%20&amp;%2010%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%200%20&amp;%200%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%5C%5C%0A%5CrmU%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%200.82%20&amp;%20-0.58%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200.58%20&amp;%200.82%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%201%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%200%20&amp;%201%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<p>similarly, the right singular vectors are obtained as eigenvalues of <img src="https://latex.codecogs.com/png.latex?%5CrmA%5ET%20%5CrmA">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5CrmA%5ET%20%5CrmA%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%20%205%20&amp;%2011%20%5C%5C%0A%20%2011%20&amp;%2025%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%5C%5C%0A%5CrmV%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%20%200.4%20&amp;%20-0.91%20%5C%5C%0A%20%200.91%20&amp;%200.4%0A%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<p>instead, <img src="https://latex.codecogs.com/png.latex?%5CrmS"> is formed by the squared root of the eivenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmV"> or <img src="https://latex.codecogs.com/png.latex?%5CrmU">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CrmS%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%20%205.46%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200.37%20%5C%5C%0A%20%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%0A%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-youtubeEssenceLinear" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">3Blue1Brown <span>E</span>ssence of linear algebra — youtube.com</div>
</div>
<div id="ref-murphy2022probabilistic" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Murphy KP (2022) Probabilistic machine learning: An introduction. MIT press</div>
</div>
</div>


</section>

 ]]></description>
  <category>Linear Algebra</category>
  <guid>https://andompesta.github.io/posts/linear_algebra/</guid>
  <pubDate>Wed, 23 Dec 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Gradinet Descent and Backpropagation</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/gradient_descent_and_backprop/</link>
  <description><![CDATA[ 




<p>Most <strong>deep learning</strong> algorithm relay on the idea of <strong>learning</strong> some useful information from the data to solve a specific task. That is, instead of explicitly define every single instruction that a program has to perform, in machine learning, we specify an optimization routine that a program executes over a set of examples to improve its performances. By executing the optimization algorithm, a machine automatically navigates the solution space to find the best “program” that solve the given task starting from a random state: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D">. It is expectable that the initial program obtained based on the random state would not perform well on the chosen task, however, by iterating over the dataset, we can adjust <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D"> until we obtain an optimal solution.</p>
<section id="gradient-descent" class="level1">
<h1>Gradient Descent</h1>
<p>One of the most common learning algorithm is known as Gradient Descent or <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"><strong>Stochastic Gradient Descent</strong></a> (SGD) <span class="citation" data-cites="hanson1988comparing">[1]</span>. The core idea of SGD is to iteratively evaluate the difference between the obtained prediction of the model (<img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctheta%7D">), and, the desired output (<img src="https://latex.codecogs.com/png.latex?y">) utilizing a loss function <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y_%7B%5Cmathbf%7B%5Ctheta%7D%7D,%20y)">. Once the difference is known, it is possible to adjust <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to reduce the difference or prediction error.</p>
<p>Formally, SGD is composed of 3 main steps:</p>
<ol type="1">
<li>evaluate the loss function: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y_%7B%5Cmathbf%7B%5Ctheta%7D%7D,%20y)">,</li>
<li>compute the gradient of the loss function w.r.t. the model parameters: <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Cmathcal%7BL%7D_%7B%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D(y_%7B%5Ctheta%7D,%20y)%7D%7B%5Cpartial%20%5Ctheta%7D">,</li>
<li>update the model parameters (or solution) to decrease the loss function: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D%20=%20%5Cmathbf%7B%5Ctheta%7D%20-%20%5Ceta%20%5Cnabla%20%5Cmathcal%7BL%7D_%7B%5Ctheta%7D">.</li>
</ol>
<p>As it is possible to notice, such a learning algorithm requires a loss function that is continuous and differentiable; otherwise, it is not applicable. However, over the years, many efficient and effective loss functions have been proposed.</p>
</section>
<section id="backpropagation" class="level1">
<h1>Backpropagation</h1>
<p>Computing the analytical gradients for a deep learning algorithm might not be easy, and it is definitely an error-prone procedure. Luckily, over the years mathematicians manage to programmatically compute the derivate of most of the functions with a procedure known as <a href="https://en.wikipedia.org/wiki/Automatic_differentiation"><strong>algorithmic differentiation</strong></a>. The application of algorithmic differentiation to compute the SGD is known as <strong>backpropagation</strong>.</p>
<p>Supposing to have the current function <img src="https://latex.codecogs.com/png.latex?f(x,y,z)%20=%20(x%20+%20y)%20%5Ccdot%20z">. It is possible to simplify it’s computation defining an intermediate function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aq(x,%20y)%20=%20x%20+%20y%20%5CRightarrow%20f(q,%20z)%20=%20q%20%5Ccdot%20z.%0A"></p>
<p>Knowing that:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%201"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201"></li>
</ul>
<p>we can compute <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D"> by <strong>chain rule</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Ccdot%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D.%0A"></p>
<p>This operation can be seen even as a computational graph, where each node represent an operation ; and using backpropagation it is possible to compute the gradient of function <img src="https://latex.codecogs.com/png.latex?f"> w.r.t. its input variable <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">:</p>
<div id="fig-elephants" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<img src="https://andompesta.github.io/posts/gradient_descent_and_backprop/img/backprop.png" id="fig-back-prop" class="img-fluid figure-img">
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The forward and backward pass of the computational graph for the function $ f(x,y,z) = (x + y) z $. (Image taken from Andrej Karpathy slides, CS231n.)
</figcaption>
</figure>
</div>
<p>It has to be noted that, backpropagation is a local and global process. It is local since a gate, during the forward pass, can compute:</p>
<ol type="1">
<li>its output value: <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y%20=%203">,</li>
<li>as well as its local gradient (the gradient of its input w.r.t. its output): <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201">.</li>
</ol>
<p>It is global since, a gate need to know the gradient of its output node in order to evaluate the chain rules: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D">. The gradient of its ouput is known only during the backward pass, thus all the local computations need to be stored in memory; thus require a lot of memory.</p>
<p>The backward pass start by computing: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20=%201">. Then, knowing that <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%201%20%5Ccdot%20-4%20=%20-4">. Similarly, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%203">. Finally, our goal is to goal is to compute: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%20-4%20%5Ccdot%201%20=%20-4%0A"> and, <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%20-4%20%5Ccdot%201%20=%20-4%0A">.</p>
</section>
<section id="weight-decay" class="level1">
<h1>Weight Decay</h1>
<p>To achieve better generaliation performance it is well known that graient updates needs to be regularized so to have sparse or force small weights magnitude. The two most common regularizations for gradiens are L1-regularization or weight decay <span class="citation" data-cites="fastFastaiAdamW">[2]</span> (equivalent to the L2-regularization):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20-%20%5Calpha%20%5Cfrac%7B%5Cpartial%20f(x;%20%5Ctheta_t)%7D%7B%5Cpartial%20%5Ctheta_t%7D%20-%20%5Clambda%20%5Ctheta_t%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Ctheta_t"> stand for weight decay or L2-regularization. However, weight dacay and L2-regularization are equivalent only for SDG, but not for Adam or other adaptive optimizers. Instead of applying the same learning rate to all parameters, Adam apply a different learning rate to each parameters proportional to the update signals they recently recevied (a.k.a proportional to the recent gradients). As Adam uses a different learning rate per each parameters, it means that L2-regularization is not only affected by <img src="https://latex.codecogs.com/png.latex?%5Clambda"> but also from the learning rate and the momentum. Thus, Adam requires a bigger regularizer coefficent to achieve comparable performance as SGD.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-hanson1988comparing" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Hanson S, Pratt L (1988) Comparing biases for minimal network construction with back-propagation. Advances in neural information processing systems 1</div>
</div>
<div id="ref-fastFastaiAdamW" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Gugger S, Howard J Fast.ai - <span>A</span>dam<span>W</span> and <span>S</span>uper-convergence is now the fastest way to train neural nets — fast.ai</div>
</div>
</div>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://andompesta.github.io/posts/gradient_descent_and_backprop/</guid>
  <pubDate>Tue, 22 Dec 2020 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
