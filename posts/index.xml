<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Sandro Cavallari</title>
<link>https://andompesta.github.io/posts/</link>
<atom:link href="https://andompesta.github.io/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website of Sandro Cavallari</description>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Wed, 23 Dec 2020 00:00:00 GMT</lastBuildDate>
<item>
  <title>Basic Principles of Linear Algebra</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/linear_algebra/</link>
  <description><![CDATA[ 




<p>Linear algebra is the branch of math and statistics that is devoted to the study of matrices and vectors. As such, it is broadly used to model real-world problems in phisitcs and machine learning. Such post is a collections of my notes obtained from the 3Blue1Brown series on linear-algebra <span class="citation" data-cites="youtubeEssenceLinear">[1]</span> and Murphy’s new book <span class="citation" data-cites="murphy2022probabilistic">[2]</span>.</p>
<section id="basic-matrix-operations" class="level1">
<h1>Basic Matrix Operations</h1>
<ol type="1">
<li><strong>transpose</strong>: given a matrix <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D">, its transpose <img src="https://latex.codecogs.com/png.latex?A%5ET"> is obtained ‘’flipping’’ the rows and colums</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0AA%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B12%7D%20&amp;%20...%20&amp;%20a_%7B1n%7D%5C%5C%0A%20%20a_%7B21%7D%20&amp;%20a_%7B22%7D%20&amp;%20...%20&amp;%20a_%7B2n%7D%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7Bm1%7D%20&amp;%20a_%7Bm2%7D%20&amp;%20...%20&amp;%20a_%7Bmn%7D%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A%5CRightarrow%0AA%5ET%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20a_%7B11%7D%20&amp;%20a_%7B21%7D%20&amp;%20...%20&amp;%20a_%7Bm1%7D%5C%5C%0A%20%20a_%7B12%7D%20&amp;%20a_%7B22%7D%20&amp;%20...%20&amp;%20a_%7Bm2%7D%5C%5C%0A%20%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%0A%20%20a_%7B1n%7D%20&amp;%20a_%7B2n%7D%20&amp;%20...%20&amp;%20a_%7Bmn%7D%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
<p>The most important properties are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%7B(A%5ET)%7D%5ET%20=%20A"></li>
<li><img src="https://latex.codecogs.com/png.latex?(A%20%5Ccdot%20B)%5ET%20=%20B%5ET%20%5Ccdot%20A%5ET"></li>
<li><img src="https://latex.codecogs.com/png.latex?(A%20+%20B)%5ET%20=%20A%5ET%20+%20B%5ET"></li>
</ul>
<ol start="2" type="1">
<li><strong>matrix multiplication</strong>: while the summation of 2 matrixes is done element-wise. Matrix multiplication is done row-by-colum and requires matrixes of specific sizes. Given <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> and <img src="https://latex.codecogs.com/png.latex?B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20p%7D"> it is possible to define <img src="https://latex.codecogs.com/png.latex?C%20=%20A%20%5Ccdot%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20p%7D"> s.t. <img src="https://latex.codecogs.com/png.latex?c_%7Bi,j%7D%20=%20%5Csum_%7Bk=1%7D%5E%7Bn%7D%20a_%7Bik%7D%20b_%7Bkj%7D">. In other words, <img src="https://latex.codecogs.com/png.latex?C"> is a linear combination of the row of <img src="https://latex.codecogs.com/png.latex?A"> and the colum of <img src="https://latex.codecogs.com/png.latex?B">.</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0AC%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%20%20-%20&amp;%20%5Cmathbf%7Ba%7D_%7B1:%7D%20&amp;%20-%5C%5C%0A%20%20-%20&amp;%20%5Cmathbf%7Ba%7D_%7B2:%7D%20&amp;%20-%5C%5C%0A%20%20%20%20%20&amp;%20%5Cvdots%20&amp;%20%20%20%5C%5C%0A%20%20-%20&amp;%20%5Cmathbf%7Ba%7D_%7Bm:%7D%20&amp;%20-%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20&amp;%20%7C%5C%5C%0A%20%20%5Cmathbf%7Bb%7D_%7B:1%7D%20&amp;%20%5Cmathbf%7Bb%7D_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cmathbf%7Bb%7D_%7B:p%7D%5C%5C%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20&amp;%20%7C%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A=%0A%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%20%5Cmathbf%7Ba%7D_%7B1:%7D%5Cmathbf%7Bb%7D_%7B:1%7D%20&amp;%20%5Cmathbf%7Ba%7D_%7B1:%7D%5Cmathbf%7Bb%7D_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cmathbf%7Ba%7D_%7B1:%7D%5Cmathbf%7Bb%7D_%7B:p%7D%5C%5C%0A%20%20%5Cmathbf%7Ba%7D_%7B2:%7D%5Cmathbf%7Bb%7D_%7B:1%7D%20&amp;%20%5Cmathbf%7Ba%7D_%7B2:%7D%5Cmathbf%7Bb%7D_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cmathbf%7Ba%7D_%7B2:%7D%5Cmathbf%7Bb%7D_%7B:p%7D%5C%5C%0A%20%20%20%20%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%20%20%5C%5C%0A%20%20%5Cmathbf%7Ba%7D_%7Bm:%7D%5Cmathbf%7Bb%7D_%7B:1%7D%20&amp;%20%5Cmathbf%7Ba%7D_%7Bm:%7D%5Cmathbf%7Bb%7D_%7B:2%7D%20&amp;%20%5Cdots%20&amp;%20%5Cmathbf%7Ba%7D_%7Bm:%7D%5Cmathbf%7Bb%7D_%7B:p%7D%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D.%0A"></p>
<p>The most important properties are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(A%20%5Ccdot%20B)%20%5Ccdot%20C%20=%20A%20(B%20%5Ccdot%20C)"></li>
<li><img src="https://latex.codecogs.com/png.latex?A%20%5Ccdot%20(B%20+%20C)%20=%20(A%20%5Ccdot%20B)%20+%20(A%20%5Ccdot%20C)"></li>
<li>$A B B A $</li>
</ul>
<ol start="3" type="1">
<li><strong>matrix inverse</strong>: As for real numbers, the inverso of a matrix <img src="https://latex.codecogs.com/png.latex?A"> is denoted as <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> and is defined as the matrix such that: <img src="https://latex.codecogs.com/png.latex?A%20A%5E%7B-1%7D%20=%20I">. Besites being easy to define computing the inverse of a matrix is an expencive operations. Moreover, <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> exists if and only if <img src="https://latex.codecogs.com/png.latex?det(A)%20%5Cneq%200">.</li>
</ol>
<p>The most important properties are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(A%5E%7B-1%7D)%5E%7B-1%7D%20=%20A"></li>
<li><img src="https://latex.codecogs.com/png.latex?(A%20%5Ccdot%20B)%5E%7B-1%7D%20=%20B%5E%7B-1%7D%20%5Ccdot%20A%5E%7B-1%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?(A%5E%7B-1%7D)%5E%7BT%7D%20=%20(A%5E%7BT%7D)%5E%7B-1%7D%20=%20A%5E%7B-T%7D"></li>
</ul>
</section>
<section id="basis-vectors" class="level1">
<h1>Basis Vectors</h1>
<p>In linear algebra, a vector basis <img src="https://latex.codecogs.com/png.latex?B"> of a vector space <img src="https://latex.codecogs.com/png.latex?V"> is a set of vectors <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Cmathbf%7Bb%7D_1,%20...,%20%5Cmathbf%7Bb%7D_n%5C%7D"> that are linearly independent and allow to reconstruct every vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv_i%7D%20%5Cin%20V"> as a linear combination of <img src="https://latex.codecogs.com/png.latex?B">:</p>
<p><span id="eq-basis_vector"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%20%20%5Cmathbf%7Bv_i%7D%20&amp;%20=%20a_1%20%5Cmathbf%7Bb_1%7D%20+%20...%20+%20a_n%20%5Cmathbf%7Bb_n%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B1%7D"></span></p>
<p>For example, the vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%201%5C%5C%0A%20%200%0A%5Cend%7Barray%7D%0A%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D%20=%20%5Cleft%5B%0A%5Cbegin%7Barray%7D%7Bc%7D%0A%20%200%5C%5C%0A%20%201%0A%5Cend%7Barray%7D%0A%5Cright%5D"> are the most common base vectors for the vectors space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2">. Thus, it is possible to represent a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%203%5C%5C%0A%20%202%0A%5Cend%7Barray%7D%5Cright%5D"> as <img src="https://latex.codecogs.com/png.latex?3%20%5Cmathbf%7Bi%7D%20+%202%20%5Cmathbf%7Bj%7D">.</p>
<div id="fig-base-vector" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/basic_vectors.png" id="fig-base-vectors" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Vector representation by the base vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D">.
</figcaption>
</figure>
</div>
<p>The ability to represent any vector in <img src="https://latex.codecogs.com/png.latex?V"> as a linear combination of the basis vectors is a powerful concept. However, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D"> are not the only possible basis vectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2">. For example, another possible basis could be formed by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%201%5C%5C%0A%20%202%0A%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%0A%20%203%5C%5C%0A%20%20-1%0A%5Cend%7Barray%7D%5Cright%5D">. However, the representation of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bw%7D"> would be different than the one w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D">.</p>
</section>
<section id="span" class="level1">
<h1>Span</h1>
<p>The <strong>Span</strong> is defined as the set of all possible vectors that we can create given a basis set. Note that the number of basis vectors defines the dimension of our vector space.</p>
</section>
<section id="linear-transformations" class="level1">
<h1>Linear Transformations</h1>
<p>A <strong>linear transformation</strong> is equivalent to a function over vectors. That is, a linear transformation “move” an input vector to an output vector. While general transformations have complex features, linear transformations have some well-defined properties:</p>
<ol type="1">
<li>they maintain the origin of the vector space invariant</li>
<li>they map equally spaced lines to equally spaced lines (or points)</li>
</ol>
<p><span id="eq-linear-transform"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%20%20L(a_1%20%5Cmathbf%7Bi%7D%20+%20a_2%20%5Cmathbf%7Bj%7D)%20&amp;%20=%20a_1L(%5Cmathbf%7Bi%7D)%20+%20a_2L(%5Cmathbf%7Bj%7D)%0A%5Cend%7Balign%7D%0A%5Ctag%7B2%7D"></span></p>
<div id="fig-base-vector" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/transformations.png" id="fig-base-vectors" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples of the most commont linear transformations. (Image taken from <a href="https://mathigon.org/course/linear-algebra/linear-transformations"> Samuel S. Watson</a>.
</figcaption>
</figure>
</div>
<p>Thanks to their properties, it is possible to linearly transform any vector by means to its basis. In other words, given a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20-1%5C%5C%202%20%5Cend%7Barray%7D%5Cright%5D"> w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D"> and any lineart transformation <img src="https://latex.codecogs.com/png.latex?L">. It is possible to represent <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7Bx%7D)"> as a function of <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7Bi%7D)"> and <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7Bj%7D)"> (formally <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7Bx%7D)%20=%20-1%20L(%5Cmathbf%7Bi%7D)%20+%202%20L(%5Cmathbf%7Bj%7D)">).</p>
<p>For example, assume <img src="https://latex.codecogs.com/png.latex?L%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%203%5C%5C%20-2%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D">, then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AL(%5Cmathbf%7Bi%7D)%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%203%5C%5C%20-2%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%200%20%5Cend%7Barray%7D%5Cright%5D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%20-2%20%5Cend%7Barray%7D%5Cright%5D%5C%5C%0AL(%5Cmathbf%7Bj%7D)%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%203%5C%5C%20-2%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%200%5C%5C%201%20%5Cend%7Barray%7D%5Cright%5D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%200%20%5Cend%7Barray%7D%5Cright%5D%20%5C%5C%0AL(%5Cmathbf%7Bx%7D)%20&amp;=%20-1%20L(%5Cmathbf%7Bi%7D)%20+%202%20L(%5Cmathbf%7Bj%7D)%20%5C%5C%0A%20%20%20%20&amp;=%20-1%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%20-2%20%5Cend%7Barray%7D%5Cright%5D%20+%202%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%200%20%5Cend%7Barray%7D%5Cright%5D%20%5C%5C%0A%20%20%20%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%205%5C%5C%202%20%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<div id="fig-base-vector" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://andompesta.github.io/posts/linear_algebra/img/linear_algebra/linear-transform.png" id="fig-linear-transform" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-base-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Visualization of the linear transformation appled to vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.
</figcaption>
</figure>
</div>
<p>Finally, as a linear transformation is represented by a matrix, it is possible to define the <strong>composition of two or more linear transformations</strong> as he left-to-right product of the transformation matrix:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AL_2(L_1(%20%5Cmathbf%7Bx%7D%20))%20=%20%20L_2L_1(%5Cmathbf%7Bx%7D)%0A%5Cend%7Balign%7D%0A"></p>
<p>For example, if <img src="https://latex.codecogs.com/png.latex?L_1%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%20-2%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D">, <img src="https://latex.codecogs.com/png.latex?L_2%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%200%20&amp;%202%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D">. Then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AL_2(L_1(%20%5Cmathbf%7Bx%7D%20))%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%200%20&amp;%202%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5CBig%20(%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%20-2%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D%20%5CBig)%0A%5C%5C%0AL_2%20L_1(%20%5Cmathbf%7Bx%7D%20)%20&amp;=%20%5CBig%20(%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%200%20&amp;%202%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%201%20&amp;%20-2%5C%5C%201%20&amp;%200%20%5Cend%7Barray%7D%5Cright%5D%20%5CBig)%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%202%20&amp;%200%5C%5C%201%20&amp;%20-2%20%5Cend%7Barray%7D%5Cright%5D%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20x%5C%5C%20y%20%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Note that as matrix multiplication is equal to applying different linear transformations, the multiplication order does matter.</p>
</section>
<section id="determinant" class="level1">
<h1>Determinant</h1>
<p>As linear transformations alter the original vector space, it is important to evaluate by how much the original space is expanded or contracted by a given linear transformation <img src="https://latex.codecogs.com/png.latex?L">. The <strong>determinant</strong> define how much the original unit surface is changed by <img src="https://latex.codecogs.com/png.latex?L">.</p>
<p>The determinant has some interesting properties:</p>
<ol type="1">
<li>A liner transantformation with 0 determinant (<img src="https://latex.codecogs.com/png.latex?det(L)%20=%200">) means that squash all the vectrs on a single line/plane. Moreover, it also means that <img src="https://latex.codecogs.com/png.latex?L"> has linearly dependents columns.</li>
<li>The determinant can be negative if it change orientation of the space.</li>
<li>Determinant is associative: <img src="https://latex.codecogs.com/png.latex?det(L)%20%5Ccdot%20det(M)%20=%20det(L%20%5Ccdot%20M)">.</li>
</ol>
<div>
<img src="https://andompesta.github.io/posts/linear_algebra/{{site.baseurl}}/assets/img/linear_algebra/determinant.png" style="max-width: 85%">
<p style="font-size:small;">
Figure 4: Visualization of the determinant for a initial vector space defined by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bi%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bj%7D"> and the vector space obtained after applying the transformation <img src="https://latex.codecogs.com/png.latex?L">.)
</p>
</div>
</section>
<section id="system-of-linear-equations" class="level1">
<h1>System of Linear Equations</h1>
<p>It is convininet to use inear algebra to represent a system of linear equations, e.g.</p>
<table align="center">
<tbody><tr>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bcases%7D%0A%20%20%20%202x%20+%205y%20+%203z%20=%20-3%20%5C%5C%0A%20%20%20%204x%20+%208z%20=%200%20%5C%5C%0A%20%20%20%20x%20+%203y%20=%202%0A%20%20%20%20%5Cend%7Bcases%7D">
</td>
<td>
$ $
</td>
<td style="width: 5px">
$ $
</td>
<td style="width: 5px">
$ $
</td>
<td style="width: 3px">
=
</td>
<td style="width: 5px">
$ $
</td>
</tr>
<tr>
<td>
</td>
<td>
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?A">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Crvx">
</td>
<td>
=
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">
</td>
</tr>
</tbody></table>
<p>Thus, any system of linear equations can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0AA%20%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7Bb%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> is a known linear transformation(a matrix), <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%201%7D"> is a known vector in the space of <img src="https://latex.codecogs.com/png.latex?A">, and <img src="https://latex.codecogs.com/png.latex?%5Crvx%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%201%7D"> is an unkown vector that after the transformation <img src="https://latex.codecogs.com/png.latex?A"> lies over <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bb%7D">.</p>
<p>Note that the existence of such unkown vector is tightly related to the determinant of <img src="https://latex.codecogs.com/png.latex?A">: * if <img src="https://latex.codecogs.com/png.latex?det(A)%20=%200">, in general, there is no such <img src="https://latex.codecogs.com/png.latex?%5Crvx"> * if <img src="https://latex.codecogs.com/png.latex?det(A)%20%5Cneq%200">, in general, there is one-and-only-one <img src="https://latex.codecogs.com/png.latex?%5Crvx"> that satisfy $ A = $, namely <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D">.</p>
<p>Mathematically, the solution to <img src="https://latex.codecogs.com/png.latex?A%20%5Crvx%20=%20%5Cmathbf%7Bb%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Crvx%20=%20A%5E%7B-1%7D%20%5Cmathbf%7Bb%7D">. However, computing <img src="https://latex.codecogs.com/png.latex?A%5E%7B-1%7D"> is a complex operation and is subject to numerical instabilities (<a href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/">Don’t invert that matrix</a>). Thus, mathematicians have develop multiple solvers for that same problam that does not require to compute the matrix invers and they leverage some specific property of matrix <img src="https://latex.codecogs.com/png.latex?A">.</p>
</section>
<section id="change-of-basis" class="level1">
<h1>Change of Basis</h1>
<p>Given a vector <img src="https://latex.codecogs.com/png.latex?%5Crvx%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%5C%5C%202%5Cend%7Barray%7D%5Cright%5D"> imagine this vector represented in terms of the unit vectors <img src="https://latex.codecogs.com/png.latex?%5Crvi%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%200%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%200%5C%5C%201%5Cend%7Barray%7D%5Cright%5D">, and, scale them by 3 and 2, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Crvx%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%203%20%5Crvi%5C%5C%202%20%5Crvj%20%5Cend%7Barray%7D%5Cright%5D"></p>
<p>However, as shown if Fig. 5, we can also represent <img src="https://latex.codecogs.com/png.latex?%5Crvx"> in terms of different basis vectors <img src="https://latex.codecogs.com/png.latex?%5Crvu%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%202%5C%5C%201%5Cend%7Barray%7D%5Cright%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20-1%5C%5C%201%5Cend%7Barray%7D%5Cright%5D">. That is, <img src="https://latex.codecogs.com/png.latex?%5Crvx"> can be represented as the linear combination of <img src="https://latex.codecogs.com/png.latex?%5Crvu"> <img src="https://latex.codecogs.com/png.latex?%5Crvj">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Crvx%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20%5Cfrac%7B5%7D%7B3%7D%20%5Crvu%5C%5C%20%5Cfrac%7B1%7D%7B3%7D%20%5Crvv%20%5Cend%7Barray%7D%5Cright%5D"></p>
<p>In other words, it is possible to represent <img src="https://latex.codecogs.com/png.latex?%5Crvx"> in two different languages: one according to basis <img src="https://latex.codecogs.com/png.latex?%5Crvi"> <img src="https://latex.codecogs.com/png.latex?%5Crvj"> the other according to basis <img src="https://latex.codecogs.com/png.latex?%5Crvu%20%5Crvv">.</p>
<div>
<img src="https://andompesta.github.io/posts/linear_algebra/{{site.baseurl}}/assets/img/linear_algebra/base_change.png" style="max-width: 85%">
<p style="font-size:small;">
Figure 5: Visualization of the same vector <img src="https://latex.codecogs.com/png.latex?%5Crvx"> represented according to two different basis vectors.)
</p>
</div>
<p>As overstated, we can express <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv"> in terms of basis vectors <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Crvu%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%202%5C%5C%201%5Cend%7Barray%7D%5Cright%5D%20~~~~~~%20%5Crvv%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%20-1%5C%5C%201%5Cend%7Barray%7D%5Cright%5D"></p>
<p>or in terms of <img src="https://latex.codecogs.com/png.latex?%5Crvu"> <img src="https://latex.codecogs.com/png.latex?%5Crvv"> it-self:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Crvu%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%201%5C%5C%200%5Cend%7Barray%7D%5Cright%5D%20~~~~~~%20%5Crvv%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%200%5C%5C%201%5Cend%7Barray%7D%5Cright%5D"></p>
<p>Yet, the linear transformation <img src="https://latex.codecogs.com/png.latex?%5CrmU%20=%20%5B%5Crvu,%20%5Crvv%5D"> (composed by the collum vectors <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv">) allow to convert any vector written in terms of <img src="https://latex.codecogs.com/png.latex?%5Crvu"> <img src="https://latex.codecogs.com/png.latex?%5Crvv"> to its equivalent vector w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj">:</p>
<p>$ = $</p>
<p>Similarly, we can use <img src="https://latex.codecogs.com/png.latex?%5CrmU%5E%7B-1%7D"> to convert any vector written in terms of <img src="https://latex.codecogs.com/png.latex?%5Crvi"> <img src="https://latex.codecogs.com/png.latex?%5Crvj"> to it equivalent representeation in <img src="https://latex.codecogs.com/png.latex?%5Crvu"> <img src="https://latex.codecogs.com/png.latex?%5Crvv">:</p>
<p>$ = $</p>
<p>More generaly, any transformation <img src="https://latex.codecogs.com/png.latex?A"> expressed in terms of the basis <img src="https://latex.codecogs.com/png.latex?%5Crvi"> and <img src="https://latex.codecogs.com/png.latex?%5Crvj"> can be applyed to any vectror <img src="https://latex.codecogs.com/png.latex?%5Crvx"> defined in temrs of basis <img src="https://latex.codecogs.com/png.latex?%5Crvu"> and <img src="https://latex.codecogs.com/png.latex?%5Crvv"> applying the change-of-basis equation:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Clabel%7Beq:cob%7D%0A%5B%5CrmU%5E%7B-1%7D%20A%20%5CrmU%5D%20%5Crvx%0A%5Cend%7Bequation%7D"> $</p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmU%5E%7B-1%7D%20A%20%5CrmU"> express a sort of mathematical empathy between different reference systems; i.e., it converts a tranformation <img src="https://latex.codecogs.com/png.latex?A"> to a different reference systems.</p>
</section>
<section id="eigenvectors-and-eigenvalues" class="level1">
<h1>Eigenvectors and Eigenvalues</h1>
<p>It is often convinient to study linear transformations, not on their matrix formulation, but ratehr on their base component. Among the most common decomposition methods, <strong>eigenvectors</strong> and <strong>eigenvalues</strong> are the most common matrix decomposition thecnique.</p>
<p>Given a linear transformation <img src="https://latex.codecogs.com/png.latex?L%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%203%20&amp;%201%5C%5C%200%20&amp;%202%20%5Cend%7Barray%7D%5Cright%5D">, most of the vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_i"> are rotated by <img src="https://latex.codecogs.com/png.latex?L"> away from their original span. Instead some special vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i"> are only streched or squished by <img src="https://latex.codecogs.com/png.latex?L">, but they remain on the original span. Moreover, every vector on the span of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i"> is also only scaled by <img src="https://latex.codecogs.com/png.latex?L">.</p>
<div>
<img src="https://andompesta.github.io/posts/linear_algebra/{{site.baseurl}}/assets/img/linear_algebra/eigen_values.png" style="max-width: 85%">
<p style="font-size:small;">
Figure 6: Visualization of one of the eigenvalue of <img src="https://latex.codecogs.com/png.latex?L">. Note that, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_1"> remain on its own span, while a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> is moved away from its original span.)
</p>
</div>
<p>Base on the intuition shown in Fig. 6 and on the “move from the span” consepts, we can formally define the eigenvalues of a squared matrix <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D"> as the non-zero vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i">:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AA%20%5Ccdot%20%5Cmathbf%7Be%7D_i%20=%20%5Clambda%20%5Cmathbf%7Be%7D_i%0A%5Cend%7Balign%7D"> $</p>
<p>where: * <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is known as the eigenvalue of the eigenvector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D">. * $ $. * if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Be%7D_i"> is an eigenvectors of <img src="https://latex.codecogs.com/png.latex?A">, then any rescaled vector $c ~ _i $ for $ c , c $ is also an eigenvectors of <img src="https://latex.codecogs.com/png.latex?A">. Thus, usually only the unit eigenvectors are considered.</p>
<p>There is an interesting connection between eigenvectors are determinant. According to the formal definition of eigenvectors, we are tring to map a matrix to a vector. Thus, we are tring to map a volume/surface to a single line/point; which is possible only if the determinant of the matrix is 0:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Clabel%7Beq:eigenvectors%7D%0AA%20%5Ccdot%20%5Cmathbf%7Be%7D_i%20&amp;=%20%5Clambda%20%5Cmathbf%7Be%7D_i%20%5C%5C%0AA%20%5Ccdot%20%5Cmathbf%7Be%7D_i%20&amp;=%20(%5CrmI%20%5Clambda)%20%5Cmathbf%7Be%7D_i%20%5Cnonumber%20%5C%5C%0A(A%20-%20%5Clambda%20%5CrmI)%20%5Cmathbf%7Be%7D_i%20&amp;=%200%20%5Cnonumber%20%5C%5C%0A&amp;%20%5CRightarrow%20det(A%20-%20%5Clambda%20%5CrmI)%20=%200%20%5Cnonumber%0A%5Cend%7Balign%7D"> $</p>
<p>The most important properties are: * the trance of a matrix is equal to the some of its eigenvalues: <img src="https://latex.codecogs.com/png.latex?tr(A)%20=%20%5Csum_%7Bi=0%7D%5E%7Bn-1%7D%20%5Clambda_i"> * the determinanto of <img src="https://latex.codecogs.com/png.latex?A"> is equal to the producto of its eigenvalues: $ det(A) = _{i=0}^{n-1} _i$</p>
</section>
<section id="matrix-decomposition" class="level1">
<h1>Matrix Decomposition</h1>
<p>Similarly, to how it is conveninet to express <img src="https://latex.codecogs.com/png.latex?15"> as product of its factors <img src="https://latex.codecogs.com/png.latex?5%20%5Ccdot%203">; sometimes it is convenient to express a matrix <img src="https://latex.codecogs.com/png.latex?A"> as product of other matrixes. There are multiple method to decpompose a matrix, but they are mostly used to eficiently solve systems of linear equation.</p>
<section id="eigendecomposition" class="level2">
<h2 class="anchored" data-anchor-id="eigendecomposition">Eigendecomposition</h2>
<p>Given a squared matrix <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D">, it is possible to rewrite Eq. <img src="https://latex.codecogs.com/png.latex?%5Cref%7Beq:eigenvectors%7D"> in matrix form as:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Clabel%7Beq:eigenvectors_matrix%7D%0AA%20%5CrmU%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D%0A%5Cend%7Bequation%7D"> $</p>
<p>Moreover, according to Eq. <img src="https://latex.codecogs.com/png.latex?%5Cref%7Beq:cob%7D">, using the eigenvectors of <img src="https://latex.codecogs.com/png.latex?A"> as new basis of <img src="https://latex.codecogs.com/png.latex?A"> will generate a diagonal matrix of eigenvalues:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5CrmU%5E%7B-1%7D%20A%20%5CrmU%20=%20%5Cmathbf%7B%5CLambda%7D%0A%5Cend%7Bequation%7D"> $</p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmU%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20%5C%5C%0A%20%20%5Crve_1%20&amp;%20%5Cdots%20&amp;%20%5Crve_%7Bn%7D%5C%5C%0A%20%20%7C%20&amp;%20%7C%20&amp;%20%7C%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D"> is the matrix formed by the eigenvectors of <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CLambda%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D%20=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D%0A%20%20%5Clambda_1%20&amp;%20%20&amp;%20%20%5C%5C%0A%20%20%20&amp;%20%5Cddots%20&amp;%20%5C%5C%0A%20%20%20&amp;%20&amp;%20%5Clambda_n%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D"> is the diagonal matrix formed by the eigenvalues assogiated to the eigenvectors of <img src="https://latex.codecogs.com/png.latex?A">.</p>
<p>This process of expressing <img src="https://latex.codecogs.com/png.latex?A"> in terms of its eigenvalue and eigenvectors is know as <strong>diagonalization</strong>. If the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A"> are linearly indipendent, then the matrix <img src="https://latex.codecogs.com/png.latex?%5CrmU"> is invertible, thus, it is possible to <strong>decompose</strong> <img src="https://latex.codecogs.com/png.latex?A"> as:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Clabel%7Beq:eigendecomposition%7D%0AA%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D%20%5CrmU%5E%7B-1%7D%20.%0A%5Cend%7Bequation%7D"> $</p>
<p>Moreover, if <img src="https://latex.codecogs.com/png.latex?A"> is real valued and symmetric then it can be shown that <img src="https://latex.codecogs.com/png.latex?%5CrmU"> is orthonormal, i.e., <img src="https://latex.codecogs.com/png.latex?%5Crvu%5ET_i%20%5Crvu_j%20=%200"> if <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j"> and <img src="https://latex.codecogs.com/png.latex?%5Crvu%5ET_i%20%5Crvu_i%20=%201"> (or <img src="https://latex.codecogs.com/png.latex?%5CrmU%5ET%5CrmU%20=%20%5CrmU%20%5CrmU%5ET%20=%20%5CrmI">). Thus, we can futher symplify Eq. <img src="https://latex.codecogs.com/png.latex?%5Cref%7Beq:eigendecomposition%7D"> <img src="https://latex.codecogs.com/png.latex?A"> as:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0AA%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D%20%5CrmU%5ET%20.%0A%5Cend%7Bequation%7D"> $</p>
<p>As a final note, it is possible to leverage such eigendecomposition to easily compute the inverse of a matrix <img src="https://latex.codecogs.com/png.latex?A">. Since <img src="https://latex.codecogs.com/png.latex?%5CrmU%5ET%20=%20%5CrmU%5E%7B-1%7D">, we have:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0AA%5E%7B-1%7D%20=%20%5CrmU%20%5Cmathbf%7B%5CLambda%7D%5E%7B-1%7D%20%5CrmU%5ET%20.%0A%5Cend%7Bequation%7D"> $</p>
<section id="lagrangian-methods-for-constrained-optimization" class="level3">
<h3 class="anchored" data-anchor-id="lagrangian-methods-for-constrained-optimization">Lagrangian Methods for Constrained Optimization</h3>
<p>While eigen decomposition is commonly applied to solve systems of liear equations. It is also a powerful method for optimization subject to linear constrains (constrained optimization). That is, it can be used to solve quadratic constrained problems of the form:</p>
<p>$ _{} ^T + d, ~~ ~~ ^T - 1 = 0 $</p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmH%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D"> is symmetric. Such problems are a specific instanche of the <strong>Lagrangian method</strong>, in which an augmented objective is created to ensure the constrain satisfability:</p>
<p>$ L(, ) = <em>{} </em>{} ^T + d - (^T - 1) $</p>
<p>The optimal <img src="https://latex.codecogs.com/png.latex?%5Crvx%5E*"> that solve the problem, need to satisfy the zero-gradient condition:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cfrac%7B%5Cpartial%20L(%5Crvx,%20%5Clambda)%7D%20%7B%5Cpartial%20%5Crvx%7D%20&amp;%20=%200%20%5C%5C%0A&amp;%20=%20%5Cfrac%7B%20%5Cpartial%20%7D%20%7B%5Cpartial%20%5Crvx%7D%20%5Crvx%5ET%20%5CrmH%20%5Crvx%20%20%20+%20%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Crvx%7D%20d%20-%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Crvx%7D%20%5Clambda%20(%5Crvx%5ET%20%5Crvx%20-%201)%20%20%5C%5C%0A&amp;%20=%20%5Crvx%5ET%20(%5CrmH%20+%20%5CrmH%5ET)%20+%200%20-%202%20%5Clambda%20%5Crvx%5ET%20%20&amp;&amp;%20%7B%20%5Csmall%20%5CrmH%20=%20%5CrmH%5ET%20%5Ctext%7B%20since%20is%20symmetric.%7D%20%7D%5C%5C%0A&amp;%20=%202%20%5Crvx%5ET%20%5CrmH%20-%202%20%5Clambda%20%5Crvx%5ET%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20L(%5Crvx,%20%5Clambda)%7D%20%7B%5Cpartial%20%5Clambda%7D%20&amp;%20=%200%20%20%5C%5C%0A&amp;%20=%20%20%5Cfrac%7B%20%5Cpartial%20%7D%7B%20%5Cpartial%20%5Clambda%20%7D%20%5Crvx%5ET%20%5CrmH%20%5Crvx%20+%20%5Cfrac%7B%20%5Cpartial%20%7D%7B%20%5Cpartial%20%5Clambda%20%7D%20d%20-%20%5Cfrac%7B%20%5Cpartial%20%7D%7B%20%5Cpartial%20%5Clambda%20%7D%20%5Clambda%20(%5Crvx%5ET%20%5Crvx%20-%201)%20%5C%5C%0A&amp;%20=%200%20+%200%20-%20%5Crvx%5ET%20%5Crvx%20+%201%20%5C%5C%0A&amp;%20=%20%5Crvx%5ET%20%5Crvx%20-%201%0A%5Cend%7Balign*%7D"> $</p>
<p>which is equivalent to the eigenvector equation (Eq. <img src="https://latex.codecogs.com/png.latex?%5Cref%7Beq:eigenvectors_matrix%7D">) $ = $.</p>
</section>
</section>
<section id="singular-value-decomposition-svd" class="level2">
<h2 class="anchored" data-anchor-id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h2>
<p>While eigendecomposition require squared matrices, <strong>SVD</strong> allow the factorization of rectangular matrices into <strong>singular vectors</strong> and <strong>singular values</strong>. Given any <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D">, it is possible to depompose it as:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0AA%20=%20%5CrmU%20%5CrmS%20%5CrmV%5ET%0A%5Cend%7Bequation%7D"> $</p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CrmU%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20m%7D"> is composed by orthonormal columns (<img src="https://latex.codecogs.com/png.latex?%5CrmU%5ET%20%5CrmU%20=%20%5CrmI">), <img src="https://latex.codecogs.com/png.latex?%5CrmV%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D"> is compesed by orthonormals rows and columns (<img src="https://latex.codecogs.com/png.latex?%5CrmV%5ET%5CrmV%20=%20%5CrmV%20%5CrmV%5ET%20=%20%5CrmI">), and <img src="https://latex.codecogs.com/png.latex?%5CrmS%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> is a diagonal matrix containing the <strong>singular values</strong> <img src="https://latex.codecogs.com/png.latex?%5Csigma_i%20%5Cgeq%200">. <img src="https://latex.codecogs.com/png.latex?%5CrmU"> and <img src="https://latex.codecogs.com/png.latex?%5CrmV%5ET"> are respectively known as the <strong>left singular vectors</strong> and <strong>right singular vectors</strong> of <img src="https://latex.codecogs.com/png.latex?A"> and are obtained as the eigenvectors of <img src="https://latex.codecogs.com/png.latex?AA%5ET"> and <img src="https://latex.codecogs.com/png.latex?A%5ETA">. Similarly, <img src="https://latex.codecogs.com/png.latex?%5CrmS"> is composed by the squared root of the eigenvalues of <img src="https://latex.codecogs.com/png.latex?AA%5ET"> and <img src="https://latex.codecogs.com/png.latex?A%5ETA"> arranged in descending order.</p>
<p>For example, consider</p>
<p>$ A = $</p>
<p>then we know that the columns of <img src="https://latex.codecogs.com/png.latex?%5CrmU"> are made by the eigenvalues of <img src="https://latex.codecogs.com/png.latex?A%20A%5ET">:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AA%20A%5ET%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%2020%20&amp;%2014%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%2014%20&amp;%2010%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%200%20&amp;%200%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%5C%5C%0A%5CrmU%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccc%7D%0A%20%200.82%20&amp;%20-0.58%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200.58%20&amp;%200.82%20&amp;%200%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%201%20&amp;%200%20%5C%5C%0A%20%200%20&amp;%200%20&amp;%200%20&amp;%201%20%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D"> $</p>
<p>similarly, the right singular vectors are obtained as eigenvalues of <img src="https://latex.codecogs.com/png.latex?A%5ET%20A">:</p>
<p>$ <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AA%5ET%20A%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%20%205%20&amp;%2011%20%5C%5C%0A%20%2011%20&amp;%2025%5C%5C%0A%5Cend%7Barray%7D%5Cright%5D%5C%5C%0A%5CrmV%20&amp;=%20%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%0A%20%200.4%20&amp;%20-0.91%20%5C%5C%0A%20%200.91%20&amp;%200.4%0A%5Cend%7Barray%7D%5Cright%5D%0A%5Cend%7Balign*%7D"> $</p>
<p>instead, <img src="https://latex.codecogs.com/png.latex?%5CrmS"> is formed by the squared root of the eivenvectors of <img src="https://latex.codecogs.com/png.latex?%5CrmV"> or <img src="https://latex.codecogs.com/png.latex?%5CrmU">:</p>
<p>$ = $</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-youtubeEssenceLinear" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">3Blue1Brown <span>E</span>ssence of linear algebra — youtube.com</div>
</div>
<div id="ref-murphy2022probabilistic" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Murphy KP (2022) Probabilistic machine learning: An introduction. MIT press</div>
</div>
</div>


</section>

 ]]></description>
  <category>Linear Algebra</category>
  <guid>https://andompesta.github.io/posts/linear_algebra/</guid>
  <pubDate>Wed, 23 Dec 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Gradinet Descent and Backpropagation</title>
  <dc:creator>Sandro Cavallari</dc:creator>
  <link>https://andompesta.github.io/posts/gradient_descent_and_backprop/</link>
  <description><![CDATA[ 




<p>Most <strong>deep learning</strong> algorithm relay on the idea of <strong>learning</strong> some useful information from the data to solve a specific task. That is, instead of explicitly define every single instruction that a program has to perform, in machine learning, we specify an optimization routine that a program executes over a set of examples to improve its performances. By executing the optimization algorithm, a machine automatically navigates the solution space to find the best “program” that solve the given task starting from a random state: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D">. It is expectable that the initial program obtained based on the random state would not perform well on the chosen task, however, by iterating over the dataset, we can adjust <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D"> until we obtain an optimal solution.</p>
<section id="gradient-descent" class="level1">
<h1>Gradient Descent</h1>
<p>One of the most common learning algorithm is known as Gradient Descent or <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"><strong>Stochastic Gradient Descent</strong></a> (SGD) <span class="citation" data-cites="hanson1988comparing">[1]</span>. The core idea of SGD is to iteratively evaluate the difference between the obtained prediction of the model (<img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctheta%7D">), and, the desired output (<img src="https://latex.codecogs.com/png.latex?y">) utilizing a loss function <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y_%7B%5Cmathbf%7B%5Ctheta%7D%7D,%20y)">. Once the difference is known, it is possible to adjust <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to reduce the difference or prediction error.</p>
<p>Formally, SGD is composed of 3 main steps:</p>
<ol type="1">
<li>evaluate the loss function: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D(y_%7B%5Cmathbf%7B%5Ctheta%7D%7D,%20y)">,</li>
<li>compute the gradient of the loss function w.r.t. the model parameters: <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Cmathcal%7BL%7D_%7B%5Ctheta%7D%20=%20%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D(y_%7B%5Ctheta%7D,%20y)%7D%7B%5Cpartial%20%5Ctheta%7D">,</li>
<li>update the model parameters (or solution) to decrease the loss function: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D%20=%20%5Cmathbf%7B%5Ctheta%7D%20-%20%5Ceta%20%5Cnabla%20%5Cmathcal%7BL%7D_%7B%5Ctheta%7D">.</li>
</ol>
<p>As it is possible to notice, such a learning algorithm requires a loss function that is continuous and differentiable; otherwise, it is not applicable. However, over the years, many efficient and effective loss functions have been proposed.</p>
</section>
<section id="backpropagation" class="level1">
<h1>Backpropagation</h1>
<p>Computing the analytical gradients for a deep learning algorithm might not be easy, and it is definitely an error-prone procedure. Luckily, over the years mathematicians manage to programmatically compute the derivate of most of the functions with a procedure known as <a href="https://en.wikipedia.org/wiki/Automatic_differentiation"><strong>algorithmic differentiation</strong></a>. The application of algorithmic differentiation to compute the SGD is known as <strong>backpropagation</strong>.</p>
<p>Supposing to have the current function <img src="https://latex.codecogs.com/png.latex?f(x,y,z)%20=%20(x%20+%20y)%20%5Ccdot%20z">. It is possible to simplify it’s computation defining an intermediate function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aq(x,%20y)%20=%20x%20+%20y%20%5CRightarrow%20f(q,%20z)%20=%20q%20%5Ccdot%20z.%0A"></p>
<p>Knowing that:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%201"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201"></li>
</ul>
<p>we can compute <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D"> by <strong>chain rule</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Ccdot%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D.%0A"></p>
<p>This operation can be seen even as a computational graph, where each node represent an operation ; and using backpropagation it is possible to compute the gradient of function <img src="https://latex.codecogs.com/png.latex?f"> w.r.t. its input variable <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">:</p>
<div id="fig-elephants" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<img src="https://andompesta.github.io/posts/gradient_descent_and_backprop/img/backprop.png" id="fig-back-prop" class="img-fluid figure-img">
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-elephants-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The forward and backward pass of the computational graph for the function $ f(x,y,z) = (x + y) z $. (Image taken from Andrej Karpathy slides, CS231n.)
</figcaption>
</figure>
</div>
<p>It has to be noted that, backpropagation is a local and global process. It is local since a gate, during the forward pass, can compute:</p>
<ol type="1">
<li>its output value: <img src="https://latex.codecogs.com/png.latex?q%20=%20x%20+%20y%20=%203">,</li>
<li>as well as its local gradient (the gradient of its input w.r.t. its output): <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%201">.</li>
</ol>
<p>It is global since, a gate need to know the gradient of its output node in order to evaluate the chain rules: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D">. The gradient of its ouput is known only during the backward pass, thus all the local computations need to be stored in memory; thus require a lot of memory.</p>
<p>The backward pass start by computing: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20=%201">. Then, knowing that <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%201%20%5Ccdot%20-4%20=%20-4">. Similarly, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20%5Cfrac%7Bf%7D%7B%5Cpartial%20f%7D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%203">. Finally, our goal is to goal is to compute: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20x%7D%20=%20-4%20%5Ccdot%201%20=%20-4%0A"> and, <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%20=%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20%5Cfrac%7B%5Cpartial%20q%7D%7B%5Cpartial%20y%7D%20=%20-4%20%5Ccdot%201%20=%20-4%0A">.</p>
</section>
<section id="weight-decay" class="level1">
<h1>Weight Decay</h1>
<p>To achieve better generaliation performance it is well known that graient updates needs to be regularized so to have sparse or force small weights magnitude. The two most common regularizations for gradiens are L1-regularization or weight decay <span class="citation" data-cites="fastFastaiAdamW">[2]</span> (equivalent to the L2-regularization):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20-%20%5Calpha%20%5Cfrac%7B%5Cpartial%20f(x;%20%5Ctheta_t)%7D%7B%5Cpartial%20%5Ctheta_t%7D%20-%20%5Clambda%20%5Ctheta_t%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Ctheta_t"> stand for weight decay or L2-regularization. However, weight dacay and L2-regularization are equivalent only for SDG, but not for Adam or other adaptive optimizers. Instead of applying the same learning rate to all parameters, Adam apply a different learning rate to each parameters proportional to the update signals they recently recevied (a.k.a proportional to the recent gradients). As Adam uses a different learning rate per each parameters, it means that L2-regularization is not only affected by <img src="https://latex.codecogs.com/png.latex?%5Clambda"> but also from the learning rate and the momentum. Thus, Adam requires a bigger regularizer coefficent to achieve comparable performance as SGD.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body">
<div id="ref-hanson1988comparing" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Hanson S, Pratt L (1988) Comparing biases for minimal network construction with back-propagation. Advances in neural information processing systems 1</div>
</div>
<div id="ref-fastFastaiAdamW" class="csl-entry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Gugger S, Howard J Fast.ai - <span>A</span>dam<span>W</span> and <span>S</span>uper-convergence is now the fastest way to train neural nets — fast.ai</div>
</div>
</div>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://andompesta.github.io/posts/gradient_descent_and_backprop/</guid>
  <pubDate>Tue, 22 Dec 2020 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
