---
categories:
- Statistics

author: Sandro Cavallari
date: 2024-01-15
title: "Linear Regression"
bibliography: references.bib
csl: diabetologia.csl
comments:
  giscus:
    repo: quarto-dev/quarto-web
format:
  html:
    toc: true
    code-fold: true
execute:
  freeze: auto  # re-render only when source changes
---

# Linear Regression

Linear regression [@montgomery2021introduction] is a widely utilized and straightforward machine learning model. 
It establishes a linear relationship between a dependent variable (represented on the y-axis) and one or more independent variables (represented on the x-axis). 
The fundamental formulation of linear regression is as follows:

{{< include macro.tex >}}

$$
y = w_0 + w_1 x_1 + ... + w_n x_n + \epsilon
$${#eq-linear-model}

where:

- $y$ is the dependent variable, also referred to as the response, target, or outcome variable.
- $x_i$ represents the independent variables, also known as predictors or features.
- $w_i$ are the model parameters establishing a linear relationship between the independent variables and the target. These coefficients indicate the expected change in the dependent variable for a one-unit change in the respective independent variable, with all other variables held constant.
- $\epsilon$ is the error term, which account for the viariability in $y$ that can not be explained by linear model.

If we assume that the only source of error in our model would the difference between the predicted outcome ($\hat{y}$) and the actual value ($y$) of the dependent variable; then learning process would adjust the parameteris to minimise the error terms:

$$
\begin{align}
\hat{y} & = w_0 + w_1 x_1 + ... + w_n x_n \\
\epsilon &= y - \hat{y}.
\end{align}
$$ {#eq-least-square}

While directly minimising $\epsilon$ is impractical, a common alternative is to optimise the following loss function:

$$
\arg \min \mathcal{L}(y, \hat{y}; \rvw) = (y - \hat{y})^2.
$$ {#eq-sum-of-squared-residuals}

Note that [@eq-least-square] and [@eq-sum-of-squared-residuals] entail minimisation of **the vertical distance** between $\hat{y}$ and $y$, also knonw as residuals.
Thus, there is an implicit assumtion that the only possible error would be related to the model not being accurate enough in predicting the actual value, while the measurments of the independent variable are **error-free**.
Finally, this sort of model formulation is usually known as the *least-square model* as [@eq-least-square] is called least-square method and [@eq-sum-of-squared-residuals] is the sum-of-sqaured-residual loss.

```{python}
# | label: fig-least-square
# | fig-cap: "Linear model fitted to a given data distribution by means of least-square. Note that the differece between the predicted value and the given datapoint is known as residual."

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

n_datapoints = 10
np.random.seed(1)
x = 10 * np.random.rand(n_datapoints)
y = 2 * x + 1 + (10 * np.random.randn(n_datapoints))
model = LinearRegression(fit_intercept=True)
model.fit(x[:, np.newaxis], y)
# get prediction
y_pred = model.predict(x[:, np.newaxis])

residual = y - y_pred

# get prediction for best line fit
xfit = np.linspace(0, 8, 50)
y_ = model.predict(xfit[:, np.newaxis])

fig, axs = plt.subplots(1, 2, figsize=(10.5, 4))

ax = axs[0]
ax.scatter(
    x,
    y,
    label="Data points",
    edgecolors="k",
)

# plot data
ax.plot(
    [0, 8],
    [y.mean(), y.mean()],
    color="#ff7f0e",
    label="Initial fit",
)
for i in range(len(x)):
    ax.plot(
        [x[i], x[i]],
        [y[i], y.mean()],
        color="gray",
        linestyle="--",
    )
ax.grid(True)
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.legend()
ax.set_title("Initial model")

ax = axs[1]
ax.scatter(
    x,
    y,
    label="Data points",
    edgecolors="k",
)
# plot best line fit
ax.plot(
    xfit,
    y_,
    color="#2ca02c",
    label="Best fit",
)
# Optionally, plot residuals (errors)
for i in range(len(x)):
    ax.plot(
        [x[i], x[i]],
        [y[i], y_pred[i]],
        color="gray",
        linestyle="--",
    )
ax.scatter(
    x, y_pred, color="green", label="Predicted value"
)  # If you want to show where the predicted points lie on the line

ax.annotate(
    "residual",
    xy=(1, -10),
    xycoords="data",
    xytext=(0.2, 0.1),
    textcoords="axes fraction",
    va="top",
    ha="left",
    fontsize=16,
    arrowprops=dict(
        arrowstyle="->",
        facecolor="black",
    ),
)

ax.grid(True)
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.set_title("Fited model")
ax.legend()
plt.show()
```

### Least square estimation
As shown in [@fig-least-square] a linear model find the line that minimises the distnace between the fitted line and the data points.
While it is possible to use SGD to find a solution, a better solution to [@eq-sum-of-squared-residuals] can be obtained by linear algebra.
 Assuming a dataset with composed of m elements, we can rewrite the loss function as:

$$
\begin{align}
\arg \min \mathcal{L}(y, \hat{y}) &= \sum_{i=1}^m(y_i - \hat{y}_i)^2 \\
& = (\rvy^T - (\rmX \rvw^T))^2 \\
& = \rvd^2 \\
& = \rvd \rvd^T \\
& = || \rvd ||^2
\end{align}
$$ {#eq-derivation}

where:

- $\rvy^T$ is the collumn vector $\left[\begin{align} y_1 \\ \vdots \\ y_m \end{align}\right]$;
- $\rmX$ is the $m \times n$ matrix $\left[\begin{array}{ccc} 1 & x_{1,1} & \cdots & x_{n,1} \\ \vdots & \vdots & \vdots & \vdots \\ 1 & x_{1,m} & \cdots & x_{n,m}\end{array}\right]$;
- $\rvw^T$ is the is the column vector $\left[\begin{align} w_0 \\ w_1 \\ \vdots \\ w_n \end{align}\right]$;
- $\rvd$ is the row vector $\left[y_1 - \hat{y}_1, ...,  y_m - \hat{y}_m\right]$.

It is then possible to compute the optimal paramters by differenciating w.r.t. $\rvw$:
$$
\begin{align}
\frac{\partial \mathcal{L}(y, \hat{y})}{\partial \rvw^T} = & || \rvy^T - \rmX \rvw^T ||^2  = 0 \\
& 2\rmX^T (\rvy^T - \rmX \rvw^T) = 0 \\
& \rmX^T\rvy^T - \rmX^T \rmX \rvw^T = 0 \\
& \rvw^T = \frac{\rmX^T\rvy^T}{\rmX^T \rmX}
\end{align}
$$

Note that $\rmX^T \rmX$ is an $n \times n$ matrix; thus could be invertible, $\rmX^T\rvy^T$ is an $n \times 1$ column vector and $\rvw^T$ is the $n \times 1$ column vector of unknown parameters.
Finally, it is important to list some of the key assumptions of this type of model:

1. Linearity: There must be a linear relationship between the independent and dependent variables.
2. Independence: The predictors should be independent of each other. This is vital for the model's stability and interpretability. Collinearity, or correlation between variables, can lead to significant changes in the outcome variable for minor alterations in the predictor variables, contradicting the assumption of a linear relationship. Additionally, a model with independent variables is easier to interpret as each variable contributes uniquely to the prediction.
3. Homoscedasticity: The residuals should be uniformly distributed with constant variance. Without this, it becomes challenging to ensure that the model is unbiased and to conduct accurate error analysis.


## Residual Plot

Residuals plots [@sefidianUnderstandingInterpreting] [@tsai1998examination] are one of the most common methods to validate the presence of Homoscedasticity.
As shonw in [@fig-residual-plot], residuals plots disply the residual values of a regression as a function of the predicted values and helps in understanding if the model is a good fit for the given data.
Namelly residuals plot can be used for:

1. Checking Homoscedasticity: A key assumption in linear regression is that the residuals have constant variance at every level of the independent variable(s) (homoscedasticity). If the residuals fan out or form a pattern as the predicted values increase, this indicates heteroscedasticity, which can lead to inefficient estimates and affect hypothesis testing.
2. Identifying Non-Linearity: If the relationship between the variables is not linear, the residuals will often display a systematic pattern. A residuals plot can help identify such non-linear relationships, suggesting that a different model or a transformation of variables might be more appropriate.
3. Detecting Outliers: Residuals plots can reveal outliers – points that have a much larger error than the other observations. These outliers can disproportionately influence the model fit and can be indicative of data entry errors, or that the outlier is from a different population.
4. Evaluating Model Fit: If a model is a good fit for the data, the residuals should be randomly scattered around zero. Patterns or trends in the residuals suggest that the model is not capturing some aspect of the data, which could lead to biased or inaccurate predictions.
5. Checking Independence of Residuals: For a good model, the residuals should be independent of each other. If there's a pattern over time (in time series data) or a pattern associated with another variable, this is a sign that the model is missing a key piece of information.
6. Verifying Normal Distribution of Residuals: In many forms of regression analysis (like linear regression), it's assumed that residuals are normally distributed. By plotting the residuals and visually checking their distribution (or using statistical tests), we can validate this assumption. A common method is to use a Q-Q plot (quantile-quantile plot) to compare the distribution of the residuals to a normal distribution.


```{python}
# | label: fig-residual-plot
# | fig-cap: "Residual plot. As you can see, the residual are simmetrically randomly distributed and in general, there aren’t any clear patterns. Moreover, the best-fit line to the residual is almost identical to the x-axis (y=0) indicating independence bewteen residuals and predicted values."

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(7, 4))

ax.scatter(y_pred, residual, label="residual", color="gray", edgecolors="k")
ax.hlines(0, 0, 15, colors="k", linestyles="dashed")

z = np.polyfit(y_pred, residual, 1)
residual_pred = (z[0] * y_pred) + z[1]
ax.plot(y_pred, residual_pred, color="red", label="residual fit")

ax.grid(True)
ax.set_xlabel("prediction")
ax.set_ylabel("residual")
ax.legend()
```

<!-- ## Total Least Squares



Total Least Squares [@markovsky2007overview] (a.k.a Orthogonal Distance Regression) are a variant of the linear regression model specifically designed to handle situations in which we have errors not only on the mesurment of $y$, but also in the dependent variable $x$. -->


## Robust Linear Model

In the real world, datasets often suffer from multicollinearity among independent variables. One effective method to mitigate this issue is through [**Ridge Regression**](https://www.youtube.com/watch?v=Q81RR3yKn30&ab_channel=StatQuestwithJoshStarmer), described by the formula:

$$
Y = \epsilon + w_1 X_1 + ... + w_n X_n + \lambda \sum_{1 \leq i \leq n} w_i^2.
$$

Ridge Regression is essentially linear regression augmented with a penalty term. This term comprises the squared coefficients of the model, effectively minimizing their magnitude. This added penalty increases the model's bias but decreases prediction variance by imposing a normal distribution prior on the model parameters.


----

On the other hand, [**Lasso Regression**](https://www.youtube.com/watch?v=NGf0voTMlcs&ab_channel=StatQuestwithJoshStarmer) is another widely-used variation of linear regression. Similar to Ridge, Lasso adds a penalty to the loss function of the linear model. However, Lasso minimizes the absolute value of the coefficients rather than their square:

$$
Y = \epsilon + w_1 X_1 + ... + w_n X_n + \lambda \sum_{1 \leq i \leq n} |w_i|.
$$

The key distinction is that Lasso regression can reduce some coefficients to zero, producing a sparse model, whereas Ridge regression only reduces them to near zero.

In conclusion, both these regularization approaches generate a model that has a bigger bias, but better generalization capability.

## Parameter's Analysis

There are instances when we seek to comprehend how one of our predictors influences the dependent variable. Specifically, our interest lies in determining whether the parameter $w_i$ significantly affects the response variable $Y$ - that is, whether including the predictor $X_i$ leads to a notable reduction in the model's loss.

Formally, this involves testing the following hypotheses:

$$
H_0 : w_i = 0 \\
H_1 : w_i \neq 0.
$$

For clarity, let's denote $L(Y, \hat{Y}: \theta_{\not i})$ and $L(Y, \hat{Y}: \theta_{i})$ as the sum of squared residuals for models excluding and including the $i$-th predictor, respectively. Assuming independence and homoscedasticity of the model's parameters, the significance of the $i$-th predictor can be assessed using the F-test:

$$
F = \frac{ \frac{L(Y, \hat{Y}: \theta_{\not i}) - L(Y, \hat{Y}: \theta_{i})}{p_2} }{ \frac{L(Y, \hat{Y}: \theta_{i})}{n - p} } .
$$

Here, represent the degrees of freedom for the overall model and the model containing only the $i$-th predictor; while $n$ is the number of training examples.


The numerator of $F$-test represents the reduction in the residual sum of squares per additional degree of freedom utilized. The denominator is an estimate of the residual variance, serving as a measure of the model's inherent noise. An $F$-ratio of one suggests that the predictors merely contribute noise. A ratio greater than one implies meaningful contribution, or signal, from the predictors. Typically, we reject $H_0$ and conclude that the $i$-th variable significantly impacts the response if the $F$-statistic exceeds the 95th percentile of the $F$-distribution with $p_2$ and $(n-p)$ degrees of freedom. A full derivation of this result is available [here](https://grodri.github.io/glms/notes/c2s3).
 
## R-squared
The $R^2$ metric, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a regression model. 
It represents the proportion of the variance in the dependent variable that is predictable from the independent variables. 
In simpler terms, $R^2$ indicates how well the data fit the regression model (the closer the value of $R^2$ is to 1, the better the fit) and can be computed as:

$$
R^2 = \frac{\mathcal(Y, \hat{Y}; \varnothing) - \mathcal(Y, \hat{Y}; \theta)}{\mathcal(Y, \hat{Y}; \varnothing)}
$$

here $\mathcal(Y, \hat{Y}; \varnothing)$ represent the sum of squared residuals of a model wihtout parameters, a.k.a a model that alwasy predict the mean of the response variable.
Similarly, $\mathcal(Y, \hat{Y}; \theta)$ is the linear regression developed.

The $R^2$ value ranges from 0 to 1. A vale of 0 means that the model does not explain any of the variability of the response data around its mean.
On the other hand, an $R^2$  of 1 indicates that the model explains all the variability of the response data around its mean.
In conclusion $R^2$ is a simple to compute, yet informative metric to determin how much our variance our model is able to predict correctly.

## Improvements

In general it is possible to rank the model performance in terms of $\mathcal{L}$.
Thus, here are useful methods to reduce the sum of squared residuals:

1. Feature Selection: Choose relevant and significant variables to include in the model.
2. Transformation of Variables: Apply transformations (like log, square root, or inverse) to make the relationship more linear.
3. Polynomial Regression: Use higher-order terms (quadratic, cubic, etc.) if the relationship is not purely linear.
4. Interaction Terms: Include interaction terms if the effect of one variable depends on another. An interaction variable in a linear model represents the combined effect of two or more variables on the dependent variable, where the effect of one variable depends on the level of another variable. In other words, it's used to capture situations where the relationship between a predictor and the outcome changes based on the value of another predictor. Formally, an interaction variable in alinear model is defined as:

$$
Y = \epsilon + w_1 X_1 + ... + w_n X_n + w_{n+1} (X_i \cdot X_j)
$$

5. Regularization Techniques: Methods like Ridge, Lasso, or Elastic Net can help in reducing overfitting and improving prediction.
6. Residual Plots: Use residual plots to check for non-linearity, unequal error variances, and outliers.
7. Influence Measures: Identify and investigate influential observations that might disproportionately affect the model's performance.
8. Homoscedasticity Testing: Ensure that residuals have constant variance across different levels of predictors.


# References

::: {#refs}
:::
