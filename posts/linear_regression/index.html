<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sandro Cavallari">
<meta name="dcterms.date" content="2024-01-15">

<title>Linear Regression – Sandro Cavallari</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a986a95301e671fce2c6472dffc862a1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Sandro Cavallari</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">Posts</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#least-square-estimation" id="toc-least-square-estimation" class="nav-link active" data-scroll-target="#least-square-estimation">Least Square Estimation</a></li>
  <li><a href="#residual-plot" id="toc-residual-plot" class="nav-link" data-scroll-target="#residual-plot">Residual Plot</a></li>
  <li><a href="#total-least-squares" id="toc-total-least-squares" class="nav-link" data-scroll-target="#total-least-squares">Total Least Squares</a>
  <ul class="collapse">
  <li><a href="#total-least-square-estimation" id="toc-total-least-square-estimation" class="nav-link" data-scroll-target="#total-least-square-estimation">Total Least Square Estimation</a></li>
  </ul></li>
  <li><a href="#robust-linear-model" id="toc-robust-linear-model" class="nav-link" data-scroll-target="#robust-linear-model">Robust Linear Model</a>
  <ul class="collapse">
  <li><a href="#parameters-analysis" id="toc-parameters-analysis" class="nav-link" data-scroll-target="#parameters-analysis">Parameter’s Analysis</a></li>
  <li><a href="#r-squared" id="toc-r-squared" class="nav-link" data-scroll-target="#r-squared">R-squared</a></li>
  <li><a href="#improvements" id="toc-improvements" class="nav-link" data-scroll-target="#improvements">Improvements</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Linear Regression</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Statistics</div>
    <div class="quarto-category">Linear Algebra</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sandro Cavallari </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 15, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Linear regression, as discussed by Montgomery and Runger <span class="citation" data-cites="montgomery2021introduction">[<a href="#ref-montgomery2021introduction" role="doc-biblioref">1</a>]</span>, stands out as a widely employed and intuitive machine learning model. Its primary objective is to establish a linear connection between a dependent variable (plotted on the y-axis) and one or more independent variables (plotted on the x-axis). The foundational formula for linear regression is represented as:</p>
<p><span class="math display">\[
\newcommand{\rvepsilon}{\mathbf{\epsilon}}
\newcommand{\rvtheta}{\mathbf{\theta}}
\newcommand{\rva}{\mathbf{a}}
\newcommand{\rvb}{\mathbf{b}}
\newcommand{\rvc}{\mathbf{c}}
\newcommand{\rvd}{\mathbf{d}}
\newcommand{\rve}{\mathbf{e}}
\newcommand{\rvi}{\mathbf{i}}
\newcommand{\rvj}{\mathbf{j}}
\newcommand{\rvq}{\mathbf{q}}
\newcommand{\rvu}{\mathbf{u}}
\newcommand{\rvv}{\mathbf{v}}
\newcommand{\rvw}{\mathbf{w}}
\newcommand{\rvx}{\mathbf{x}}
\newcommand{\rvy}{\mathbf{y}}
\newcommand{\rvz}{\mathbf{z}}
\newcommand{\rmA}{\mathbf{A}}
\newcommand{\rmB}{\mathbf{B}}
\newcommand{\rmC}{\mathbf{C}}
\newcommand{\rmH}{\mathbf{H}}
\newcommand{\rmI}{\mathbf{I}}
\newcommand{\rmM}{\mathbf{M}}
\newcommand{\rmN}{\mathbf{N}}
\newcommand{\rmS}{\mathbf{S}}
\newcommand{\rmU}{\mathbf{U}}
\newcommand{\rmV}{\mathbf{V}}
\newcommand{\rmW}{\mathbf{W}}
\newcommand{\rmX}{\mathbf{X}}
\newcommand{\rmY}{\mathbf{Y}}
\newcommand{\rmZ}{\mathbf{Z}}
\newcommand{\real}{\mathbb{R}}
\]</span></p>
<p><span id="eq-linear-model"><span class="math display">\[
y = w_0 + w_1 x_1 + ... + w_K x_K + \epsilon
\tag{1}\]</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y\)</span> is the dependent variable, often referred to as the response, target, or outcome variable.</li>
<li><span class="math inline">\(x_k\)</span> denotes the independent variables, also recognized as predictors or features.</li>
<li><span class="math inline">\(w_k\)</span> are the model parameters that define the linear relationship between the independent variables and the target. These coefficients signify the anticipated change in the dependent variable for a one-unit change in the respective independent variable, with all other variables held constant.</li>
<li><span class="math inline">\(\epsilon\)</span> represents the error term, accounting for the variability in <span class="math inline">\(y\)</span> that can not be explained by linear model.</li>
</ul>
<p>Assuming that the sole source of error in the model is the difference between the predicted outcome <span class="math inline">\(\hat{y}\)</span> and the actual value <span class="math inline">\(y\)</span> of the dependent variable, the learning process adjusts the parameters to minimize the error terms:</p>
<p><span id="eq-least-square"><span class="math display">\[
\begin{align}
\hat{y} &amp; = w_0 + w_1 x_1 + ... + w_K x_K \\
\epsilon &amp;= y - \hat{y}.
\end{align}
\tag{2}\]</span></span></p>
<p>While direct minimization of <span class="math inline">\(\epsilon\)</span> is impractical, a common alternative is to optimize the following loss function:</p>
<p><span id="eq-sum-of-squared-residuals"><span class="math display">\[
\arg \min \mathcal{L}(y, \hat{y}; \rvw) = (y - \hat{y})^2.
\tag{3}\]</span></span></p>
<p>It’s important to note that the minimization described in <a href="#eq-sum-of-squared-residuals" class="quarto-xref">Equation&nbsp;3</a> entails minimizing <strong>the vertical distance</strong> between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span>, also knonw as residuals. Also, <a href="#eq-sum-of-squared-residuals" class="quarto-xref">Equation&nbsp;3</a> assumes that the only possible error is related to the model not accurately predicting the actual value, while the measurements of the independent variable are <strong>error-free</strong>.</p>
<div id="cell-fig-least-square" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n_datapoints <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">10</span> <span class="op">*</span> np.random.rand(n_datapoints)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> (<span class="dv">10</span> <span class="op">*</span> np.random.randn(n_datapoints))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>model.fit(x[:, np.newaxis], y)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># get prediction</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(x[:, np.newaxis])</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>residual <span class="op">=</span> y <span class="op">-</span> y_pred</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># get prediction for best line fit</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>xfit <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">50</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>y_ <span class="op">=</span> model.predict(xfit[:, np.newaxis])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="fl">10.5</span>, <span class="dv">4</span>))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>ax.scatter(</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    x,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    y,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Data points"</span>,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    edgecolors<span class="op">=</span><span class="st">"k"</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># plot data</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">8</span>],</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    [y.mean(), y.mean()],</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"#ff7f0e"</span>,</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Initial fit"</span>,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)):</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        [x[i], x[i]],</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        [y[i], y.mean()],</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        linestyle<span class="op">=</span><span class="st">"--"</span>,</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"y"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Initial model"</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>ax.scatter(</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    x,</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    y,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Data points"</span>,</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    edgecolors<span class="op">=</span><span class="st">"k"</span>,</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co"># plot best line fit</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    xfit,</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    y_,</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"#2ca02c"</span>,</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Best fit"</span>,</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Optionally, plot residuals (errors)</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)):</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>        [x[i], x[i]],</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>        [y[i], y_pred[i]],</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>        linestyle<span class="op">=</span><span class="st">"--"</span>,</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>ax.scatter(</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    x, y_pred, color<span class="op">=</span><span class="st">"green"</span>, label<span class="op">=</span><span class="st">"Predicted value"</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># If you want to show where the predicted points lie on the line</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>ax.annotate(</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    <span class="st">"residual"</span>,</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    xy<span class="op">=</span>(<span class="dv">1</span>, <span class="op">-</span><span class="dv">10</span>),</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    xycoords<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    xytext<span class="op">=</span>(<span class="fl">0.2</span>, <span class="fl">0.1</span>),</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    textcoords<span class="op">=</span><span class="st">"axes fraction"</span>,</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    va<span class="op">=</span><span class="st">"top"</span>,</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>    ha<span class="op">=</span><span class="st">"left"</span>,</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>    fontsize<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>    arrowprops<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        arrowstyle<span class="op">=</span><span class="st">"-&gt;"</span>,</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        facecolor<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"y"</span>)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Fited model"</span>)</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-least-square" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-least-square-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-least-square-output-1.png" width="856" height="376" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-least-square-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Linear model fitted to a given data distribution by means of least-square. Note that the differece between the predicted value and the given datapoint is known as residual.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Finally, it is crucial to highlight some key characteristics of this model type:</p>
<ol type="1">
<li>Linearity: There must be a linear relationship between the independent and dependent variables.</li>
<li>Independence: The predictors should be independent of each other. This is vital for the model’s stability and interpretability. Collinearity, or correlation between variables, can lead to significant changes in the outcome variable for minor alterations in the predictor variables, contradicting the assumption of a linear relationship. Additionally, a model with independent variables is easier to interpret as each variable contributes uniquely to the prediction.</li>
<li>Homoscedasticity: The residuals should be uniformly distributed with constant variance. Without this, it becomes challenging to ensure that the model is unbiased and to conduct accurate error analysis.</li>
</ol>
<section id="least-square-estimation" class="level2">
<h2 class="anchored" data-anchor-id="least-square-estimation">Least Square Estimation</h2>
<p>As illustrated in <a href="#fig-least-square" class="quarto-xref">Figure&nbsp;1</a>,a linear model seeks to find the line that minimizes the distance between the fitted line and the data points. While stochastic gradient descent is a viable optimization approach, a more concise solution based on linear algebra can be derived from <a href="#eq-sum-of-squared-residuals" class="quarto-xref">Equation&nbsp;3</a>. Linear algebra provides a more general formulation that is robust with respect to the number of independent variables. This allows for a unified solution applicable to both the bivariate and multivariate cases. Considering a dataset composed of <span class="math inline">\(N\)</span> elements, we can reformulate the loss function as follows:</p>
<p><span id="eq-derivation"><span class="math display">\[
\begin{align}
\arg \min \mathcal{L}(y, \hat{y}) &amp;= \sum_{i=1}^N(y_i - \hat{y}_i)^2 \\
&amp; = \sum_{i=1}^N \big[ (\rvy - (\rmX \rvw))^2 \big]_i &amp; \substack{\text{With an abuse of notation} \\ \text{we reduce over the row of this matrix}}\\
&amp; = \sum_{i=1}^N \big[ \rvd^2 \big] \\
&amp; = \rvd^T \rvd &amp; \substack{\text{Note that row vector multiplied by} \\ \text{a column vector return a scalar}}\\
&amp; = || \rvd ||^2
\end{align}
\tag{4}\]</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\rvy\)</span> is the collumn vector <span class="math inline">\(\left[\begin{align} y_1 \\ \vdots \\ y_N \end{align}\right]\)</span>;</li>
<li><span class="math inline">\(\rmX\)</span> is the <span class="math inline">\(N \times K+1\)</span> matrix <span class="math inline">\(\left[\begin{array}{ccc} 1 &amp; x_{1,1} &amp; \cdots &amp; x_{K,1} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; x_{1,N} &amp; \cdots &amp; x_{K,N}\end{array}\right]\)</span>;</li>
<li><span class="math inline">\(\rvw\)</span> is the is the <span class="math inline">\(k+1\)</span> column vector <span class="math inline">\(\left[\begin{align} w_0 \\ w_1 \\ \vdots \\ w_K \end{align}\right]\)</span>;</li>
<li><span class="math inline">\(\rvd^T\)</span> is the row vector <span class="math inline">\(\left[y_1 - \hat{y}_1, ...,  y_N - \hat{y}_N\right]\)</span>.</li>
</ul>
<p>It is then possible to compute the optimal paramters by differenciating w.r.t. <span class="math inline">\(\rvw\)</span>: <span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}(\rvy, \hat{\rvy})}{\partial \rvw} = &amp; \frac{\partial || \rvy - \rmX \rvw ||^2}{\partial \rvw} \\
&amp; = 2\rmX^T (\rvy - \rmX \rvw) \\
&amp; = 2 \rmX^T\rvy - 2 \rmX^T \rmX \rvw \\
&amp; 2 \rmX^T \rmX \rvw = 2 \rmX^T\rvy &amp; \substack{\text{The minumum of } \mathcal{L} \text{is given by} \\ \partial \mathcal{L} = 0 \text{ and solving by } \rvw}\\
&amp; \rvw = \frac{\rmX^T\rvy}{\rmX^T \rmX}
\end{align}
\]</span></p>
<p>Note that <span class="math inline">\(\rmX^T \rmX\)</span> is an <span class="math inline">\(K+1 \times K+1\)</span> matrix; thus could be invertible, <span class="math inline">\(\rmX^T\rvy\)</span> is an <span class="math inline">\(K+1 \times 1\)</span> column vector and <span class="math inline">\(\rvw\)</span> is the <span class="math inline">\(K+1 \times 1\)</span> column vector of unknown parameters. Finally, this solution, known as ordinary least squares, is applicable to any model with a linear formulation concerning the model’s parameters, allowing for polynomial independent variables.</p>
</section>
<section id="residual-plot" class="level1">
<h1>Residual Plot</h1>
<p>Residuals plots <span class="citation" data-cites="sefidianUnderstandingInterpreting">[<a href="#ref-sefidianUnderstandingInterpreting" role="doc-biblioref">2</a>]</span> <span class="citation" data-cites="tsai1998examination">[<a href="#ref-tsai1998examination" role="doc-biblioref">3</a>]</span> are one of the most common methods to validate the presence of Homoscedasticity. As shonw in <a href="#fig-residual-plot" class="quarto-xref">Figure&nbsp;2</a>, residuals plots disply the residual values of a regression as a function of the predicted values and helps in understanding if the model is a good fit for the given data. Namelly residuals plot can be used for:</p>
<ol type="1">
<li>Checking Homoscedasticity: A key assumption in linear regression is that the residuals have constant variance at every level of the independent variable(s) (homoscedasticity). If the residuals fan out or form a pattern as the predicted values increase, this indicates heteroscedasticity, which can lead to inefficient estimates and affect hypothesis testing.</li>
<li>Identifying Non-Linearity: If the relationship between the variables is not linear, the residuals will often display a systematic pattern. A residuals plot can help identify such non-linear relationships, suggesting that a different model or a transformation of variables might be more appropriate.</li>
<li>Detecting Outliers: Residuals plots can reveal outliers – points that have a much larger error than the other observations. These outliers can disproportionately influence the model fit and can be indicative of data entry errors, or that the outlier is from a different population.</li>
<li>Evaluating Model Fit: If a model is a good fit for the data, the residuals should be randomly scattered around zero. Patterns or trends in the residuals suggest that the model is not capturing some aspect of the data, which could lead to biased or inaccurate predictions.</li>
<li>Checking Independence of Residuals: For a good model, the residuals should be independent of each other. If there’s a pattern over time (in time series data) or a pattern associated with another variable, this is a sign that the model is missing a key piece of information.</li>
<li>Verifying Normal Distribution of Residuals: In many forms of regression analysis (like linear regression), it’s assumed that residuals are normally distributed. By plotting the residuals and visually checking their distribution (or using statistical tests), we can validate this assumption. A common method is to use a Q-Q plot (quantile-quantile plot) to compare the distribution of the residuals to a normal distribution.</li>
</ol>
<div id="cell-fig-residual-plot" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(y_pred, residual, label<span class="op">=</span><span class="st">"residual"</span>, color<span class="op">=</span><span class="st">"gray"</span>, edgecolors<span class="op">=</span><span class="st">"k"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>ax.hlines(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">15</span>, colors<span class="op">=</span><span class="st">"k"</span>, linestyles<span class="op">=</span><span class="st">"dashed"</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.polyfit(y_pred, residual, <span class="dv">1</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>residual_pred <span class="op">=</span> (z[<span class="dv">0</span>] <span class="op">*</span> y_pred) <span class="op">+</span> z[<span class="dv">1</span>]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>ax.plot(y_pred, residual_pred, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"residual fit"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"prediction"</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"residual"</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-residual-plot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residual-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-residual-plot-output-1.png" width="596" height="356" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residual-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Residual plot. As you can see, the residual are simmetrically randomly distributed and in general, there aren’t any clear patterns. Moreover, the best-fit line to the residual is almost identical to the x-axis (y=0) indicating independence bewteen residuals and predicted values.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="total-least-squares" class="level1">
<h1>Total Least Squares</h1>
<p>Total Least Squares (TLS), as outlined by <span class="citation" data-cites="markovsky2007overview">[<a href="#ref-markovsky2007overview" role="doc-biblioref">4</a>]</span>, represents a distinctive approach within the realm of linear regression models. Its design is tailored to accommodate scenarios wherein errors not only impact the measurement of the dependent variable, represented by <span class="math inline">\(y\)</span>, but also <strong>extend to the independent variable</strong>, denoted as <span class="math inline">\(x\)</span>. Consequently, Total Least Squares proves especially valuable in situations where the assumption prevails that the measurement of <span class="math inline">\(x\)</span> is subject to noise.</p>
<p>In contrast to the above mentioned approach of minimizing the error solely between <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span>, as depicted in <a href="#fig-total-least-square" class="quarto-xref">Figure&nbsp;3</a>, the TLS objective is to minimize the <strong>orthogonal distance</strong> between the given data points and the fitted line. In essence, TLS seeks the minimal perturbation to both the dependent and independent variables, ensuring the orthogonal distance between the fitted line and each data point is minimized.</p>
<p>As described in <span class="citation" data-cites="youtube">[<a href="#ref-youtube" role="doc-biblioref">5</a>]</span>, any given line represented by the equation <span class="math inline">\(ax ~~ + ~~ by = 0\)</span>, the formalization of the orthogonal distance between this line and a specific point <span class="math inline">\((x_i, y_i)\)</span> is expressed as follows:</p>
<p><span id="eq-orthogonal-distance"><span class="math display">\[
\bar{\epsilon}_i = \frac{|ax_i + by_i|}{\sqrt{a^2 + b^2}}.
\tag{5}\]</span></span></p>
<p>It is worth noting that the vector parameterized by <span class="math inline">\(\left[a , b \right]\)</span> must be perpendicular to the line <span class="math inline">\(ax + by\)</span> to satisfy the equality constraint of the line formulation. Consequently, we can reasonably assume <span class="math inline">\(\sqrt{a^2 + b^2} = 1\)</span>, as the length of this vector does not impact orthogonality. This assumption allows for a more convenient formulation of <a href="#eq-orthogonal-distance" class="quarto-xref">Equation&nbsp;5</a>:</p>
<p><span id="eq-orthogonal-distance-prime"><span class="math display">\[
\bar{\epsilon}_i^2 = \left[ \left[ x_i ~~ y_i \right] \cdot \left[\begin{align} a \\ b\end{align}\right] \right]^2.
\tag{6}\]</span></span></p>
<p>In this revised expression, the absolute value has been replaced by its squared counterpart, and a matrix formulation has been introduced for enhanced clarity. Finally, note that the model here proposed assume a centered dataset as the formulation <span class="math inline">\(ax + by = 0\)</span> imposes the line to pass through the origin.</p>
<div id="cell-fig-total-least-square" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot data</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    x,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    y,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Data points"</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    edgecolors<span class="op">=</span><span class="st">"k"</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># plot least square model</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>ax.plot(x, y_pred, color<span class="op">=</span><span class="st">"#2ca02c"</span>, label<span class="op">=</span><span class="st">"least square fit"</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>ax.scatter(x, y_pred, color<span class="op">=</span><span class="st">"#2ca02c"</span>, edgecolor<span class="op">=</span><span class="st">"k"</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># plot least square residuals (errors)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)):</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        [x[i], x[i]],</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        [y[i], y_pred[i]],</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"#2ca02c"</span>,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        linestyle<span class="op">=</span><span class="st">"--"</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co"># TOTAL LEAST SQUARE</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># center data</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> centering(Z: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    Z_min <span class="op">=</span> Z.<span class="bu">min</span>(<span class="dv">0</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    Z_max <span class="op">=</span> Z.<span class="bu">max</span>(<span class="dv">0</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    Z_norm <span class="op">=</span> (Z <span class="op">-</span> Z_min) <span class="op">/</span> (Z_max <span class="op">-</span> Z_min)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    Z_means <span class="op">=</span> Z_norm.mean(axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reverse_centering(Z_centered: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        Z_norm <span class="op">=</span> Z_centered <span class="op">+</span> Z_means</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        Z <span class="op">=</span> Z_norm <span class="op">*</span> (Z_max <span class="op">-</span> Z_min) <span class="op">+</span> Z_min</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Z</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Z_norm <span class="op">-</span> Z_means, reverse_centering</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="co"># center the data, as we use a simplified model</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.column_stack((x, y))</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>Z_centered, reverse_centering <span class="op">=</span> centering(Z)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="co"># total least square decomposition</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>e_val, e_vec <span class="op">=</span> np.linalg.eig(Z_centered.T <span class="op">@</span> Z_centered)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> e_val.argsort()[<span class="dv">0</span>]</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="co"># get minimum eigenvectors</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> e_vec[:, idx]</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> u[:, np.newaxis]</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="co"># fitted line slope. Note that, as data is centered, the line passes through the origin</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>negative_a_over_b <span class="op">=</span> (<span class="op">-</span>u[<span class="dv">0</span>] <span class="op">/</span> u[<span class="dv">1</span>])[<span class="dv">0</span>]</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a><span class="co"># generate data for better plot TLS fit line</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>Z_ <span class="op">=</span> reverse_centering(</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>    np.column_stack(</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        (</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>            np.arange(<span class="op">-</span><span class="fl">0.55</span>, <span class="fl">0.7</span>, <span class="fl">0.2</span>)[:, np.newaxis],</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>            np.arange(<span class="op">-</span><span class="fl">0.55</span>, <span class="fl">0.7</span>, <span class="fl">0.2</span>)[:, np.newaxis].dot(negative_a_over_b),</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="co"># plot TLS fitted line</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>ax.plot(</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    Z_[:, <span class="dv">0</span>],</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>    Z_[:, <span class="dv">1</span>],</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"#9467bd"</span>,</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"total least square (TLS) fit"</span>,</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>Z_centered_tls <span class="op">=</span> <span class="op">-</span>Z_centered.dot(u).dot(u.T)</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a><span class="co"># get ortogonal value for X and Y</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>X_tls_error <span class="op">=</span> Z_centered_tls[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>X_tls <span class="op">=</span> Z_centered[:, <span class="dv">0</span>][:, np.newaxis] <span class="op">+</span> X_tls_error</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>y_pred_tls <span class="op">=</span> (X_tls).dot(negative_a_over_b)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="co"># reverse centering of adjusted matrix</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>Z_centered_pred_tls <span class="op">=</span> np.column_stack((X_tls, y_pred_tls))</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>Z_pred_tls <span class="op">=</span> reverse_centering(Z_centered_pred_tls)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>ax.scatter(</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>    Z_pred_tls[:, <span class="dv">0</span>],</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>    Z_pred_tls[:, <span class="dv">1</span>],</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"#9467bd"</span>,</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>    edgecolor<span class="op">=</span><span class="st">"k"</span>,</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"TLS predictions"</span>,</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)):</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>        [Z[i, <span class="dv">0</span>], Z_pred_tls[i, <span class="dv">0</span>]],</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>        [Z[i, <span class="dv">1</span>], Z_pred_tls[i, <span class="dv">1</span>]],</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"#9467bd"</span>,</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>        linestyle<span class="op">=</span><span class="st">":"</span>,</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>ax.annotate(</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>    <span class="st">"$</span><span class="ch">\\</span><span class="st">bar{</span><span class="ch">\\</span><span class="st">epsilon}$"</span>,</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>    xy<span class="op">=</span>(<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">11.0</span>),</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>    xycoords<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>    xytext<span class="op">=</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>),</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>    textcoords<span class="op">=</span><span class="st">"axes fraction"</span>,</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>    va<span class="op">=</span><span class="st">"top"</span>,</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>    ha<span class="op">=</span><span class="st">"left"</span>,</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>    fontsize<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>    arrowprops<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>        arrowstyle<span class="op">=</span><span class="st">"-&gt;"</span>,</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>        facecolor<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Total least square vs. least square"</span>)</span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"y"</span>)</span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-total-least-square" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-total-least-square-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-total-least-square-output-1.png" width="596" height="597" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-total-least-square-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Total least square. As it is possible to see, residuals line are orthogonal w.r.t. the fitted line. Moreover, the predictions are done for both x and y variables. This is in plain contrast with the traditional least square model shown in green colors.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="total-least-square-estimation" class="level2">
<h2 class="anchored" data-anchor-id="total-least-square-estimation">Total Least Square Estimation</h2>
<p>For a dataset comprising <span class="math inline">\(N\)</span> elements, the loss function for Total Least Squares (TLS) is expressed as follows: <span id="eq-tls-loss"><span class="math display">\[
\begin{align}
\mathcal{L}(x, y; a, b) &amp; = \sum_{i=1}^N (ax_i + by_i)^2 \\
&amp; = \sum_{i=1}^N \bar{\epsilon}_i ^2 \\
&amp; = \sum_{i=1}^N \left[ \begin{bmatrix} x_1 ~~ y_1 \\ x_j ~~ y_j \\ x_N ~~ y_N \end{bmatrix} \cdot \begin{bmatrix} a \\ b \end{bmatrix} \right]^2_i   \quad \substack{\text{With an abuse of notation, a sum reduction operation} \\ \text{is applied to all the rows of the matrix}} \\
&amp; = \sum_{i=1}^N \left[ \rmZ \cdot \rvu \right]^2_i \\
&amp; = \left[ \rmZ \cdot \rvu \right]^T \cdot \left[ \rmZ \cdot \rvu \right] \quad \substack{\text{The sum of squared values of a column vector is obtained by} \\ \text{the matrix multiplication with the transpose column vector.}}\\
&amp; = || \rmZ \cdot \rvu ||^2
\end{align}
\tag{7}\]</span></span></p>
<p>As before, optimization of <a href="#eq-tls-loss" class="quarto-xref">Equation&nbsp;7</a> involves computing its derivative with respect to the model parameters analytically. However, to prevent the degenerate solution <span class="math inline">\(\rvu = 0\)</span> and adhere to the previous assumption <span class="math inline">\(||\rvu||^2 = 1\)</span>, Total Least Squares optimization becomes a constrained problem:</p>
<p><span id="eq-tls-loss-constrained"><span class="math display">\[
\begin{align}
\arg \min \mathcal{L}(\rmZ; \rvu) \\
\text{s.t.} || \rvu ||^2 = 1.
\end{align}
\tag{8}\]</span></span></p>
<p>While various methods exist for solving constrained optimization problems, Lagrangian multipliers <span class="citation" data-cites="rockafellar1993lagrange">[<a href="#ref-rockafellar1993lagrange" role="doc-biblioref">6</a>]</span> offer a common approach, transforming <a href="#eq-tls-loss-constrained" class="quarto-xref">Equation&nbsp;8</a> into a continuous optimization problem:</p>
<p><span id="eq-tls-loss-constrained-solution"><span class="math display">\[
\begin{equation}
\arg \min \mathcal{L}(\rmZ; \rvu, \lambda) = || \rmZ \cdot \rvu ||^2 + \lambda(|| \rvu ||^2 - 1).
\end{equation}
\tag{9}\]</span></span></p>
<p>Here, <span class="math inline">\(\lambda\)</span> acts as an additional parameter, known as the Lagrange multiplier, to be optimized. The analytical differentiation of <a href="#eq-tls-loss-constrained-solution" class="quarto-xref">Equation&nbsp;9</a> yields:</p>
<p><span id="eq-tls-loss-solution-1"><span class="math display">\[
\begin{align}
\frac{ \partial \mathcal{L}(\rmZ; \rvu, \lambda)}{\partial \rvu} &amp; = 0 \\
2 \rmZ^T (\rmZ \cdot \rvu) + 2\lambda \rvu &amp; = 0 \\
(\rmZ^T \cdot \rmZ) \rvu &amp; = -\lambda \rvu \quad \substack{\text{Formulation of an} \\ \text{eigenvalue associated to} (\rmZ^T \cdot \rmZ).} \\
\end{align}
\tag{10}\]</span></span></p>
<p><span id="eq-tls-loss-solution-2"><span class="math display">\[
\begin{align}
\frac{ \partial \mathcal{L}(\rmZ; \rvu, \lambda)}{\partial \lambda} &amp; = || \rvu ||^2 - 1 \\
&amp; = \rvu^T \cdot \rvu -1 \\
\rvu^T \cdot \rvu = 1
\end{align}
\tag{11}\]</span></span></p>
<p>By combining <a href="#eq-tls-loss-solution-2" class="quarto-xref">Equation&nbsp;11</a> into <a href="#eq-tls-loss-solution-1" class="quarto-xref">Equation&nbsp;10</a> we obtain: <span id="eq-tls-loss-solution-3"><span class="math display">\[
\begin{align}
\rvu^T (\rmZ^T \cdot \rmZ) \rvu &amp; = -\lambda \rvu^T \rvu \quad \substack{\text{By left-multiply } \rvu^T \text{ on both side.}} \\
(\rvu^T \rmZ^T) \cdot (\rmZ \rvu) &amp; = -\lambda \\
\left[ \rmZ \rvu \right]^T \cdot \left[ \rmZ \rvu \right] &amp; = -\lambda \\
|| \rmZ \rvu ||^2 &amp; = -\lambda \quad \substack{\text{The original objective of Equation 7.}}
\end{align}
\tag{12}\]</span></span></p>
<p>From <a href="#eq-tls-loss-solution-1" class="quarto-xref">Equation&nbsp;10</a>, we deduce that <span class="math inline">\(\rvu\)</span> and <span class="math inline">\(\lambda\)</span> must be an eigenvector and eigenvalue of a <span class="math inline">\(2 \times 2\)</span> matrix, specifically <span class="math inline">\(\rmZ^T \rmZ\)</span>. Moreover, <a href="#eq-tls-loss-solution-3" class="quarto-xref">Equation&nbsp;12</a> establishes that <span class="math inline">\(\lambda\)</span> is equal to the original loss function aimed to be minimized in <a href="#eq-tls-loss" class="quarto-xref">Equation&nbsp;7</a>. Consequently, the estimated optimal parameters for a Total Least Squares model are equivalent to the eigenvector associated with the smallest eigenvalue of the matrix <span class="math inline">\(\rmZ^T \rmZ\)</span>.</p>
</section>
</section>
<section id="robust-linear-model" class="level1">
<h1>Robust Linear Model</h1>
<p>Real-world datasets frequently encounter multicollinearity among independent variables. One effective approach to address this issue is through <a href="https://www.youtube.com/watch?v=Q81RR3yKn30&amp;ab_channel=StatQuestwithJoshStarmer"><strong>Ridge Regression</strong></a>, characterized by the formula:</p>
<p><span id="eq-ridge-regression"><span class="math display">\[
y = \epsilon + w_0 + w_1 x_1 + ... + w_K x_K + \lambda \sum_{1 \leq k \leq K} w_k^2.
\tag{13}\]</span></span></p>
<p>Ridge Regression essentially augments linear regression with a penalty term, where this term consists of the squared coefficients of the model. Consequently, it minimizes the magnitude of the coefficients, introducing a bias to the model but simultaneously reducing prediction variance by imposing a normal distribution prior on the model parameters.</p>
<p>The derivative of the loss defined in Equation <a href="#eq-sum-of-squared-residuals" class="quarto-xref">Equation&nbsp;3</a> for a Ridge Regression model can be expressed as:</p>
<p><span id="eq-ridge-regression-loss"><span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}(y, \hat{y})} {\partial w_k} &amp; = \frac{\partial \sum_{i=1}^N(y_i - \hat{y}_i)^2 + \lambda \sum_{k=1}^K w_k^2}{\partial w_k} \\
&amp; = \frac{\partial \sum_{i=1}^N \left(y_i - w_0 - w_1 x_1 - ... - w_K x_K \right)^2 + \lambda \sum_{k=1}^K w_k^2}{\partial w_k} \\
&amp; = -2 \sum_{i=1}^N \left(y_i - w_0 - w_1 x_1 - ... - w_K x_K \right) + 2 \lambda \sum_{k=1}^K w_k.
\end{align}
\tag{14}\]</span></span></p>
<p><a href="#eq-ridge-regression-loss" class="quarto-xref">Equation&nbsp;14</a> illustrates that Ridge Regression is akin to linear regression but with smaller-weighted coefficients. Additionally, due to its quadratic formulation, Ridge Regression ensures that no model weights are zeroed out, making it a preferable choice when a dense model is desidered.</p>
<hr>
<p><a href="https://www.youtube.com/watch?v=NGf0voTMlcs&amp;ab_channel=StatQuestwithJoshStarmer"><strong>Lasso Regression</strong></a> another widely employed variant of linear regression, incorporates a penalty term into the linear model’s loss function. Unlike Ridge Regression, Lasso minimizes the absolute values of the coefficients: <span class="math display">\[
y = \epsilon + w_0 + w_1 x_1 + ... + w_K x_K + \lambda \sum_{1 \leq k \leq K} |w_k|.
\]</span></p>
<p>The weight update for Lasso Regression is computed as:</p>
<p><span id="eq-lasso-regression-loss"><span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}(y, \hat{y})} {\partial w_k} &amp; = \frac{\partial \sum_{i=1}^N(y_i - \hat{y}_i)^2 + \lambda \sum_{k=1}^K |w_k|}{\partial w_k} \\
&amp; = \frac{\partial \sum_{i=1}^N \left(y_i - w_0 - w_1 x_1 - ... - w_K x_K \right)^2 + \lambda \sum_{k=1}^K |w_k|}{\partial w_k}.
\end{align}
\tag{15}\]</span></span></p>
<p>The differentiation of the absolute value function poses challenges when <span class="math inline">\(w_k = 0\)</span> as it is not defined. However, an approximate solution of <a href="#eq-lasso-regression-loss" class="quarto-xref">Equation&nbsp;15</a> is defined by case:</p>
<p><span id="eq-lasso-regression-loss-2"><span class="math display">\[
\frac{\partial \mathcal{L}(y, \hat{y})} {\partial w_k} = \begin{cases}
0 \quad \text{if } - \frac{\lambda}{2} &lt; \frac{\partial \mathcal{L}(y, \hat{y})} {\partial w_k} &lt; \frac{\lambda}{2} \\
-2 \sum_{i=1}^N \left(y_i - w_0 - w_1 x_1 - ... - w_K x_K \right) + \lambda \quad \text{otherwise}
\end{cases}.
\tag{16}\]</span></span></p>
<p>From <a href="#eq-lasso-regression-loss-2" class="quarto-xref">Equation&nbsp;16</a>, it is evident that Lasso Regression can drive some coefficients to zero, resulting in a sparse model. In contrast to Ridge Regression, which merely reduces coefficients to values near zero, Lasso Regression is a favorable choice when a sparse model is desired or when feature selection is necessary.</p>
<section id="parameters-analysis" class="level2">
<h2 class="anchored" data-anchor-id="parameters-analysis">Parameter’s Analysis</h2>
<p>There are instances when we seek to comprehend how one of our predictors influences the dependent variable. Specifically, our interest lies in determining whether the parameter <span class="math inline">\(w_i\)</span> significantly affects the response variable <span class="math inline">\(y\)</span> - that is, whether including the predictor <span class="math inline">\(x_i\)</span> leads to a notable reduction in the model’s loss.</p>
<p>Formally, this involves testing the following hypotheses:</p>
<p><span class="math display">\[
\begin{align*}
H_0 : w_i = 0 \\
H_1 : w_i \neq 0.
\end{align*}
\]</span></p>
<p>For clarity, let’s denote <span class="math inline">\(\mathcal{L}(y, \hat{y}: \rvw^T_{\not i})\)</span> and <span class="math inline">\(\mathcal{L}(y, \hat{y}: \rvw^T)\)</span> as the sum of squared residuals for models excluding and including the <span class="math inline">\(i\)</span>-th predictor, respectively. Assuming independence and homoscedasticity of the model’s parameters, the significance of the <span class="math inline">\(i\)</span>-th predictor can be assessed using the F-test:</p>
<p><span class="math display">\[
F = \frac{ \frac{\mathcal{L}(y, \hat{y}: \rvw^T_{\not i}) - \mathcal{L}(y, \hat{y}: \rvw^T)}{p_2} }{ \frac{\mathcal{L}(y, \hat{y}: \rvw^T)}{N - p} } .
\]</span></p>
<p>Here, <span class="math inline">\(p\)</span> and <span class="math inline">\(p_2\)</span> represent the degrees of freedom for the overall model and the model containing only the <span class="math inline">\(i\)</span>-th predictor; while <span class="math inline">\(N\)</span> is the number of training examples. The numerator of <span class="math inline">\(F\)</span>-test represents the reduction in the residual sum of squares per additional degree of freedom utilized. The denominator is an estimate of the residual variance, serving as a measure of the model’s inherent noise. An <span class="math inline">\(F\)</span>-ratio of one suggests that the predictors merely contribute noise. A ratio greater than one implies meaningful contribution, or signal, from the predictors. Typically, we reject <span class="math inline">\(H_0\)</span> and conclude that the <span class="math inline">\(i\)</span>-th variable significantly impacts the response if the <span class="math inline">\(F\)</span>-statistic exceeds the 95th percentile of the <span class="math inline">\(F\)</span>-distribution. A full derivation of this result is available <a href="https://grodri.github.io/glms/notes/c2s3">here</a>.</p>
</section>
<section id="r-squared" class="level2">
<h2 class="anchored" data-anchor-id="r-squared">R-squared</h2>
<p>The <span class="math inline">\(R^2\)</span> metric, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a regression model. It represents the proportion of the variance in the dependent variable that is predictable from the independent variables. In simpler terms, <span class="math inline">\(R^2\)</span> indicates how well the data fit the regression model (the closer the value of <span class="math inline">\(R^2\)</span> is to 1, the better the fit) and can be computed as:</p>
<p><span class="math display">\[
R^2 = \frac{\mathcal{L}(y, \hat{y}: \varnothing) - \mathcal{L}(y, \hat{y}: \rvw^T)}{\mathcal{L}(y, \hat{y}: \varnothing)}
\]</span></p>
<p>here <span class="math inline">\(\mathcal{L}(y, \hat{y}: \varnothing)\)</span> represent the sum of squared residuals of a model wihtout parameters, a.k.a a model that alwasy predict the mean of the response variable. Similarly, <span class="math inline">\(\mathcal{L}(y, \hat{y}: \rvw^T)\)</span> is the linear regression developed.</p>
<p>The <span class="math inline">\(R^2\)</span> value ranges from 0 to 1. A vale of 0 means that the model does not explain any of the variability of the response data around its mean. On the other hand, an <span class="math inline">\(R^2\)</span> of 1 indicates that the model explains all the variability of the response data around its mean. In conclusion <span class="math inline">\(R^2\)</span> is a simple to compute, yet informative metric to determin how much our variance our model is able to predict correctly.</p>
</section>
<section id="improvements" class="level2">
<h2 class="anchored" data-anchor-id="improvements">Improvements</h2>
<p>In general it is possible to rank the model performance in terms of <span class="math inline">\(\mathcal{L}\)</span>. Thus, here are useful methods to reduce the sum of squared residuals:</p>
<ol type="1">
<li>Feature Selection: Choose relevant and significant variables to include in the model.</li>
<li>Transformation of Variables: Apply transformations (like log, square root, or inverse) to make the relationship more linear.</li>
<li>Polynomial Regression: Use higher-order terms (quadratic, cubic, etc.) if the relationship is not purely linear.</li>
<li>Interaction Terms: Include interaction terms if the effect of one variable depends on another. An interaction variable in a linear model represents the combined effect of two or more variables on the dependent variable, where the effect of one variable depends on the level of another variable. In other words, it’s used to capture situations where the relationship between a predictor and the outcome changes based on the value of another predictor. Formally, an interaction variable in alinear model is defined as:</li>
</ol>
<p><span class="math display">\[
y = \epsilon + w_0 + w_1 x_1 + ... + w_n x_n (x_i \cdot x_j)
\]</span></p>
<ol start="5" type="1">
<li>Regularization Techniques: Methods like Ridge, Lasso, or Elastic Net can help in reducing overfitting and improving prediction.</li>
<li>Residual Plots: Use residual plots to check for non-linearity, unequal error variances, and outliers.</li>
<li>Influence Measures: Identify and investigate influential observations that might disproportionately affect the model’s performance.</li>
<li>Homoscedasticity Testing: Ensure that residuals have constant variance across different levels of predictors.</li>
</ol>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-montgomery2021introduction" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Montgomery DC, Peck EA, Vining GG (2021) Introduction to linear regression analysis. John Wiley &amp; Sons</div>
</div>
<div id="ref-sefidianUnderstandingInterpreting" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><span>U</span>nderstanding and interpreting <span>R</span>esiduals <span>P</span>lot for linear regression - <span>A</span>mir <span>M</span>asoud <span>S</span>efidian - <span>S</span>efidian <span>A</span>cademy — sefidian.com</div>
</div>
<div id="ref-tsai1998examination" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Tsai C-L, Cai Z, Wu X (1998) The examination of residual plots. Statistica Sinica 445–465</div>
</div>
<div id="ref-markovsky2007overview" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Markovsky I, Van Huffel S (2007) Overview of total least-squares methods. Signal processing 87(10):2283–2302</div>
</div>
<div id="ref-youtube" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Farid H (2023) Supervised learning: Regression: Total least squares: Line fitting. <a href="https://www.youtube.com/watch?v=zstTZf5AiWE&amp;ab_channel=HanyFarid%2CProfessoratUCBerkeley">https://www.youtube.com/watch?v=zstTZf5AiWE&amp;ab_channel=HanyFarid%2CProfessoratUCBerkeley</a></div>
</div>
<div id="ref-rockafellar1993lagrange" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Rockafellar RT (1993) Lagrange multipliers and optimality. SIAM review 35(2):183–238</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/andompesta\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "quarto-dev/quarto-web";
    script.dataset.repoId = "MDEwOlJlcG9zaXRvcnkzNDc2MzMzNTg=";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOFLh2zs4CBQCO";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->




</body></html>