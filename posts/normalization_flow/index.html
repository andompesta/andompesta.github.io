<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sandro Cavallari">
<meta name="dcterms.date" content="2023-11-17">

<title>Normalizing Flows – Sandro Cavallari</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-d3a9a206728f765718155175211757d5.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Sandro Cavallari</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">Posts</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#generative-process-as-finate-composition-of-transformations" id="toc-generative-process-as-finate-composition-of-transformations" class="nav-link active" data-scroll-target="#generative-process-as-finate-composition-of-transformations">Generative Process as Finate Composition of Transformations</a></li>
  <li><a href="#training-procedures" id="toc-training-procedures" class="nav-link" data-scroll-target="#training-procedures">Training Procedures</a>
  <ul class="collapse">
  <li><a href="#training-example" id="toc-training-example" class="nav-link" data-scroll-target="#training-example">Training Example</a></li>
  <li><a href="#d-training-example" id="toc-d-training-example" class="nav-link" data-scroll-target="#d-training-example">2D Training Example</a></li>
  </ul></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits">Credits</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Normalizing Flows</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Deep Learning</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sandro Cavallari </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 17, 2023</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Normalizing Flows (NF) represent a potent technique that facilitates the learning and sampling from intricate probability distributions <span class="citation" data-cites="normalization-flow-review">[<a href="#ref-normalization-flow-review" role="doc-biblioref">1</a>]</span> <span class="citation" data-cites="dinh2014nice">[<a href="#ref-dinh2014nice" role="doc-biblioref">2</a>]</span>. These models, categorized as generative models, enable the precise estimation of likelihood for continuous input data, denoted as <span class="math inline">\(p(x)\)</span>. In contrast to methods such as variational inference that rely on approximations, normalizing flows function by transforming samples from a simple distribution, denoted as <span class="math inline">\(z \sim p(z)\)</span>, into samples from a more complex distribution using the following transformation:</p>
<p><span class="math display">\[
x = f_{\theta}(z), ~~ z \sim p(z; \psi).
\]</span></p>
<p>Here, <span class="math inline">\(f_{\theta}(\cdot)\)</span> is a mapping function from <span class="math inline">\(z\)</span> to <span class="math inline">\(x\)</span>, parametrized by <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(p(z; \psi)\)</span> is the base distribution (sometimes referred to as the prior distribution), parametrized by <span class="math inline">\(\psi\)</span>, from which samples can be drawn. The essential properties defining a normalizing flow include:</p>
<p>The defining propertires of a normalizing flow are:</p>
<ul>
<li><span class="math inline">\(f_{\theta}(\cdot)\)</span> must be invertible,</li>
<li>Both <span class="math inline">\(f_{\theta}(\cdot)\)</span> and <span class="math inline">\(f_{\theta}^{-1}(\cdot)\)</span> must be differentiable.</li>
</ul>
<p>Adhering to these constraints ensures the well-defined density of <span class="math inline">\(x\)</span>, as established by the change-of-variable theorem <span class="citation" data-cites="change-of-variable">[<a href="#ref-change-of-variable" role="doc-biblioref">3</a>]</span>:</p>
<p><span class="math display">\[
\begin{align*}
    \int p(x) \partial x &amp;= \int p(z) \partial z = 1 \\
    \implies p(x) &amp; = p(z) \cdot |\frac{\partial z}{\partial x}| \\
    &amp; = p\big(f_{\theta}^{-1}(x)\big) \cdot |\frac{\partial f_{\theta}^{-1}(x)}{\partial x}| \\
\end{align*}
\]</span></p>
<p>In its definition, <span class="math inline">\(\partial x\)</span> represents the width of an infinitesimally small rectangle with height <span class="math inline">\(p(x)\)</span>. Consequently, <span class="math inline">\(\frac{\partial f_{\theta}^{-1}(x)}{\partial x}\)</span> denotes the ratio between the areas of rectangles defined in two distinct coordinate systems: one in terms of <span class="math inline">\(x\)</span> and the other in terms of <span class="math inline">\(z\)</span>.<br>
For illustrative purposes, consider <a href="#fig-change-of-variable" class="quarto-xref">Figure&nbsp;1</a>, which depicts how the affine transformation <span class="math inline">\(f_{\theta}^{-1}(x) = (5 \cdot x) - 2\)</span> maps the Normal distribution <span class="math inline">\(p(x; \mu=0, \sigma=1)\)</span> to another Gaussian distribution <span class="math inline">\(p(z; \mu=-2, \sigma=5)\)</span>. With <span class="math inline">\(\frac{\partial z}{\partial x} = 5\)</span>, the area <span class="math inline">\(\partial x\)</span> undergoes a stretching factor of 5 when transformed into the variable <span class="math inline">\(z\)</span>. Consequently, <span class="math inline">\(p(z)\)</span> must be lowered by a factor of 5 to maintain its validity as a probability density function, satisfying the condition <span class="math inline">\(\int p(z) \partial z = 1\)</span>:</p>
<p><span class="math display">\[
p(z) = \frac{p(x)}{\frac{\partial f_{\theta}^{-1}(x)}{\partial x}} = \frac{p(x)}{f_{\theta}^{-1'}(x)}.
\]</span></p>
<div id="fig-change-of-variable" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-change-of-variable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/change-of-variable.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-change-of-variable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Illustration of a Change-of-Variable. The random variable <span class="math inline">\(x\)</span> undergoes a transformation into another variable <span class="math inline">\(z\)</span> through the affine function <span class="math inline">\(f_{\theta}^{-1}(x) = 5x - 2\)</span>; equivalently, <span class="math inline">\(z\)</span> can be expressed as <span class="math inline">\(5x-2\)</span>. Ensuring the validity of the density function <span class="math inline">\(p(z)\)</span> requires satisfying the condition <span class="math inline">\(\int p(z) \partial z = 1\)</span>. However, due to the stretching effect of the transformation <span class="math inline">\(f_{\theta}^{-1'}(x)\)</span> by a factor of 5, the density must be adjusted accordingly.<br>
Take note of the disparity between the maximum values of <span class="math inline">\(p(z)\)</span> and <span class="math inline">\(p(x)\)</span> for a visual representation, as depicted in the lower-left image illustrating the stretching of <span class="math inline">\(\partial x\)</span> caused by the transformation <span class="math inline">\(f_{\theta}^{-1}(\cdot)\)</span>.
</figcaption>
</figure>
</div>
<p>In the preceding paragraph, we introduced the concept of area-preserving transformations. Extending this notion to the multidimensional space involves considering <span class="math inline">\(\frac{\partial z}{\partial x}\)</span> not as a simple derivative but as the <strong>Jacobian</strong> matrix:</p>
<p><span class="math display">\[
J_{z}(x) = \begin{bmatrix}
    \frac{\partial z_1}{\partial x_1} &amp; \dots &amp; \frac{\partial z_1}{\partial x_D}\\
    \vdots &amp; \ddots &amp;  \vdots \\
    \frac{\partial z_D}{\partial x_1} &amp; \dots &amp; \frac{\partial z_D}{\partial x_D}
\end{bmatrix}.
\]</span></p>
<p>In the multidimensional setting, the difference in areas translates to a difference in volumes quantified by the determinant of the Jacobian matrix, denoted as <span class="math inline">\(det(J_{z}(x)) \approx \frac{Vol(z)}{Vol(x)}\)</span>. Consolidating these concepts, we can formalize a multidimensional normalization flow as follows:</p>
<p><span class="math display">\[
\begin{align*}
p(x) &amp; = p(z) \cdot |det(\frac{\partial z}{\partial x})| \\
&amp; = p(f_{\theta}^{-1}(x)) \cdot |det(\frac{\partial f_{\theta}^{-1}(x)}{\partial x})| \\
&amp; = p(f_{\theta}^{-1}(x)) \cdot |det(J_{f_{\theta}^{-1}}(x))|.
\end{align*}
\]</span></p>
<section id="generative-process-as-finate-composition-of-transformations" class="level2">
<h2 class="anchored" data-anchor-id="generative-process-as-finate-composition-of-transformations">Generative Process as Finate Composition of Transformations</h2>
<p>In the general case, the transformations <span class="math inline">\(f_{\theta}(\cdot)\)</span> and <span class="math inline">\(f_{\theta}^{-1}(\cdot)\)</span> are defined as finite compositions of simpler transformations <span class="math inline">\(f_{\theta_i}\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
x &amp; = z_{K} = f_{\theta}(z_0) = f_{\theta_K} \dots f_{\theta_2} \circ f_{\theta_1}(z_0) &amp; \\
p(x) &amp; = p_K(z_{k}) = p_{K-1}(f_{\theta_K}^{-1}(z_{k})) \cdot \Big| det\Big(J_{f_{\theta_K}^{-1}}(z_{k})\Big)\Big| &amp; \\
&amp; = p_{K-1}(z_{K-1}) \cdot \Big| det\Big(J_{f_{\theta_K}^{-1}}(z_k)\Big)\Big| &amp; \text{Due to the definition of } f_{\theta_K}(z_{K-1}) = z_K\\
&amp; = p_{K-1}(z_{K-1}) \cdot \Big| det\Big( J_{f_{\theta_K}(z_{K-1})} \Big)^{-1}\Big| &amp; \text{As: } J_{f_{\theta_K}^{-1}}(x) = \frac{f_{\theta_K}^{-1}(z_k)}{\partial z_K} \\
&amp; &amp; = \Big(\frac{\partial z_K}{\partial f_{\theta_k}^{-1}(z_k)} \Big)^{-1} \\
&amp; &amp; = \Big(\frac{\partial f_{\theta_{k}}(z_{K-1})}{\partial z_{K-1}}\Big)^{-1} \\
&amp; &amp; = \Big( J_{f_{\theta_K}(z_{K-1})} \Big)^{-1} \\
&amp; = p_{K-1}(z_{K-1}) \cdot \Big| det\Big( J_{f_{\theta_K}}(z_{K-1}) \Big)\Big|^{-1}. \\
\end{align*}
\]</span></p>
<p>By this process, <span class="math inline">\(p(z_i)\)</span> is fully described by <span class="math inline">\(z_{i-1}\)</span> and <span class="math inline">\(f_{\theta_i}\)</span>, allowing the extension of the previous reasoning to all i-steps of the overall generative process:</p>
<p><span id="eq-flow-generator"><span class="math display">\[
\begin{equation}
p(x) = p(z_0) \cdot \prod_{i=1}^k \Big| det \big( J_{f_{\theta_i}}(z_{i-1}) \big) \Big|^{-1}.
\end{equation}
\tag{1}\]</span></span></p>
<p>It is noteworthy that in the context of generative models, <span class="math inline">\(f_{\theta}\)</span> is also referred to as a pushforward mapping from a simple density <span class="math inline">\(p(z)\)</span> to a more complex <span class="math inline">\(p(x)\)</span>. On the other hand, the inverse transformation <span class="math inline">\(f_{\theta}^{-1}\)</span> is known as the normalization function, as it systematically “normalizes” a complex distribution into a simpler one, one step at a time.</p>
</section>
<section id="training-procedures" class="level2">
<h2 class="anchored" data-anchor-id="training-procedures">Training Procedures</h2>
<p>As previously mentioned, NFs serve as efficient models for both sampling from and learning complex distributions. The primary applications of NFs lie in density estimation and data generation.<br>
Density estimation proves valuable for computing statistical quantities over unseen data, as demonstrated in works such as <span class="citation" data-cites="dinh2016density">[<a href="#ref-dinh2016density" role="doc-biblioref">4</a>]</span> and <span class="citation" data-cites="grathwohl2018ffjord">[<a href="#ref-grathwohl2018ffjord" role="doc-biblioref">5</a>]</span>, where NF models effectively estimate densities for tabular and image datasets. Additionally, NFs find application in anomaly detection <span class="citation" data-cites="hirschorn2023normalizing">[<a href="#ref-hirschorn2023normalizing" role="doc-biblioref">6</a>]</span>, although requiring careful tuning for out-of-distribution detection <span class="citation" data-cites="kirichenko2020normalizing">[<a href="#ref-kirichenko2020normalizing" role="doc-biblioref">7</a>]</span>.<br>
On the flip side, data generation stands out as the central application for NFs. As mentioned earlier, NFs, under mild assumptions, can sample new data points from a complex distribution <span class="math inline">\(p(x)\)</span>. Exemplifying this, <span class="citation" data-cites="kingma2018glow">[<a href="#ref-kingma2018glow" role="doc-biblioref">8</a>]</span> showcases NFs applied to image generation, while <span class="citation" data-cites="oord2016wavenet">[<a href="#ref-oord2016wavenet" role="doc-biblioref">9</a>]</span> and <span class="citation" data-cites="kim2018flowavenet">[<a href="#ref-kim2018flowavenet" role="doc-biblioref">10</a>]</span> demonstrate successful learning of audio signals through NFs.</p>
<p>A key advantage of NFs over other probabilistic generative models lies in their ease of training, achieved by minimizing a divergence metric between <span class="math inline">\(p(x; \theta)\)</span> and the target distribution <span class="math inline">\(p(x)\)</span>. In most cases, NFs are trained by minimizing the Kullback-Leibler (KL) divergence between these two distributions:</p>
<p><span class="math display">\[
\begin{align*}
    \mathcal{L}(\theta) &amp; = D_{KL}[p(x) || p(x; \theta)] \\
        &amp; = - \sum_{x \sim p(x)} p(x) \cdot \log \frac{p(x; \theta)}{p(x)} \\
        &amp; = - \sum_{x \sim p(x)} p(x) \cdot \log p(x; \theta) + \sum_{x \sim p(x)} p(x) \cdot \log p(x) \\
        &amp; = - \mathbb{E}_{x \sim p(x)} \Big[ \log p(x; \theta) \Big] + \mathbb{E}_{x \sim p(x)} \Big[ \log p(x) \Big] \\
        &amp; = - \mathbb{E}_{x \sim p(x)}\Big[\log p(x; \theta)\Big] + const. ~~~ \text{As it does not depend on $\theta$} \\
        &amp; = - \mathbb{E}_{x \sim p(x)}\Big[\log p\big(f_{\theta}^{-1}(x)\big) + \sum_{i=1}^{K} \log \Big| det\big( J_{f_{\theta_i}^{-1}}(z_{i}) \big)\Big| \Big] + const.
\end{align*}
\]</span></p>
<p>Here, <span class="math inline">\(p(f_{\theta}^{-1}(x)) = p(z_0)\)</span>, and <span class="math inline">\(z_K\)</span> is equal to <span class="math inline">\(x\)</span>. For a fixed training set <span class="math inline">\(X_N = \\{ x_n \\}_{n=1}^N\)</span>, the loss function is derived as the negative log-likelihood typically optimized using stochastic gradient descent:</p>
<p><span id="eq-flow-loss"><span class="math display">\[
\begin{equation}
\mathcal{L}(\theta) = - \frac{1}{N} \sum_{n=1}^N \log p\big(f_{\theta}^{-1}(x)\big) + \sum_{i=1}^{K} \log \Big| det\big( J_{f_{\theta_i}^{-1}}(z_{i}) \big)\Big|.
\end{equation}
\tag{2}\]</span></span></p>
<p>It is important to note that the loss function (<a href="#eq-flow-loss" class="quarto-xref">Equation&nbsp;2</a>) is computed by starting from a datapoint <span class="math inline">\(x\)</span> and reversing it to a plausible latent variable <span class="math inline">\(z_0\)</span>. Consequently, the structural formulation of <span class="math inline">\(p(z_0)\)</span> plays a critical role in defining the training signals: if <span class="math inline">\(p(z_0)\)</span> is too lax, the training process lacks substantial information; if it is too stringent, the training process may become overly challenging. Furthermore, the training process is the inverse of the generative process defined in <a href="#eq-flow-generator" class="quarto-xref">Equation&nbsp;1</a>, emphasizing the importance of the sum of determinants. Achieving computationally efficient training requires the efficient computation of determinants of <span class="math inline">\(J_{f_{\theta_i}^{-1}}\)</span>. While auto-diff libraries can compute gradients with respect to <span class="math inline">\(\theta_i\)</span> of the Jacobian matrix and its determinant, such computations are computationally expensive (<span class="math inline">\(O(n)^3\)</span>). Therefore, significant research efforts have focused on designing transformations with efficient Jacobian determinant formulations.</p>
<section id="training-example" class="level3">
<h3 class="anchored" data-anchor-id="training-example">Training Example</h3>
<p>As previously mentioned, the training process of a NF involves mapping a given input data <span class="math inline">\(x\)</span> to a specific base distribution <span class="math inline">\(p(z_0)\)</span>. Typically, the base distribution is a well-known distribution such as a multivariate Gaussian, Uniform, or any other exponential distribution. Similarly, the mapping function is usually implemented as a neural network.</p>
<p>Starting from first principles, any NF model can be specified as comprising a base distribution and a series of flows that map <span class="math inline">\(x\)</span> to <span class="math inline">\(z_0\)</span>. Here is a Python implementation:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NormalizingFlow(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        prior: Distribution,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        flows: nn.ModuleList,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior <span class="op">=</span> prior</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flows <span class="op">=</span> flows</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        bs <span class="op">=</span> x.size(<span class="dv">0</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        zs <span class="op">=</span> [x]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        sum_log_det <span class="op">=</span> torch.zeros(bs).to(x.device)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> flow <span class="kw">in</span> <span class="va">self</span>.flows:</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> flow(x)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            log_det <span class="op">=</span> flow.log_abs_det_jacobian(x, z)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>            zs.append(z)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            sum_log_det <span class="op">+=</span> log_det</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> z</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        prior_logprob <span class="op">=</span> <span class="va">self</span>.prior.log_prob(z).view(bs, <span class="op">-</span><span class="dv">1</span>).<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        log_prob <span class="op">=</span> prior_logprob <span class="op">+</span> sum_log_det</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        intermediat_results <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            prior_logprob<span class="op">=</span>prior_logprob,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            sum_log_det<span class="op">=</span>sum_log_det,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            zs<span class="op">=</span>zs,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> log_prob, intermediat_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>where the prior can be any base distribution inplemented in <a href="https://pytorch.org/docs/stable/distributions.html">torch.distributions</a>, and flows can be any module that statisfy the NF’s properties.</p>
<p>Suppose we are given a 1D dataset, as shown in Fig. <a href="#fig:1d_dataset">[2.a]</a>. We can fit an NF to the underlying probability distribution <span class="math inline">\(p(x)\)</span> of the dataset. To successfully learn the density of the dataset, we need a base distribution (let’s say a Beta distribution parameterized by <span class="math inline">\(\alpha = 2\)</span> and <span class="math inline">\(\beta = 5\)</span>) and a functional definition for our flow. In this case, let’s use the cumulative distribution function of a Gaussian Mixture Model (GMM) with 4 different components:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NormalizingFlow(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    prior<span class="op">=</span>Beta(<span class="fl">2.0</span>, <span class="fl">5.0</span>),</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    flows<span class="op">=</span>nn.ModuleList([</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        GMMFlow(n_components<span class="op">=</span><span class="dv">4</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">5e-3</span>, weight_decay<span class="op">=</span><span class="fl">1e-5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With these ingredients, we can train the model by minimizing the negative log-likelihood using stochastic gradient descent (SGD):</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        log_prob, _ <span class="op">=</span> model(x)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="op">-</span>log_prob.mean()  <span class="co"># nll</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        nn.utils.clip_grad_norm_(model.parameters(), <span class="dv">1</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that no labels are used for training, as the objective is to directly maximize the predicted density of the dataset.</p>
<p><a href="#fig-1d-dataset" class="quarto-xref">Figure&nbsp;2</a> shows the learned density and how the 4 different components are used to correctly model <span class="math inline">\(p(x)\)</span>. While the same results might be achieved by using only 2 components, in the general case, the minimum number of needed components is not known a priori; thus using a larger number of components is a good practice.</p>
<p>Finally, <a href="#fig-1d-learned-transform" class="quarto-xref">Figure&nbsp;4</a> demonstrates how the learned model is able to map a dataset coming from an unknown density to the Beta distributin over-defined.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-1d-dataset" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1d-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/1d/dataset.png" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1d-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Training dataset build by sampling 750 elements from two distinct gaussian distributions
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-1d-fit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1d-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/1d/fit-model.png" class="img-fluid figure-img" width="420">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1d-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: A normalizing flow fitted to the given dataset to learn p(x). The normalizing flow is composed by a beta distributon as a prior and as a gaussian mixture model with 4 different component as a flow.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="fig-1d-learned-transform" class="quarto-float quarto-figure quarto-figure-center anchored" width="600px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1d-learned-transform-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/1d/learned-transformation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1d-learned-transform-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Learned normalizing flow from the unknown distribution <span class="math inline">\(p(x)\)</span> to the choosen prior <span class="math inline">\(p(z)\)</span>.
</figcaption>
</figure>
</div>
<p>Full code is contained in the following <a href="https://github.com/andompesta/pytorch-normalizing-flows/blob/main/nf_demo.ipynb">notebook</a>.</p>
</section>
<section id="d-training-example" class="level3">
<h3 class="anchored" data-anchor-id="d-training-example">2D Training Example</h3>
<p>Consider a more intricate dataset, such as the famous 2 Moon dataset depicted in Fig. <a href="#fig:2d_dataset">[3.a]</a>. The objective here is to map samples from this dataset into a latent variable that conforms to a Gaussian distribution.</p>
<p>In this context, relying solely on the cumulative distribution function of a Gaussian Mixture model as NF formulation may not provide the necessary expressiveness. While Neural Networks serve as powerful function approximators, they do not inherently guarantee the conditions required by a normalizing flow. Furthermore, computing the determinant of a linear layer within a neural network is computationally expensive.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-2d-moon" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-width="300px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2d-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/2d/dataset.png" class="img-fluid figure-img" data-fig-width="300px">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2d-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 2D Moon dataset.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-2d-moon-flow" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-width="500px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2d-moon-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/2d/2d_moon_flow.gif" class="img-fluid figure-img" data-fig-width="500px">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2d-moon-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Gif of all the steps needed by the normalization flow to map the 2 Moon dataset into a Gaussian distribution.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>In recent years, <strong>Coupling layers</strong> <span class="citation" data-cites="dinh2016density">[<a href="#ref-dinh2016density" role="doc-biblioref">4</a>]</span> have emerged as effective solutions for Normalizing Flows. They prove efficient both during sampling and training, while delivering competitive performances. The fundamental idea involves splitting the input variables of the i-th layer into equally sized groups:</p>
<ul>
<li>The first group of input variables (<span class="math inline">\(z_i[0], ..., z_i[d]\)</span>) is considered constant during the i-th layer<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</li>
<li>The second group of parameters (<span class="math inline">\(z_{i}[d+1], ..., z_{i}[D]\)</span>) undergoes transformation by a Neural Network that depends solely on <span class="math inline">\(z_{i}[\leq d]\)</span>.</li>
</ul>
<p>Mathematically, we can represent the transformation applied to all input variables in the i-th layer as:</p>
<p><span class="math display">\[
\begin{align*}
    z_{i+1}[0], ..., z_{i+1}[d] &amp; = z_{i}[0], ..., z_{i}[d] \\
    d_{i}[d+1], ..., d_{i}[D], t_{i}[d+1], ..., t_{i}[D] &amp; = f(z_{i}[0], ..., z_{i}[d]; \theta_{i}) \\
    z_{i+1}[d+1], ..., z_{i+1}[D] &amp; = (z_{i}[d+1] \cdot d_{i}[d+1]) + t_{i}[d+1], ..., (z_{i}[D] \cdot d_{i}[D]) + t_{i}[D]
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(f(\cdot; \theta_i)\)</span> is any neural network. Intuitively, a coupling layer is akin to an autoregressive layer, where the autoregressive mask only permits <span class="math inline">\(z_{i+1}[&gt;d]\)</span> to depend on <span class="math inline">\(z_{i}[\leq d]\)</span>.<br>
As shown in <a href="#fig-coupling-forward" class="quarto-xref">Figure&nbsp;7</a> and <a href="#fig-coupling-backward" class="quarto-xref">Figure&nbsp;8</a>, the beauty of coupling layers lies in the ease of inverting their transformation. Given the initial conditiokn <span class="math inline">\(z_{i+1}[\leq d] = z_{i}[\leq d]\)</span>, it is possible to derive the affine parameters <span class="math inline">\(d_{i}[&gt; d]\)</span> and <span class="math inline">\(t_{i}[&gt; d]\)</span> by directly applying <span class="math inline">\(f(\cdot; \theta_i)\)</span> to <span class="math inline">\(z_{i+1}[\leq d]\)</span>.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-coupling-forward" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coupling-forward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/coupling_layer-forward.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coupling-forward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Coupling layer forward pass.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-coupling-backward" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coupling-backward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/coupling_layer-backward.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coupling-backward-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Coupling layer backward pass.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>By construction, the Jacobian matrix of any such layer is lower triangular, following the structure:</p>
<p><span class="math display">\[
J_{z_{i+1}}(z_{i}) = \begin{bmatrix}
    \mathbf{I} &amp; \mathbf{O} \\
    \mathbf{A} &amp; \mathbf{D} \\
\end{bmatrix}.
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{I}\)</span> is an identity matrix of size <span class="math inline">\(d \times d\)</span>, <span class="math inline">\(\mathbf{O}\)</span> is a zeros matrix of size <span class="math inline">\(d \times (D-d)\)</span>, <span class="math inline">\(\mathbf{A}\)</span> is a full matrix of size <span class="math inline">\((D-d) \times d\)</span> and <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix of shape <span class="math inline">\((D-d) \times (D-d)\)</span>. The determinant of such a matrix is formed by the product of the diagonal elements of <span class="math inline">\(\mathbf{D}\)</span>, making it efficient to compute.</p>
<p><a href="#fig-2d-normalizing-flow" class="quarto-xref">Figure&nbsp;9</a> illustrates the dynamics of a NF trained on a 2 Moon dataset. Note how the final latent space (step 4) conforms to a Gaussian distribution.</p>
<div id="fig-2d-normalizing-flow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2d-normalizing-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/2d/training-process.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2d-normalizing-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Normalizing flow from a 2 moon dataset to the guassian prior visualized step by step. Bottom right picture shows the distribution of the final latent variable extracted by the flow, demonstrating that it is clrearly Gaussian.
</figcaption>
</figure>
</div>
<p>Finally, a simple implementation of a coupling layer in pytorch is proviceded as follow:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CouplingFlow(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        dim: <span class="bu">int</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        hidden_dim: <span class="bu">int</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        mask: Tensor,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> dim <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>, <span class="st">"dim must be even"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> dim <span class="op">==</span> mask.size(<span class="op">-</span><span class="dv">1</span>), <span class="st">"mask dimension must equal dim"</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dim <span class="op">=</span> hidden_dim</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"mask"</span>, mask)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, hidden_dim),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_dim, dim),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        x_masked <span class="op">=</span> x <span class="op">*</span> <span class="va">self</span>.mask</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.net(x_masked)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        log_d, t <span class="op">=</span> output.chunk(<span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> x_masked <span class="op">+</span> ((<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.mask) <span class="op">*</span> (x <span class="op">*</span> torch.exp(log_d) <span class="op">+</span> t))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> log_abs_det_jacobian(</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        x: Tensor,</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        z: Tensor,</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        x_masked <span class="op">=</span> x <span class="op">*</span> <span class="va">self</span>.mask</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        log_d, t <span class="op">=</span> <span class="va">self</span>.net(x_masked).chunk(<span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> log_d.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="credits" class="level1">
<h1>Credits</h1>
<p>The content of this post is based on the lectures and code of <a href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Pieter Abbeel</a>, <a href="https://groups.csail.mit.edu/gdpgroup/6838_spring_2021.html">Justin Solomon</a> and <a href="https://github.com/karpathy/pytorch-normalizing-flows">Karpathy’s</a> tutorial. Moreover, I want to credit <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lil’Long</a> and <a href="https://blog.evjang.com/2018/01/nf1.html">Eric Jang</a> for their amazing tutorials. For example, the pioneering work done by <a href="#ref:nice">Dinh et. al.</a> is the first to leverage transformations with triangular matrix for efficent determinatnt computation.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-normalization-flow-review" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Papamakarios G, Nalisnick E, Rezende DJ, Mohamed S, Lakshminarayanan B (2021) Normalizing flows for probabilistic modeling and inference. The Journal of Machine Learning Research 22(1):2617–2680</div>
</div>
<div id="ref-dinh2014nice" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Dinh L, Krueger D, Bengio Y (2014) Nice: Non-linear independent components estimation. arXiv preprint arXiv:14108516</div>
</div>
<div id="ref-change-of-variable" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><span>C</span>hange of <span>V</span>ariables <span>T</span>heorem – from <span>W</span>olfram <span>M</span>ath<span>W</span>orld — mathworld.wolfram.com</div>
</div>
<div id="ref-dinh2016density" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Dinh L, Sohl-Dickstein J, Bengio S (2016) Density estimation using real nvp. arXiv preprint arXiv:160508803</div>
</div>
<div id="ref-grathwohl2018ffjord" class="csl-entry" role="listitem">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Grathwohl W, Chen RT, Bettencourt J, Sutskever I, Duvenaud D (2018) Ffjord: Free-form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:181001367</div>
</div>
<div id="ref-hirschorn2023normalizing" class="csl-entry" role="listitem">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Hirschorn O, Avidan S (2023) Normalizing flows for human pose anomaly detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp 13545–13554</div>
</div>
<div id="ref-kirichenko2020normalizing" class="csl-entry" role="listitem">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Kirichenko P, Izmailov P, Wilson AG (2020) Why normalizing flows fail to detect out-of-distribution data. Advances in neural information processing systems 33:20578–20589</div>
</div>
<div id="ref-kingma2018glow" class="csl-entry" role="listitem">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Kingma DP, Dhariwal P (2018) Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems 31</div>
</div>
<div id="ref-oord2016wavenet" class="csl-entry" role="listitem">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Oord A van den, Dieleman S, Zen H, et al (2016) Wavenet: A generative model for raw audio. arXiv preprint arXiv:160903499</div>
</div>
<div id="ref-kim2018flowavenet" class="csl-entry" role="listitem">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Kim S, Lee S, Song J, Kim J, Yoon S (2018) FloWaveNet: A generative flow for raw audio. arXiv preprint arXiv:181102155</div>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Here we introduce the notation <span class="math inline">\(z_{i}[d]\)</span> as indicating the <span class="math inline">\(d\)</span> dimention of the latent variable at the i-th layer of a flow (<span class="math inline">\(z_{i}\)</span>).<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/andompesta\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "quarto-dev/quarto-web";
    script.dataset.repoId = "MDEwOlJlcG9zaXRvcnkzNDc2MzMzNTg=";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOFLh2zs4CBQCO";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->




</body></html>