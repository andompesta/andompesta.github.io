---
categories:
- Deep Learning

author: Sandro Cavallari
date: 2024-04-10
title: "Neural ODEs"
comments:
  giscus:
    repo: quarto-dev/quarto-web
format:
  html:
    toc: true
---


# Ordinalry Differential Equations

Ordinalry Differential Equations or ODEs are equations with a single indipendent varlaible (usually called time $t$) and one or more derivatives of functions defined in terms of the indipendent variable. Formally,

$$
f(x(t), t) = x'(t) = \frac{ \partial x(t)} {\partial t} = \lim_{\Delta t \rightarrow 0} \frac{x(t + \Delta t) - x(t)}{\Delta t}
$$

where:

  - $t$ represent the time or any other indipendent variable used as domain of the derivate operator;
  - $x(t) \in \mathbb{R}^d$ is the dependent variable defining the system's state;
  - $x'(t) \in \mathbb{R}^d$ is the first order derivative of $x(t)$;
  - $f \in \mathbb{R}^d \times \mathbb{R}^+$ is the **vector-field** differential functioin describing the systems's evolution over time.


As $f$ defines the evolution of a complex system over every infinitesimal interval of time $\Delta t$, we can formally define an ODE problem as:

$$
\begin{align}
    x(t + \Delta t) & = x(t) + \Delta t \cdot f(x(t), t) \\
    \text{s.t.} & ~~ \Delta t \rightarrow 0.
\end{align}
$$


## Computing a Solution

In the general term, the solution to an ODE involve to computetion of the antiderivative of $f$, in other words the integral of $f$.
As the integral of any function involve an arbitrary constant, usually defined as $C$, there is the need to specify an *initial condition* $x_0$ to guarantee that the solution of the ODE is unique:

$$
x_t = x_0 + \int_0^t f(x_\tau) \partial \tau
$$

Note that, given the inital state $x_0$ and a set of points in time $\{ t_0, ..., t_N\}$, the objective is to obtain the state solution $x_{0:N} \equiv \{ x_0, ..., x_N\}$.
Unfortunetly, solving the above integral analytically is possible only for a limited amount of differential functions.
Therefore, numerical solvers are used in practice. 



```{python}
import torch
import torch.nn as nn
from torchdyn.numerics import odeint
from IPython.display import HTML

device = torch.device("cpu")


class VDPoscillator(nn.Module):
    def __init__(self, mu: float) -> None:
        super().__init__()
        self.mu = mu

    def forward(
        self,
        t: torch.Tensor,
        x: torch.Tensor,
    ):
        var_0 = x[..., 1]
        var_1 = self.mu * (1 - x[..., 0] ** 2) * x[..., 1] - x[..., 0]
        return torch.stack(
            (var_0, var_1),
            dim=-1,
        )


from typing import Callable
import numpy as np
import torch

vdp = VDPoscillator(1.0).to(device)

# initial value, of shape [N, d]
x0 = torch.tensor([[1.0, 0.0]]).float().to(device)

# integration time points, of shape [T]
t_span = torch.linspace(0.0, 15.0, 500).to(device)

with torch.no_grad():
    t_eval, vdp_sol_euler = odeint(
        vdp, x0, t_span=t_span, solver="euler"
    )  # [T], [T, N, d]

t_eval.size(), vdp_sol_euler.size()
```

{{< video animations//vdp-oscillator_.mp4 >}}


As overmentioned the solution depend on the initial state of the system. Following there is a demonstration of how the system will behave if you used different starting points.

```{python}
x0 = (
    torch.tensor(
        [
            [1.0, 0.0],
            [-2.0, -3.0],
            [-2.0, 3.0],
        ]
    )
    .float()
    .to(device)
)

# integration time points, of shape [T]
t_span = torch.linspace(0.0, 15.0, 500).to(device)

# forward integration
with torch.no_grad():
    t_eval, vdps_sol_euler = odeint(vdp, x0, t_span=t_span, solver="euler")

t_eval.size(), vdp_sol_euler.size()
```

::: {#fig-vpds-solution}

![](img/vdps.png){#fig-vpds-solution}

Representation of 3 different solutions for a vdp oscillator system.
:::


# Neural ODEs 

Neural ODEs are a family of ODEs for which the vector-field $f(x_t, t)$ is defined by a neural network. As such, $f(x_t, t)$ is both differentiable and learnable. Thus, given a set of observation $y_{0:N}$ from a unknwon dynamical system, we can used it to learn a model of the evolution of the system's dynamics.

## Problem Formulation

Given a dataset containing noise observation $y_n$ where each observation is the perturbation of an unknown state $x_n$ generated by an unknown underling dynamics $f_{true}$:

$$
\begin{align}
    y_{n} & = x_{n} + \epsilon, ~~ \epsilon \sim \mathcal{N}(0, \sigma^2) \quad \substack{y_n \text{ is a noise variable}} \\
    x_{n} & = x_{0} + \int_{0}^{n} f_{true}(x_{\tau}) \partial \tau
\end{align}
$$


The objective is to learn a neural network $f_\theta$ that matches the unknown dynamics:

$$
f_\theta \approx f_{true}.
$$

```{python}
from torch import nn, Tensor
from torchdyn.core import NeuralODE


class NODE(nn.Module):
    def __init__(self, d: int):
        """d - ODE dimensionality"""
        super().__init__()
        self.d = d
        self._f = nn.Sequential(
            nn.Linear(d, 200),
            nn.ReLU(),
            nn.Linear(200, 200),
            nn.ReLU(),
            nn.Linear(200, d),
        )
        self.reset_parameter()

    def reset_parameter(self):
        for name, param in self.named_parameters():
            if "bias" in name:
                nn.init.constant_(param, 0.0)

            else:
                nn.init.xavier_uniform_(param, gain=nn.init.calculate_gain("relu"))

    def ode_rhs(self, t, x):
        """differential function = f(x)"""
        return self._f(x)

    def forward(
        self,
        t: Tensor,
        x: Tensor,
        **kwargs
    ):
        """Forward integrates the NODE system and returns state solutions
        Input
            ts - [T]   time points
            x0 - [N,d] initial value
        Returns
            X  - [T,N,d] forward simulated states
        """
        return self.ode_rhs(t, x)
```

# Resources

- Original paper [Neural Oridnaly Differential Equations](https://papers.nips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf)
- Probabilistic AI tutorial: [video](https://www.youtube.com/watch?v=oh2X89rmMdQ&list=PLRy-VW__9hV8zhnmhzoz3yUzGE1NAA7ka&index=18&ab_channel=ProbabilisticAI) [website](https://cagatayyildiz.github.io/notes/node/NODE.html#References)
- Deep Implicit Layer: [video](https://implicit-layers-tutorial.org/) [website](https://implicit-layers-tutorial.org/)
- [Adjoint State Method](https://ilya.schurov.com/post/adjoint-method/) blog by Ilya Schurov
- [ODE demo](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Dynamical_systems/dynamical_systems_neural_odes.html#0.-Introduction)
- [Adjoint state Method from Lagrange multipliers](https://vaipatel.com/deriving-the-adjoint-equation-for-neural-odes-using-lagrange-multipliers/#:~:text=Luckily%2C%20a%20very%20well%2Dknown,to%20store%20intermediate%20function%20evaluations.)