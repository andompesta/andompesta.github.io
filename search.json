[
  {
    "objectID": "posts/linear_algebra/index.html",
    "href": "posts/linear_algebra/index.html",
    "title": "Basic Principles of Linear Algebra",
    "section": "",
    "text": "Linear algebra is the branch of math and statistics that is devoted to the study of matrices and vectors. As such, it is broadly used to model real-world problems in phisitcs and machine learning. Such post is a collections of my notes obtained from the 3Blue1Brown series on linear-algebra [1] and Murphy’s new book [2].\n\\[\n\\newcommand{\\rvepsilon}{\\mathbf{\\epsilon}}\n\\newcommand{\\rvtheta}{\\mathbf{\\theta}}\n\\newcommand{\\rva}{\\mathbf{a}}\n\\newcommand{\\rvb}{\\mathbf{b}}\n\\newcommand{\\rvc}{\\mathbf{c}}\n\\newcommand{\\rve}{\\mathbf{e}}\n\\newcommand{\\rvi}{\\mathbf{i}}\n\\newcommand{\\rvj}{\\mathbf{j}}\n\\newcommand{\\rvu}{\\mathbf{u}}\n\\newcommand{\\rvv}{\\mathbf{v}}\n\\newcommand{\\rvx}{\\mathbf{x}}\n\\newcommand{\\rmA}{\\mathbf{A}}\n\\newcommand{\\rmB}{\\mathbf{B}}\n\\newcommand{\\rmC}{\\mathbf{C}}\n\\newcommand{\\rmH}{\\mathbf{H}}\n\\newcommand{\\rmI}{\\mathbf{I}}\n\\newcommand{\\rmM}{\\mathbf{M}}\n\\newcommand{\\rmS}{\\mathbf{S}}\n\\newcommand{\\rmU}{\\mathbf{U}}\n\\newcommand{\\rmV}{\\mathbf{V}}\n\\newcommand{\\rmX}{\\mathbf{X}}\n\\newcommand{\\rmY}{\\mathbf{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\]"
  },
  {
    "objectID": "posts/linear_algebra/index.html#eigendecomposition",
    "href": "posts/linear_algebra/index.html#eigendecomposition",
    "title": "Basic Principles of Linear Algebra",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\nGiven a squared matrix \\(\\rmA \\in \\real^{n \\times n}\\), it is possible to rewrite Equation 4 in matrix form as:\n\\[\n\\begin{equation}\n\\rmA \\rmU = \\rmU \\mathbf{\\Lambda}.\n\\end{equation}\n\\tag{5}\\]\nMoreover, according to Equation 3, using the eigenvectors of \\(\\rmA\\) as new basis of \\(\\rmA\\) will generate a diagonal matrix of eigenvalues:\n\\[\n\\begin{equation}\n\\rmU^{-1} \\rmA \\rmU = \\mathbf{\\Lambda}\n\\end{equation}\n\\]\nwhere \\[\n\\rmU \\in \\real^{n \\times n} = \\left[\\begin{array}{ccc}\n  | & | & | \\\\\n  \\rve_1 & \\dots & \\rve_{n}\\\\\n  | & | & | \\\\\n\\end{array}\\right]\\] is the matrix formed by the eigenvectors of \\(\\rmA\\) and\n\\[\\mathbf{\\Lambda} \\in \\real^{n \\times n} = \\left[\\begin{array}{ccc}\n  \\lambda_1 &  &  \\\\\n   & \\ddots & \\\\\n   & & \\lambda_n \\\\\n\\end{array}\\right]\\] is the diagonal matrix formed by the eigenvalues assogiated to the eigenvectors of \\(\\rmA\\).\nThis process of expressing \\(\\rmA\\) in terms of its eigenvalue and eigenvectors is know as diagonalization. If the eigenvalues of \\(\\rmA\\) are linearly indipendent, then the matrix \\(\\rmU\\) is invertible, thus, it is possible to decompose \\(\\rmA\\) as:\n\\[\n\\begin{equation}\n\\rmA = \\rmU \\mathbf{\\Lambda} \\rmU^{-1} .\n\\end{equation}\n\\tag{6}\\]\nMoreover, if \\(\\rmA\\) is real valued and symmetric then it can be shown that \\(\\rmU\\) is orthonormal, i.e., \\(\\rvu^T_i \\rvu_j = 0\\) if \\(i \\neq j\\) and \\(\\rvu^T_i \\rvu_i = 1\\) (or \\(\\rmU^T\\rmU = \\rmU \\rmU^T = \\rmI\\)). Thus, we can futher symplify Equation 6 as:\n\\[\n\\begin{equation}\n\\rmA = \\rmU \\mathbf{\\Lambda} \\rmU^T.\n\\end{equation}\n\\]\nAs a final note, it is possible to leverage such eigendecomposition to easily compute the inverse of a matrix \\(\\rmA\\). Since \\(\\rmU^T = \\rmU^{-1}\\), we have:\n\\[\n\\begin{equation}\n\\rmA^{-1} = \\rmU \\mathbf{\\Lambda}^{-1} \\rmU^T .\n\\end{equation}\n\\]\n\nLagrangian Methods for Constrained Optimization\nWhile eigen decomposition is commonly applied to solve systems of liear equations. It is also a powerful method for optimization subject to linear constrains (constrained optimization). That is, it can be used to solve quadratic constrained problems of the form:\n\\[\n\\min_{\\rvx} \\rvx^T \\rmH \\rvx + d, ~~\\text{subject to} ~~ \\rvx^T \\rvx - 1 = 0\n\\]\nwhere \\(\\rmH \\in \\real^{n \\times n}\\) is symmetric. Such problems are a specific instanche of the Lagrangian method, in which an augmented objective is created to ensure the constrain satisfability:\n\\[\nL(\\rvx, \\lambda) = \\max_{\\lambda} \\min_{\\rvx} \\rvx^T \\rmH \\rvx + d - \\lambda (\\rvx^T \\rvx - 1).\n\\]\nThe optimal \\(\\rvx^*\\) that solve the problem, need to satisfy the zero-gradient condition:\n\\[\n\\begin{align*}\n\\frac{\\partial L(\\rvx, \\lambda)} {\\partial \\rvx} & = 0 \\\\\n& = \\frac{ \\partial } {\\partial \\rvx} \\rvx^T \\rmH \\rvx   +  \\frac{\\partial}{\\partial \\rvx} d - \\frac{\\partial}{\\partial \\rvx} \\lambda (\\rvx^T \\rvx - 1)  \\\\\n& = \\rvx^T (\\rmH + \\rmH^T) + 0 - 2 \\lambda \\rvx^T  && { \\small \\rmH = \\rmH^T \\text{ since is symmetric.} }\\\\\n& = 2 \\rvx^T \\rmH - 2 \\lambda \\rvx^T \\\\\n\\frac{\\partial L(\\rvx, \\lambda)} {\\partial \\lambda} & = 0  \\\\\n& =  \\frac{ \\partial }{ \\partial \\lambda } \\rvx^T \\rmH \\rvx + \\frac{ \\partial }{ \\partial \\lambda } d - \\frac{ \\partial }{ \\partial \\lambda } \\lambda (\\rvx^T \\rvx - 1) \\\\\n& = 0 + 0 - \\rvx^T \\rvx + 1 \\\\\n& = \\rvx^T \\rvx - 1\n\\end{align*}\n\\]\nwhich is equivalent to the eigenvector equation Equation 5 \\(\\rmH \\rvx = \\lambda \\rvx\\)."
  },
  {
    "objectID": "posts/linear_algebra/index.html#singular-value-decomposition-svd",
    "href": "posts/linear_algebra/index.html#singular-value-decomposition-svd",
    "title": "Basic Principles of Linear Algebra",
    "section": "Singular Value Decomposition (SVD)",
    "text": "Singular Value Decomposition (SVD)\nWhile eigendecomposition require squared matrices, SVD allow the factorization of rectangular matrices into singular vectors and singular values. Given any \\(\\rmA \\in \\real^{m \\times n}\\), it is possible to depompose it as:\n\\[\n\\begin{equation}\n\\rmA = \\rmU \\rmS \\rmV^T\n\\end{equation}\n\\]\nwhere \\(\\rmU \\in \\real^{m \\times m}\\) is composed by orthonormal columns (\\(\\rmU^T \\rmU = \\rmI\\)), \\(\\rmV \\in \\real^{n \\times n}\\) is compesed by orthonormals rows and columns (\\(\\rmV^T\\rmV = \\rmV \\rmV^T = \\rmI\\)), and \\(\\rmS \\in \\real^{m \\times n}\\) is a diagonal matrix containing the singular values \\(\\sigma_i \\geq 0\\). \\(\\rmU\\) and \\(\\rmV^T\\) are respectively known as the left singular vectors and right singular vectors of \\(\\rmA\\) and are obtained as the eigenvectors of \\(\\rmA\\rmA^T\\) and \\(\\rmA^T\\rmA\\). Similarly, \\(\\rmS\\) is composed by the squared root of the eigenvalues of \\(\\rmA\\rmA^T\\) and \\(\\rmA^T\\rmA\\) arranged in descending order.\nFor example, consider\n\\[\n\\rmA =\n\\left[\\begin{array}{cc}\n  2 & 4 \\\\\n  1 & 3 \\\\\n  0 & 0 \\\\\n  0 & 0 \\\\\n\\end{array}\\right]\n\\]\nthen we know that the columns of \\(\\rmU\\) are made by the eigenvalues of \\(\\rmA \\rmA^T\\):\n\\[\n\\begin{align*}\n\\rmA \\rmA^T &= \\left[\\begin{array}{cccc}\n  20 & 14 & 0 & 0 \\\\\n  14 & 10 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 \\\\\n\\end{array}\\right]\\\\\n\\rmU &= \\left[\\begin{array}{cccc}\n  0.82 & -0.58 & 0 & 0 \\\\\n  0.58 & 0.82 & 0 & 0 \\\\\n  0 & 0 & 1 & 0 \\\\\n  0 & 0 & 0 & 1 \\\\\n\\end{array}\\right]\n\\end{align*}\n\\]\nsimilarly, the right singular vectors are obtained as eigenvalues of \\(\\rmA^T \\rmA\\):\n\\[\n\\begin{align*}\n\\rmA^T \\rmA &= \\left[\\begin{array}{cc}\n  5 & 11 \\\\\n  11 & 25\\\\\n\\end{array}\\right]\\\\\n\\rmV &= \\left[\\begin{array}{cc}\n  0.4 & -0.91 \\\\\n  0.91 & 0.4\n\\end{array}\\right]\n\\end{align*}\n\\]\ninstead, \\(\\rmS\\) is formed by the squared root of the eivenvectors of \\(\\rmV\\) or \\(\\rmU\\):\n\\[\n\\rmS = \\left[\\begin{array}{cc}\n  5.46 & 0 \\\\\n  0 & 0.37 \\\\\n  0 & 0 \\\\\n  0 & 0\n\\end{array}\\right].\n\\]"
  },
  {
    "objectID": "posts/data_preprocessing/index.html",
    "href": "posts/data_preprocessing/index.html",
    "title": "Efficient and Scalable Machine Learning Pipelines",
    "section": "",
    "text": "Jobs related to machine learning usually require managing massive datasets. A well-established rule of thumb that applies to most machine learning projects is that the larger and cleaner the dataset, the better the performance. Thus, the problem of preprocessing large amounts of data and efficiently feeding the produced dataset into the training pipeline emerges. While developing fancy models is a fun task for which limitless resources are available on the web, the ML community needs to cover better the topic of streamlining data preprocessing and ingestion pipelines. Backed by the fast iteration philosophy, this document aims to find the most efficient training process to minimise the cost of experimentation as more experiment results in better performance: as Elon Musk says, “high production rate solve many ills”.\nTo be as general as possible, this article follow the work done by [1] and will focus on finetuning a CLIP-like model on the farfetch dataset. This task’s choice enables us to preprocess a large number of images as well as text, which are the most common data-type currently used in machine learning."
  },
  {
    "objectID": "posts/data_preprocessing/index.html#complex-raw-datasets",
    "href": "posts/data_preprocessing/index.html#complex-raw-datasets",
    "title": "Efficient and Scalable Machine Learning Pipelines",
    "section": "Complex Raw Datasets",
    "text": "Complex Raw Datasets\nThese types of datasets are composed of a mixture of texts, images and audio in the form of large multi-dimensional arrays. The large input space requires deep models to learn good embeddings of the data. Thus, it is assumed that the forward and backward passes of the model is the main bottelneck of the training phase due to the model’s complexity. Instead, the data loading process is assumed to be comparatively less expensive.\nBased on these assumptions, a common pattern emerged across the dataloaders solutions: spawning thread-based or process-based workers to ingest the data while the GPUs are used for training. Among the others, Petastorm, a general-purpose solution provided by Uber that easily integrates with Databricks, follows exactly this pattern. At the core of Petastorm, there is the Codecs concept, an API that specifies methods to encode and decode custom datatypes. For example, numpy arrays and images, two types not supported by Spark, are encoded by Petastorm into a Spark DataFrames as BinaryType and decoded at training time. As above mentioned, when a new column containing a non-native datatype is added to the DataFrame, the encode function is applied to every row.\ndef encode(value)\n    memfile = BytesIO()\n    np.save(memfile, value)\n    return bytearray(memfile.getvalue())\nSimilarly, once the dataset is stored on any distributed file system, the Petastorm Reader decodes each row of the dataset while feeding the data into the training pipeline.\ndef decode(value)\n    memfile = BytesIO(value)\n    return np.load(memfile)\nYet, performing random access of distributed datasets containing large arrays is costly due to the multiple I/O operations involved. Thus, an evaluation of Petastorm dataloaders, on the dataset previously prepared is reported. The dataset consists of about 300K image/text pairs. Images are represented as 3 x 224 x 224 arrays, while text by a list of 77 elements. Each batch is composed of 64 examples. The objective is to find the best worker-type and number of worker combinations possible. Thus, a grid search is reported in [fig-dataloader-cn-nlp-bnc], where thread-based and process-based workers are compared with a setting that uses 5, 10 and 20 workers.\n\n\n\n\n\n\nFigure 3: Bencharking of dataloaders for complex raw datatype. Upper figure represent the overall execution time for a single epoch, while bottom figure shows the amount of batch per seconds (BpS) processed.\n\n\n\nThe results show that a setting with 5 processes is the fastest as it can process 74 batches per second (BpS), which is a 172 % improvement over the default configuration (threaded with 10 workers). 74 batches per second might seem like a bad result, but the computational cost of a deep model would likely be an order of magnitude larger, even if working in a data-parallel setting. Thus, most of the research focuses on speeding up the model computational time with strategies like model pruning, mixed-precision, ecc."
  },
  {
    "objectID": "posts/data_preprocessing/index.html#tabular-datasets",
    "href": "posts/data_preprocessing/index.html#tabular-datasets",
    "title": "Efficient and Scalable Machine Learning Pipelines",
    "section": "Tabular Datasets",
    "text": "Tabular Datasets\nTabular datasets are commonly found in recommendation systems (RecSys) applications where the objective is to score (user, item) or (query, document) pairs. RecSys have key differences w.r.t. other deep learning applications:\n\nmany recommendation applications need to perform in a real-time environment; thus the models need to satisfy tight latency constraints;\nthe datasets are usually large since the collection of (weakly) labelled examples is inexpensive;\nthe inputs are composed of a large set of handcrafted features.\n\nIn the RecSys settings, efficient data-loading pipelines are an extremely important component of the training phase as the computational cost of the model is relatively small w.r.t. the loading operations. Note that, this is the exact opposite of the traditional deep learning environment, thus the multi-worker solution might perform poorly. To this end, custom-designed solutions for tabular datasets such as NVIDIA Merlin emerged. Merlin is a complete toolkit for hardware-accelerated RecSys systems built on top of Dask, cuDF and Rapids. Merlin dataloader is a package specifcally built for the RecSys usecase; it leverages cuDF to efficiently load data into the GPUs and DLPack to transfer the data to the appropriate backend framework (usually Tensorflow, PyTorch or JAX).\nTo evaluate the importance of having efficient data-loading solutions in this setting, a benchmark of Petastorm, Tensorflow Datasets with TFRecords and Merlin dataloader is conducted. The dataset used for the experiment is composed of ~8 M examples. Each row is composed of 100 columns containing only scalar values. A batch is 64 examples is loaded at each step.\nThe following setups are used to fine-tune each framework:\n\nPetastorm uses 5 or 10 workers in a thread or process-based solutions;\nTensorflow uses 5 or 10 workers for reading and parsing the TFRecords;\nMerlin uses the default configuration.\n\n\n\n\n\n\n\nFigure 4: Benchmarking of dataloaders for tabular datasets. The results demonstrate how the custom-design proposed by Merlin achieves 5x to 1000x better performance against other deep learning solutions.\n\n\n\nFigure 4 highlights how a general-purpose solution like Petastorm does not fit the RecSys settings as it is more than 1000x slower than Merlin. Tensorflow Datasets are showing decent performances, but handling TFRecords is challenging as they consume a large amount of disk space and need to know the dataset schema at parsing time. Without many surprises, Merlin demonstrates astonishing performances being more than 4000 batches per second and it is 10 times faster than Tensorflow while being almost a plug-and-play solution if the datasets are stored in a parquet format. Unfortunately, Merlin does not support any other datatype than numerical values; thus datasets containing strings and multi-dimensional arrays are not supported."
  },
  {
    "objectID": "posts/datastructure/index.html",
    "href": "posts/datastructure/index.html",
    "title": "Data Structures",
    "section": "",
    "text": "Data scrtucures are efficent memory construct used to sotre and organize data in an efficent manner. Adopting the right data structure and having efficent access to the needed information is a fundamentala to build usable and scalable products."
  },
  {
    "objectID": "posts/datastructure/index.html#array",
    "href": "posts/datastructure/index.html#array",
    "title": "Data Structures",
    "section": "Array",
    "text": "Array\nArrays are collections of items stored at a contiguous memory locations. Such property makes array easy to traverse and genearlly it provides random access to its element in constant complexity.\nGenearally speaking arrays have fixed size and new element can’t be added if the array is already full. However, it is possible to implement dynamic arrays at the expences of a memory overhead (unused memory is reserved for new items that will be added later on). Dynamic arrays achieve constant time complexity when it comes to append and delete operation in the general case, but if resize is needed then a new copy of the current array has to be create; thus requireing high memory and time complexity. The dynamic structure is obtained by creating a new array double size of the original array and copy all element from the previous array to the new array.\n\nArray complexities\n\n\n\n\n\n\n\nName\nDescription\nComplexity\n\n\n\n\nAppend\nAdd an element to the end of the array\nTime and Space: \\(O(1)\\) (in ammortized time)\n\n\nInsert\nInsert an element to the i-th position of the array\nTime and Space: \\(O(N)\\)\n\n\nRemove\nRemove the i-th element of the array\nTime: \\(O(N)\\) and Space: \\(O(N)\\)\n\n\nRemove Last\nRemove the last element of the array\nTime and Space: \\(O(1)\\) (in ammortized time)\n\n\nSearch\nCheck if an element is present in the list\nTime: \\(O(N)\\) and Space: \\(O(1)\\)\n\n\nGet\nGet the i-th element in the list\nTime: \\(O(1)\\) and Space: \\(O(1)\\)\n\n\nSort\nSort element in the list\nTime: \\(O(N \\log N)\\) and Space: \\(O(N)\\)"
  },
  {
    "objectID": "posts/datastructure/index.html#hash-tables",
    "href": "posts/datastructure/index.html#hash-tables",
    "title": "Data Structures",
    "section": "Hash Tables",
    "text": "Hash Tables\nHash tabels are one of the most importat data strcutre build uppon arrays. By organising data in (key, values) pairs it allows for fast insertion, lookup and access to data. It is composed by an array and the position of each key in this array is determined by the function:\n\\[\nidx = hash(key) \\% size(hash\\_table)\n\\].\nPython provide a native implementation of hash table under the dict class.\n\nHash table complexities\n\n\n\n\n\n\n\nName\nDescription\nComplexity\n\n\n\n\nInsert\nAdd an element to the dictionary\nTime and Space: \\(O(1)\\) (in ammortized time)\n\n\nRemove\nRemove a key from the dictonary\nTime: \\(O(1)\\) and Space: \\(O(1)\\)\n\n\nSearch\nCheck if a key is present in the dictionary\nTime: \\(O(1)\\) and Space: \\(O(1)\\)\n\n\nGet\nGet a given key in the dictionary\nTime: \\(O(1)\\) and Space: \\(O(1)\\)\n\n\nIterate\nIterate over all element of the dictionary\nTime: \\(O(N)\\) and Space: \\(O(1)\\)"
  },
  {
    "objectID": "posts/datastructure/index.html#linked-list",
    "href": "posts/datastructure/index.html#linked-list",
    "title": "Data Structures",
    "section": "Linked List",
    "text": "Linked List\nA linked list is a linear data structure that includes a series of connected nodes. Usually every nodes is composed by a data filed that contains some value and a pointer to the next element (if there is). While arrays are contiguous in memory, linked lists allows for a dynamic memory management where nodes can be scattered across the memory and simply point to each other. Linked lists are the fundamental backbone for other data structure as stacks and queue.\n\nLinked list complexities\n\n\n\n\n\n\n\nName\nDescription\nComplexity\n\n\n\n\nInsert\nAdd an element to the list\nTime and Space: \\(O(1)\\)\n\n\nRemove\nRemove an element from the list\nTime and Space: \\(O(1)\\)\n\n\nSearch\nSearch an element in the list\nTime: \\(O(N)\\) Space: \\(O(1)\\)\n\n\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n\nclass LinkedList:\n    # Initializing a stack.\n    # Use a dummy node, which is\n    # easier for handling edge cases.\n    def __init__(self):\n        self.head = None\n        self.tail = None\n\n    # Check if the stack is empty\n    def isEmpty(self):\n        return self.head is None\n\n    # Insert at the end\n    def insert(self, value):\n        node = Node(value)\n        if self.isEmpty():\n            self.head = node\n            self.tail = node\n        else:\n            self.tail.next = node\n            self.tail = self.tail.next\n\n    # remove from the beginning.\n    def remove(self):\n        if self.isEmpty():\n            raise Exception(\"Remove from an empty list\")\n\n        node = self.head\n        if self.head == self.tail:\n            self.tail = self.tail.next\n        self.head = self.head.next\n        return node\n\n    # sarch the node with a given value\n    def search(self, value):\n        node = self.head\n        while node is not None:\n            if node.value == value:\n                break\n            node = node.next\n        return node\n\nFloyd’s Cycle Finding Algorithm\nOne of the most famous algorithm for LinkedList is the so called Floyd’s finding algorithm. This algorithm is used to find a loop in a linked list. It uses two pointers one moving twice as fast as the other one. The faster one is called the faster pointer and the other one is called the slow pointer. While traversing the linked list one of these things will occur:\n\nthe fast pointer may reach the end (NULL) this shows that there is no loop in the linked list.\nthe fast pointer again catches the slow pointer at some time therefore a loop exists in the linked list.\n\ndef detectLoop(llist):\n    slow_pointer = llist.head\n    fast_pointer = llist.head\n\n    while (slow_pointer != None\n           and fast_pointer != None\n           and fast_pointer.next != None):\n        slow_pointer = slow_pointer.next\n        fast_pointer = fast_pointer.next.next\n        if (slow_pointer == fast_pointer):\n            return 1\n\n    return 0\n\n\nStack\nThe Stack is a special kind of linked list that follows the LIFO principle. Intuitivly it’s a deck of cards where the top card of the deck (the last added element added) is the first card to picked (the first element to remove next).\n\nStack complexities\n\n\n\n\n\n\n\nName\nDescription\nComplexity\n\n\n\n\nPush\nAdd an element to the top of a stack\nTime and Space: \\(O(1)\\)\n\n\nPop\nRemove an element from the top of a stack\nTime and Space: \\(O(1)\\)\n\n\nPeek\nGet the value of the top element without removing it\nTime and Space: \\(O(1)\\)\n\n\nIsEmpty\nCheck if the stack is empty\nTime and Space: \\(O(1)\\)\n\n\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.next = None\n        self.prev = None\n\n\nclass Stack:\n    # Initializing a stack.\n    # Use a dummy node, which is\n    # easier for handling edge cases.\n    def __init__(self):\n        self.head = None\n        self.size = 0\n\n    # Get the current size of the stack\n    def getSize(self):\n        return self.size\n\n    # Check if the stack is empty\n    def isEmpty(self):\n        return self.size == 0\n\n    # Get the top item of the stack\n    def peek(self):\n        # Sanitary check to see if we\n        # are peeking an empty stack.\n        if self.isEmpty():\n            raise Exception(\"Peeking from an empty stack\")\n        return self.head.value\n\n    # Push a value into the stack.\n    def push(self, value):\n        node = Node(value)\n        if self.head is None:\n            self.head = node\n        else:\n            node.prev = self.head\n            self.head.next = node\n            self.head = node\n        self.size += 1\n\n    # Remove a value from the stack and return.\n    def pop(self):\n        if self.isEmpty():\n            raise Exception(\"Popping from an empty stack\")\n        remove = self.head\n        self.head = self.head.prev\n        self.head.next = None\n        self.size -= 1\n        return remove.value\n\n\nQueue\nQueues are an implementation of LinkedList that follows the FIFO principle. Similarly to ticket queue outside a cinema hall, where the first person entering the queue is the first person who gets the ticket.\n\nQueue complexities\n\n\n\n\n\n\n\nName\nDescription\nComplexity\n\n\n\n\nEnqueue\nAdd an element to the end of the queue\nTime and Space: \\(O(1)\\)\n\n\nDequeue\nRemove an element from the front of the queue\nTime and Space: \\(O(1)\\)\n\n\nPeek\nGet the value of the front of the queue without removing it\nTime and Space: \\(O(1)\\)\n\n\nIsEmpty\nCheck if the queue is empty\nTime and Space: \\(O(1)\\)"
  },
  {
    "objectID": "posts/datastructure/index.html#binary-trees",
    "href": "posts/datastructure/index.html#binary-trees",
    "title": "Data Structures",
    "section": "Binary Trees",
    "text": "Binary Trees\nA binary tree is a tree data structure in which each parent node can have at most two children. Each node of a binary tree consists of three items:\n\nvalue of the node,\nthe address to the left child,\nthe address to the right child.\n\n\nBinary trees complexities\n\n\n\n\n\n\n\nName\nDescription\nComplexity\n\n\n\n\nConstruct\nConstruct a binary tree\nTime and Space: \\(O(N)\\)\n\n\nTravers\nTraverse a binary tree\nTime \\(O(N)\\) and Space: \\(O(height)\\)\n\n\n\nBinary trees are generaly represetned by linked nodes structures.\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    def pre_order_traverse(self):\n        print(self.value)\n        if self.left:\n            self.left.pre_order_traverse()\n        if self.right:\n            self.right.pre_order_traverse()\n\n    def in_order_traverse(self):\n        if self.left:\n            self.left.in_order_traverse()\n\n        print(self.value)\n\n        if self.right:\n            self.right.in_order_traverse()\n\n    def post_order_traverse(self):\n        if self.left:\n            self.left.post_order_traverse()\n\n        if self.right:\n            self.right.post_order_traverse()\n\n        print(self.value)\n\ndef build(array):\n    root = None\n    n = len(array)\n\n    def add_child(idx):\n        if idx &lt; n:\n            node = Node(array[idx])\n\n            node.left = add_child(idx*2 + 1)\n\n            node.right = add_child(idx*2 + 2)\n\n        return node\n\n    root = add_child(0)\n    return root\nHowever, binary trees can be also represented using arrays in which:\n\nroot node is stored at index 0,\nleft child is stored at index \\((i \\cdot 2) + 1\\) where, i is the index of the parent,\nright child is stored at index \\((i \\cdot 2) + 1\\), i is the index of the parent.\n\nclass Tree:\n    def __init__(self, array):\n        self.array = array\n\n    def left(self, parent_idx):\n        if (parent_idx * 2) + 1 &lt; len(self.array):\n            return self.array[(parent_idx * 2) + 1]\n\n    def right(self, parent_idx):\n        if (parent_idx * 2) + 2 &lt; len(self.array):\n            return self.array[(parent_idx * 2) + 2]\n\n    def set_left(self, val, parent_idx):\n        self.array[(parent_idx * 2) + 1] = val\n\n    def set_right(self, val, parent_idx):\n        self.array[(parent_idx * 2) + 2] = val\n\n    def in_order(self, parent_idx=0):\n        if self.left(parent_idx):\n            self.in_order((parent_idx * 2) + 1)\n\n        print(self.array[parent_idx])\n\n        if self.right(parent_idx):\n            self.in_order((parent_idx * 2) + 2)\n\nBinary Search Tree\nBinary search tree is a data structure that quickly allows us to maintain a sorted list of numbers and search trought it.\nThe properties that separate a binary search tree from a regular binary tree are:\n\nall nodes of left subtree are less than the root node;\nall nodes of right subtree are more than the root node;\nboth subtrees of each node are also BSTs i.e. they have the above two properties.\n\nSearching is extreamly efficent as can be done in \\(O(\\log N)\\) time and constant space. Intuitively, searching is so efficent as we can analys only one of the two subtrees based on the relation between the current node’s value and the looked for value. Thus, at every step we half the searching space.\n\nBinary Search trees complexities\n\n\n\n\n\n\n\nName\nDescription\nComplexity\n\n\n\n\nInsertion\nInsert a node to the tree\nTime \\(O(\\log N)\\) and Space: \\(O(N)\\)\n\n\nSearch\nSearch for an element in the tree\nTime \\(O(\\log N)\\) and Space: \\(O(N)\\)\n\n\nDeletion\nRemove an element from the tree\nTime \\(O(\\log N)\\) and Space: \\(O(N)\\)\n\n\n\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left, self.right = None, None\n\n    def search(self, value):\n\n        if self.value == value:\n            return True\n        elif value &lt; self.value and self.value.left is not None:\n            return self.value.left.search(value)\n        elif value &gt; self.value and self.value.right is not None:\n            return self.value.right.search(value)\n        else:\n            return False\n    \n    @staticmethod\n    def insert(node, value):\n        if node is None:\n            return Node(value)\n\n        if value &lt; node.value:\n            node.left = Node.insert(node.left, value)\n        else:\n            node.right = Node.insert(node.right, value)\n\n        return node\n\n    @staticmethod\n    def remove(node, value):\n        if value &lt; node.value:\n            node.left = Node.remove(node.left, value)\n        elif value &gt; node.value:\n            node.right = Node.remove(node.right, value)\n\n        else:\n            # case 1: it is a leaf node\n            if node.left is None and node.right is None:\n                return None\n\n            # case 2: there is only 1 child\n            elif node.left is not None and node.right is None:\n                node.value = node.left.value\n                node.left = None\n            elif node.left is None and node.right is not None:\n                node.value = node.right.value\n                node.right = None\n\n            # case 3: take as new node, the right child or the left node of the right child if it exist\n            else:\n                # get new min value from right\n                temp = node.right\n                prev = node\n                while temp.left is not None:\n                    prev, temp = temp, temp.left\n\n                node.value = temp.value\n\n                if prev != node:\n                    prev.left = temp.right\n                else:\n                    node.right = temp.right\n            return node\n        return node"
  },
  {
    "objectID": "posts/metrics/index.html",
    "href": "posts/metrics/index.html",
    "title": "Evaluation Metrics for Ads Ranking Systems",
    "section": "",
    "text": "Ads ranking systems are the barebone of many modern business and became one of the main success story of machine learning applied to real-world problems. Given an user/client, the main goal of such ranking systems is to order a set of candidates ads according to their click or conversion score. As such it is commonly modelled as a binary classification task where the positive class (\\(y^+\\)) represents clicks or conversions, and the negative class (\\(y^-\\)) represents examples without interaction. In the most common case, the adopted classifier is a probabilistic classifier, which does not provide a class label, but rather the predicted probability of the positive class \\(\\mathbf{P}( y = y^+)\\). According to the application, it is possible to obtain a predicted label by applying a threshold \\(t\\) to \\(\\mathbf{P}( y = y^+)\\):\n\\[\n\\hat{y} = \\begin{cases}\n1 & \\text{if} ~ \\mathbf{P}( y = y^+) \\geq t \\\\\n0 & \\text{o.w.}\n\\end{cases}\n\\]\nIn most classification problem a reasonable threshold is \\(t=0.5\\). However, in many domains the datasets are not balanced, models are not perfectly calibrated, and different use-cases have different sensitivity vs specificity tradeoffs. For example, many real-world applications are dominated by negative examples; thus a more conservative classifier might be prefered. Conservative models produce positive classifications only with strong evidence; thus they are identified by a low False Positive Rate. Probabbilistic classifiers can be made more or less conservative by adjusting the threshold \\(t\\), and many experiments are required to detect the best trade-off. To this end, common evaliation metric that relay on the predicted labels, such as Accuracy or F1-score, are not suited in these domains as they fails to capture some important aspect of the model performance. To overcome this problem, it is desireable to have metric capable to identify model perforamnces based on the predicted probability rather than the predicted label and that are robust to unballanced datasets."
  },
  {
    "objectID": "posts/metrics/index.html#roc-auc",
    "href": "posts/metrics/index.html#roc-auc",
    "title": "Evaluation Metrics for Ads Ranking Systems",
    "section": "ROC-AUC",
    "text": "ROC-AUC\nThe Receiver Operating Characteristic (ROC) is an analysis tool initially proposed by Provost et al. to compare classifiers’ performance. The ROC curve is built as the interpolation between the False Positive Rate (FPR) on the x-axe, and the True Positive Rate (TPR) on the y-axe, computed at different thresholds. Intuitively, the TPR represents how well a classifier can identify all the positive examples. In contrast the FPR indicate how likely an error will occur in the negative class (for a formal definition of TPR and FPR, consult the confusion matrix reported in Tab. 1).\n\n\n\n\n\n\nFigure 1: Example of an ROC-AUC curve.\n\n\n\nAs shown in Figure 1, a model that have higher ROC cureve is deemed to have better performances. However, there is the need to summarise the knowledge captured by a ROC curve in a single scalar value to facilitate the comparison of different models. To this end, the area under the ROC curve is used as a summary statistic representative of the classifier performances. Thus, ROC-AUC is defined as:\n\\[\n\\text{ROC-AUC} = \\int_0^1 TPR ~ \\delta \\small FPR.\n\\]\nNote that ROC-AUC exhibits the following properties [1]:\n\nIt can be interpreted as “the probability that the scores given by a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one” [2] [3]. In other words, the ROC curve shows the ability of the classifier to rank the positive instances relativeto the negative instance only.\nA universal baseline is always available, as a random classifier will have ROC-AUC of 0.5.\nIt cares about the ranking obtained from the predictions but does not consider the actual predicted value.\nA classifier do not need to produce accurate, calibrated probability estimates; it need only to produce relative accurate scores that serve to discriminate positive and negative instances [4].\nThe perfect model is represented by the point \\((0, 1)\\) and has an AUC of 1.\nThe point \\((0, 0)\\) identifies a model that never issues a positive prediction.\nThe point \\((1, 1)\\) implements the opposite strategy: only positive predictions are made.\nROC-AUC is a linear space, thus allowing for easy interpolation and visual interpretation.\n\n\nHighly Imbalanced Dataset\nIn the general case, one of the most attractive properties of the ROC curve is its robustness to changes in the class distribution. This propertly derives from the fact that, ROC curves are defined as a ratios of quantities computed w.r.t. only the positive or only the negative class rather than a mix of the two. Thus, we expect a model to generate similar ROC curves regardless of the number of positive and negative examples present in the dataset. In so doing, ROC curves are a great tools to compare model across different datasets; for example dataset generated across different day.\nHowever, ROC curves are known to be “over-optimistic” at scoring model performances when the datasets are highly skewed, and there is a high interest in evaluating the model w.r.t. the positive class. For example, consider the case where you have two datasets; the former is composed of 100 negative points and 50 positive samples, while the latter is composed of 100 negative examples and 25 positive examples. As shown in Figure 2 (a) , let’s assume that the negative examples overlap with the positive ones according to a uniform distribution.\n\n\n\n\n\n\n\n\n\n\n\n(a) Two datasets with same label distribution but different amount of positive examples. Dataset 1 contains 50 positive examples while Dataset 2 contains only 25 positive examples. In both cases there are 100 negative lables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) ROC-AUC computed on D1 and D2.\n\n\n\n\n\n\n\nFigure 2\n\n\n\nLet’s try to compute the AUC for both of the datasets:\n\nthen for D1, as the threshold (\\(t\\)) moves from \\(0\\) to \\(0.5\\) the True Positive Rate remains constant at 1; instead for \\(t &gt; 0.5\\), both TPR and FPR decrease linearly since positive and negative examples start to be homogeneously mixed.\nsimilarly, for D2, when \\(0 \\leq t \\leq 0.75\\) then \\(TPR = 1\\); while for \\(t&gt;0.75\\) TRP and FPR decreases linearnly.\n\nA graphical representation is provided in Figure 2 (b) showing how the ROC-AUC of the second dataset is more significant than the first dataset even if the models have the same maximum F1-score (maximum F1-score on D1 is achieved with \\(t=0.5\\) while on D2 the best threshold is \\(t=0.5\\)). A deeper analysis suggests that this over-estimation problem arises when a significant change in the false positive leads only to a slight shift in the false positive rate since the dataset is dominated by the negative class [5]."
  },
  {
    "objectID": "posts/metrics/index.html#precision-recall-curve",
    "href": "posts/metrics/index.html#precision-recall-curve",
    "title": "Evaluation Metrics for Ads Ranking Systems",
    "section": "Precision-Recall curve",
    "text": "Precision-Recall curve\nWhen the main objective is to evaluate a model on the positive class, PR-curves are more informative. Figure 3 shows how PR-curves are built by plotting the Precision as a function of the True Positive Rate (or Recall). By inspecting Tab. 1, it is visible how PR-curves effectively consider only statistics related to the positive class; thus, they are inherently robust to highly skewed datasets [6]. Perhaps motivated by the similarity with ROC-curves, PR-curves became a popular alternative to analysis models on highly skewed datasets.\n\n\n\n\n\n\nFigure 3: Precision-Recall curve and PR-AUC.\n\n\n\nAs for ROC curves, the PR-AUC is defined as:\n\\[\n\\text{PR-AUC} = \\int_0^1 PR ~ \\delta \\small TPR.\n\\]\nOverall, we can distinguish the following properties for the PR-AUC:\n\nPR-AUC has no intrinsic meaning except the geometric one.\nWhile ROC-AUC has a baseline value of \\(0.5\\), in PR-AUC there is no universal baseline to compare with.\nPR-AUC is not directly connected with the F1-score; thus is unrelated to the calibration score of the model.\nThe perfect model is represented by the point \\((1, 1)\\) and has an area of 1.\nPR space is an hyperbolic space; thus more difficult to inspect visually and find similar performing models."
  },
  {
    "objectID": "posts/metrics/index.html#logloss",
    "href": "posts/metrics/index.html#logloss",
    "title": "Evaluation Metrics for Ads Ranking Systems",
    "section": "LogLoss",
    "text": "LogLoss\nOne of the limitations of the previous metrics is their focus on the ranking obtained from the model output, but they ignore the predicted value itself. This is not an issue in most recommendation systems, but most real-time ad allocation systems require well-calibrated and accurate prediction values to implement an optimal bidding strategy. In this problem instance, choosing a threshold t is not important. Rather there is high interest in having predictions that, on average, reliability resemble the ground-true. That is: “the probability associated with the predicted class label should reflect its ground truth correctness likelihood” [7]. For example, consider a dataset composed of 10 samples and assume that the model assigns a probability of 0.6 to every instance. Then, if the model is calibrated, we expect 6 examples to belong to the positive class.\nNote that:\n\nCalibration is a measure of uncertainty, not accuracy.\nA calibrated model allows for higher interpretability as the output probabilities have an intrinsic meaning.\n\nA standard metric used to monitor the model’s calibration is the negative LogLoss:\n\\[\n\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^N \\Big( y \\log(\\hat{p}) + (1 - y) \\log(1 - \\hat{p}) \\Big);\n\\]\nwhere \\(y\\) represents the ground-true label, \\(\\hat{p}\\) is the predicted probability, and \\(N\\) is the size of the dataset. The LogLoss has an extended application history as training loss for (deep) logistic regressions, yet here a claim is made to adopt it as a validation and test metric. Modern deep neural networks are highly accurate but overconfident, showing poor uncertainty estimation [8]. Thus, it is handy to have a scalar metric to summarise a model’s calbration characteristics and compare it to the prediction error."
  },
  {
    "objectID": "posts/metrics/index.html#brier-score",
    "href": "posts/metrics/index.html#brier-score",
    "title": "Evaluation Metrics for Ads Ranking Systems",
    "section": "Brier Score",
    "text": "Brier Score\nAnother popular method to capture the model calibration in a scalar value is by computing the Brier Score[9]:\n\\[\n\\mathcal{BS} = \\frac{1}{N} \\sum_{i=1}^N \\Big( y_i - \\hat{p}_i \\Big)^2.\n\\]\nThe Brier Score is the mean-squared-error between the ground ture and the predicted probabilities; thus the lower the value the better."
  },
  {
    "objectID": "posts/gradient_descent_and_backprop/index.html",
    "href": "posts/gradient_descent_and_backprop/index.html",
    "title": "Gradinet Descent and Backpropagation",
    "section": "",
    "text": "Most deep learning algorithm relay on the idea of learning some useful information from the data to solve a specific task. That is, instead of explicitly define every single instruction that a program has to perform, in machine learning, we specify an optimization routine that a program executes over a set of examples to improve its performances. By executing the optimization algorithm, a machine automatically navigates the solution space to find the best “program” that solve the given task starting from a random state: \\(\\mathbf{\\theta}\\). It is expectable that the initial program obtained based on the random state would not perform well on the chosen task, however, by iterating over the dataset, we can adjust \\(\\mathbf{\\theta}\\) until we obtain an optimal solution.\n\nGradient Descent\nOne of the most common learning algorithm is known as Gradient Descent or Stochastic Gradient Descent (SGD) [1]. The core idea of SGD is to iteratively evaluate the difference between the obtained prediction of the model (\\(y_{\\theta}\\)), and, the desired output (\\(y\\)) utilizing a loss function \\(\\mathcal{L}(y_{\\mathbf{\\theta}}, y)\\). Once the difference is known, it is possible to adjust \\(\\theta\\) to reduce the difference or prediction error.\nFormally, SGD is composed of 3 main steps:\n\nevaluate the loss function: \\(\\mathcal{L}(y_{\\mathbf{\\theta}}, y)\\),\ncompute the gradient of the loss function w.r.t. the model parameters: \\(\\nabla \\mathcal{L}_{\\theta} = \\frac{\\partial \\mathcal{L}(y_{\\theta}, y)}{\\partial \\theta}\\),\nupdate the model parameters (or solution) to decrease the loss function: \\(\\mathbf{\\theta} = \\mathbf{\\theta} - \\eta \\nabla \\mathcal{L}_{\\theta}\\).\n\nAs it is possible to notice, such a learning algorithm requires a loss function that is continuous and differentiable; otherwise, it is not applicable. However, over the years, many efficient and effective loss functions have been proposed.\n\n\nBackpropagation\nComputing the analytical gradients for a deep learning algorithm might not be easy, and it is definitely an error-prone procedure. Luckily, over the years mathematicians manage to programmatically compute the derivate of most of the functions with a procedure known as algorithmic differentiation. The application of algorithmic differentiation to compute the SGD is known as backpropagation.\nSupposing to have the current function \\(f(x,y,z) = (x + y) \\cdot z\\). It is possible to simplify it’s computation defining an intermediate function:\n\\[\nq(x, y) = x + y \\Rightarrow f(q, z) = q \\cdot z.\n\\]\nKnowing that:\n\n\\(\\frac{\\partial f}{\\partial q} = z\\)\n\\(\\frac{\\partial f}{\\partial z} = q\\)\n\\(\\frac{\\partial q}{\\partial x} = 1\\)\n\\(\\frac{\\partial q}{\\partial y} = 1\\)\n\nwe can compute \\(\\frac{\\partial f}{\\partial x}\\) by chain rule:\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x}.\n\\]\nThis operation can be seen even as a computational graph, where each node represent an operation ; and using backpropagation it is possible to compute the gradient of function \\(f\\) w.r.t. its input variable \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\n\nFigure 1: The forward and backward pass of the computational graph for the function $ f(x,y,z) = (x + y) z $. (Image taken from Andrej Karpathy slides, CS231n.)\n\n\n\nIt has to be noted that, backpropagation is a local and global process. It is local since a gate, during the forward pass, can compute:\n\nits output value: \\(q = x + y = 3\\),\nas well as its local gradient (the gradient of its input w.r.t. its output): \\(\\frac{\\partial q}{\\partial x} = 1\\) and \\(\\frac{\\partial q}{\\partial y} = 1\\).\n\nIt is global since, a gate need to know the gradient of its output node in order to evaluate the chain rules: \\(\\frac{\\partial f}{\\partial q}\\). The gradient of its ouput is known only during the backward pass, thus all the local computations need to be stored in memory; thus require a lot of memory.\nThe backward pass start by computing: \\(\\frac{f}{\\partial f} = 1\\). Then, knowing that \\(\\frac{\\partial f}{\\partial q} = z\\) and \\(\\frac{\\partial f}{\\partial q} = \\frac{f}{\\partial f} \\frac{\\partial f}{\\partial q} = 1 \\cdot -4 = -4\\). Similarly, \\(\\frac{\\partial f}{\\partial z} = q\\) and \\(\\frac{\\partial f}{\\partial z} = \\frac{f}{\\partial f} \\frac{\\partial f}{\\partial z} = 3\\). Finally, our goal is to goal is to compute: \\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = -4 \\cdot 1 = -4\n\\] and, \\[\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = -4 \\cdot 1 = -4\n\\].\n\n\nWeight Decay\nTo achieve better generaliation performance it is well known that graient updates needs to be regularized so to have sparse or force small weights magnitude. The two most common regularizations for gradiens are L1-regularization or weight decay [2] (equivalent to the L2-regularization):\n\\[\n\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\partial f(x; \\theta_t)}{\\partial \\theta_t} - \\lambda \\theta_t\n\\]\nwhere \\(\\lambda \\theta_t\\) stand for weight decay or L2-regularization. However, weight dacay and L2-regularization are equivalent only for SDG, but not for Adam or other adaptive optimizers. Instead of applying the same learning rate to all parameters, Adam apply a different learning rate to each parameters proportional to the update signals they recently recevied (a.k.a proportional to the recent gradients). As Adam uses a different learning rate per each parameters, it means that L2-regularization is not only affected by \\(\\lambda\\) but also from the learning rate and the momentum. Thus, Adam requires a bigger regularizer coefficent to achieve comparable performance as SGD.\n\n\nReferences\n\n\n1. Hanson S, Pratt L (1988) Comparing biases for minimal network construction with back-propagation. Advances in neural information processing systems 1\n\n\n2. Gugger S, Howard J Fast.ai - AdamW and Super-convergence is now the fastest way to train neural nets — fast.ai"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is my personal space where I keep all my notes from my ML experience. I’m mostly interested in NLP and graph embeddings/analytics. Thus expect most of the material to cover:\n\nFundamentals of Deep Learning;\nFundamentals of Reinforcement Learning;\nBasic Stats;\nBasic of Neural ODE.\n\nPlease note that this website is ment to keep only my personal material, this it my contains errors and typos.\n\nHere are some working principles I try to apply during my working expeience.\n\n\n\nThe Skunk Works workers must be delegated practically complete control of his program in all aspects.\nThe number of people having any connection with the project must be restricted in an almost vicious manner. Use a small number of good people (10% to 25% compared to the so-called normal systems).\nA very simple drawing and drawing release system with great flexibility for making changes must be provided.\nThere must be a minimum number of reports required, but important work must be recorded thoroughly.\nAccess by outsiders to the project and its personnel must be strictly controlled by appropriate security measures.\nBecause only a few people will be used in engineering and most other areas, ways must be provided to reward good performance by pay not based on the number of personnel supervised.\n\n\n\n\n\nMake the requirements less dumb. “Your requirements are definitely dumb, it does not matter who gave them to you.” Uses this step to test assumptions and question-the-question, especialy if it come from a ‘smart person’ since you might not question them enough.\nDelete the part or process. “If you’re not adding things back in at least 10% of the time, you’re clearly not deleting enough.” Starting lean and building up when and if required, but be strongly bias throward removing thinks. Each requirement or constraint must be accountable to a person, so you can ask that person about its relevance and purpose.\nSimplify or optimise the design. “Possibly the most common error of a smart engineer is to optimise a thing that should not exist”. Do your work and go through the first two steps before trying to optimise. To do this effectively you need an holistic view of the project and you need to consider the overall cost-benefit for a more complicated design.\nAccelerate cycle time. “Try a bunch of things, and do more of what works”. Once you have find the right direction is time to accelerate the lifecycle. Aim for an experiment a day. Note that, it is extreamly importanto to do the first 3 steps before as “if you’re digging your grave, don’t dig faster.”"
  },
  {
    "objectID": "index.html#my-vision-of-skunkworks-rules",
    "href": "index.html#my-vision-of-skunkworks-rules",
    "title": "Welcome",
    "section": "",
    "text": "The Skunk Works workers must be delegated practically complete control of his program in all aspects.\nThe number of people having any connection with the project must be restricted in an almost vicious manner. Use a small number of good people (10% to 25% compared to the so-called normal systems).\nA very simple drawing and drawing release system with great flexibility for making changes must be provided.\nThere must be a minimum number of reports required, but important work must be recorded thoroughly.\nAccess by outsiders to the project and its personnel must be strictly controlled by appropriate security measures.\nBecause only a few people will be used in engineering and most other areas, ways must be provided to reward good performance by pay not based on the number of personnel supervised."
  },
  {
    "objectID": "index.html#musks-development-principles",
    "href": "index.html#musks-development-principles",
    "title": "Welcome",
    "section": "",
    "text": "Make the requirements less dumb. “Your requirements are definitely dumb, it does not matter who gave them to you.” Uses this step to test assumptions and question-the-question, especialy if it come from a ‘smart person’ since you might not question them enough.\nDelete the part or process. “If you’re not adding things back in at least 10% of the time, you’re clearly not deleting enough.” Starting lean and building up when and if required, but be strongly bias throward removing thinks. Each requirement or constraint must be accountable to a person, so you can ask that person about its relevance and purpose.\nSimplify or optimise the design. “Possibly the most common error of a smart engineer is to optimise a thing that should not exist”. Do your work and go through the first two steps before trying to optimise. To do this effectively you need an holistic view of the project and you need to consider the overall cost-benefit for a more complicated design.\nAccelerate cycle time. “Try a bunch of things, and do more of what works”. Once you have find the right direction is time to accelerate the lifecycle. Aim for an experiment a day. Note that, it is extreamly importanto to do the first 3 steps before as “if you’re digging your grave, don’t dig faster.”"
  },
  {
    "objectID": "posts/linear_regression/index.html",
    "href": "posts/linear_regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression [1] is a widely utilized and straightforward machine learning model. It establishes a linear relationship between a dependent variable (represented on the y-axis) and one or more independent variables (represented on the x-axis). The fundamental formulation of linear regression is as follows:\n\\[\n\\newcommand{\\rvepsilon}{\\mathbf{\\epsilon}}\n\\newcommand{\\rvtheta}{\\mathbf{\\theta}}\n\\newcommand{\\rva}{\\mathbf{a}}\n\\newcommand{\\rvb}{\\mathbf{b}}\n\\newcommand{\\rvc}{\\mathbf{c}}\n\\newcommand{\\rvd}{\\mathbf{d}}\n\\newcommand{\\rve}{\\mathbf{e}}\n\\newcommand{\\rvi}{\\mathbf{i}}\n\\newcommand{\\rvj}{\\mathbf{j}}\n\\newcommand{\\rvq}{\\mathbf{q}}\n\\newcommand{\\rvu}{\\mathbf{u}}\n\\newcommand{\\rvv}{\\mathbf{v}}\n\\newcommand{\\rvw}{\\mathbf{w}}\n\\newcommand{\\rvx}{\\mathbf{x}}\n\\newcommand{\\rvy}{\\mathbf{y}}\n\\newcommand{\\rmA}{\\mathbf{A}}\n\\newcommand{\\rmB}{\\mathbf{B}}\n\\newcommand{\\rmC}{\\mathbf{C}}\n\\newcommand{\\rmH}{\\mathbf{H}}\n\\newcommand{\\rmI}{\\mathbf{I}}\n\\newcommand{\\rmM}{\\mathbf{M}}\n\\newcommand{\\rmS}{\\mathbf{S}}\n\\newcommand{\\rmU}{\\mathbf{U}}\n\\newcommand{\\rmV}{\\mathbf{V}}\n\\newcommand{\\rmW}{\\mathbf{W}}\n\\newcommand{\\rmX}{\\mathbf{X}}\n\\newcommand{\\rmY}{\\mathbf{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\]\n\\[\ny = w_0 + w_1 x_1 + ... + w_n x_n + \\epsilon\n\\tag{1}\\]\nwhere:\n\n\\(y\\) is the dependent variable, also referred to as the response, target, or outcome variable.\n\\(x_i\\) represents the independent variables, also known as predictors or features.\n\\(w_i\\) are the model parameters establishing a linear relationship between the independent variables and the target. These coefficients indicate the expected change in the dependent variable for a one-unit change in the respective independent variable, with all other variables held constant.\n\\(\\epsilon\\) is the error term, which account for the viariability in \\(y\\) that can not be explained by linear model.\n\nIf we assume that the only source of error in our model would the difference between the predicted outcome (\\(\\hat{y}\\)) and the actual value (\\(y\\)) of the dependent variable; then learning process would adjust the parameteris to minimise the error terms:\n\\[\n\\begin{align}\n\\hat{y} & = w_0 + w_1 x_1 + ... + w_n x_n \\\\\n\\epsilon &= y - \\hat{y}.\n\\end{align}\n\\tag{2}\\]\nWhile directly minimising \\(\\epsilon\\) is impractical, a common alternative is to optimise the following loss function:\n\\[\n\\arg \\min \\mathcal{L}(y, \\hat{y}; \\rvw) = (y - \\hat{y})^2.\n\\tag{3}\\]\nNote that Equation 2 and Equation 3 entail minimisation of the vertical distance between \\(\\hat{y}\\) and \\(y\\), also knonw as residuals. Thus, there is an implicit assumtion that the only possible error would be related to the model not being accurate enough in predicting the actual value, while the measurments of the independent variable are error-free. Finally, this sort of model formulation is usually known as the least-square model as Equation 2 is called least-square method and Equation 3 is the sum-of-sqaured-residual loss.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nn_datapoints = 10\nnp.random.seed(1)\nx = 10 * np.random.rand(n_datapoints)\ny = 2 * x + 1 + (10 * np.random.randn(n_datapoints))\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(x[:, np.newaxis], y)\n# get prediction\ny_pred = model.predict(x[:, np.newaxis])\n\nresidual = y - y_pred\n\n# get prediction for best line fit\nxfit = np.linspace(0, 8, 50)\ny_ = model.predict(xfit[:, np.newaxis])\n\nfig, axs = plt.subplots(1, 2, figsize=(10.5, 4))\n\nax = axs[0]\nax.scatter(\n    x,\n    y,\n    label=\"Data points\",\n    edgecolors=\"k\",\n)\n\n# plot data\nax.plot(\n    [0, 8],\n    [y.mean(), y.mean()],\n    color=\"#ff7f0e\",\n    label=\"Initial fit\",\n)\nfor i in range(len(x)):\n    ax.plot(\n        [x[i], x[i]],\n        [y[i], y.mean()],\n        color=\"gray\",\n        linestyle=\"--\",\n    )\nax.grid(True)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.legend()\nax.set_title(\"Initial model\")\n\nax = axs[1]\nax.scatter(\n    x,\n    y,\n    label=\"Data points\",\n    edgecolors=\"k\",\n)\n# plot best line fit\nax.plot(\n    xfit,\n    y_,\n    color=\"#2ca02c\",\n    label=\"Best fit\",\n)\n# Optionally, plot residuals (errors)\nfor i in range(len(x)):\n    ax.plot(\n        [x[i], x[i]],\n        [y[i], y_pred[i]],\n        color=\"gray\",\n        linestyle=\"--\",\n    )\nax.scatter(\n    x, y_pred, color=\"green\", label=\"Predicted value\"\n)  # If you want to show where the predicted points lie on the line\n\nax.annotate(\n    \"residual\",\n    xy=(1, -10),\n    xycoords=\"data\",\n    xytext=(0.2, 0.1),\n    textcoords=\"axes fraction\",\n    va=\"top\",\n    ha=\"left\",\n    fontsize=16,\n    arrowprops=dict(\n        arrowstyle=\"-&gt;\",\n        facecolor=\"black\",\n    ),\n)\n\nax.grid(True)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Fited model\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Linear model fitted to a given data distribution by means of least-square. Note that the differece between the predicted value and the given datapoint is known as residual.\n\n\n\n\n\n\n\nAs shown in Figure 1 a linear model find the line that minimises the distnace between the fitted line and the data points. While it is possible to use SGD to find a solution, a better solution to Equation 3 can be obtained by linear algebra. Assuming a dataset with composed of m elements, we can rewrite the loss function as:\n\\[\n\\begin{align}\n\\arg \\min \\mathcal{L}(y, \\hat{y}) &= \\sum_{i=1}^m(y_i - \\hat{y}_i)^2 \\\\\n& = (\\rvy^T - (\\rmX \\rvw^T))^2 \\\\\n& = \\rvd^2 \\\\\n& = \\rvd \\rvd^T \\\\\n& = || \\rvd ||^2\n\\end{align}\n\\tag{4}\\]\nwhere:\n\n\\(\\rvy^T\\) is the collumn vector \\(\\left[\\begin{align} y_1 \\\\ \\vdots \\\\ y_m \\end{align}\\right]\\);\n\\(\\rmX\\) is the \\(m \\times n\\) matrix \\(\\left[\\begin{array}{ccc} 1 & x_{1,1} & \\cdots & x_{n,1} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_{1,m} & \\cdots & x_{n,m}\\end{array}\\right]\\);\n\\(\\rvw^T\\) is the is the column vector \\(\\left[\\begin{align} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n \\end{align}\\right]\\);\n\\(\\rvd\\) is the row vector \\(\\left[y_1 - \\hat{y}_1, ...,  y_m - \\hat{y}_m\\right]\\).\n\nIt is then possible to compute the optimal paramters by differenciating w.r.t. \\(\\rvw\\): \\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(y, \\hat{y})}{\\partial \\rvw^T} = & || \\rvy^T - \\rmX \\rvw^T ||^2  = 0 \\\\\n& 2\\rmX^T (\\rvy^T - \\rmX \\rvw^T) = 0 \\\\\n& \\rmX^T\\rvy^T - \\rmX^T \\rmX \\rvw^T = 0 \\\\\n& \\rvw^T = \\frac{\\rmX^T\\rvy^T}{\\rmX^T \\rmX}\n\\end{align}\n\\]\nNote that \\(\\rmX^T \\rmX\\) is an \\(n \\times n\\) matrix; thus could be invertible, \\(\\rmX^T\\rvy^T\\) is an \\(n \\times 1\\) column vector and \\(\\rvw^T\\) is the \\(n \\times 1\\) column vector of unknown parameters. Finally, it is important to list some of the key assumptions of this type of model:\n\nLinearity: There must be a linear relationship between the independent and dependent variables.\nIndependence: The predictors should be independent of each other. This is vital for the model’s stability and interpretability. Collinearity, or correlation between variables, can lead to significant changes in the outcome variable for minor alterations in the predictor variables, contradicting the assumption of a linear relationship. Additionally, a model with independent variables is easier to interpret as each variable contributes uniquely to the prediction.\nHomoscedasticity: The residuals should be uniformly distributed with constant variance. Without this, it becomes challenging to ensure that the model is unbiased and to conduct accurate error analysis.\n\n\n\n\nResiduals plots [2] [3] are one of the most common methods to validate the presence of Homoscedasticity. As shonw in Figure 2, residuals plots disply the residual values of a regression as a function of the predicted values and helps in understanding if the model is a good fit for the given data. Namelly residuals plot can be used for:\n\nChecking Homoscedasticity: A key assumption in linear regression is that the residuals have constant variance at every level of the independent variable(s) (homoscedasticity). If the residuals fan out or form a pattern as the predicted values increase, this indicates heteroscedasticity, which can lead to inefficient estimates and affect hypothesis testing.\nIdentifying Non-Linearity: If the relationship between the variables is not linear, the residuals will often display a systematic pattern. A residuals plot can help identify such non-linear relationships, suggesting that a different model or a transformation of variables might be more appropriate.\nDetecting Outliers: Residuals plots can reveal outliers – points that have a much larger error than the other observations. These outliers can disproportionately influence the model fit and can be indicative of data entry errors, or that the outlier is from a different population.\nEvaluating Model Fit: If a model is a good fit for the data, the residuals should be randomly scattered around zero. Patterns or trends in the residuals suggest that the model is not capturing some aspect of the data, which could lead to biased or inaccurate predictions.\nChecking Independence of Residuals: For a good model, the residuals should be independent of each other. If there’s a pattern over time (in time series data) or a pattern associated with another variable, this is a sign that the model is missing a key piece of information.\nVerifying Normal Distribution of Residuals: In many forms of regression analysis (like linear regression), it’s assumed that residuals are normally distributed. By plotting the residuals and visually checking their distribution (or using statistical tests), we can validate this assumption. A common method is to use a Q-Q plot (quantile-quantile plot) to compare the distribution of the residuals to a normal distribution.\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 4))\n\nax.scatter(y_pred, residual, label=\"residual\", color=\"gray\", edgecolors=\"k\")\nax.hlines(0, 0, 15, colors=\"k\", linestyles=\"dashed\")\n\nz = np.polyfit(y_pred, residual, 1)\nresidual_pred = (z[0] * y_pred) + z[1]\nax.plot(y_pred, residual_pred, color=\"red\", label=\"residual fit\")\n\nax.grid(True)\nax.set_xlabel(\"prediction\")\nax.set_ylabel(\"residual\")\nax.legend()\n\n\n\n\n\n\n\n\nFigure 2: Residual plot. As you can see, the residual are simmetrically randomly distributed and in general, there aren’t any clear patterns. Moreover, the best-fit line to the residual is almost identical to the x-axis (y=0) indicating independence bewteen residuals and predicted values.\n\n\n\n\n\n\n\n\n\nIn the real world, datasets often suffer from multicollinearity among independent variables. One effective method to mitigate this issue is through Ridge Regression, described by the formula:\n\\[\nY = \\epsilon + w_1 X_1 + ... + w_n X_n + \\lambda \\sum_{1 \\leq i \\leq n} w_i^2.\n\\]\nRidge Regression is essentially linear regression augmented with a penalty term. This term comprises the squared coefficients of the model, effectively minimizing their magnitude. This added penalty increases the model’s bias but decreases prediction variance by imposing a normal distribution prior on the model parameters.\n\nOn the other hand, Lasso Regression is another widely-used variation of linear regression. Similar to Ridge, Lasso adds a penalty to the loss function of the linear model. However, Lasso minimizes the absolute value of the coefficients rather than their square:\n\\[\nY = \\epsilon + w_1 X_1 + ... + w_n X_n + \\lambda \\sum_{1 \\leq i \\leq n} |w_i|.\n\\]\nThe key distinction is that Lasso regression can reduce some coefficients to zero, producing a sparse model, whereas Ridge regression only reduces them to near zero.\nIn conclusion, both these regularization approaches generate a model that has a bigger bias, but better generalization capability.\n\n\n\nThere are instances when we seek to comprehend how one of our predictors influences the dependent variable. Specifically, our interest lies in determining whether the parameter \\(w_i\\) significantly affects the response variable \\(Y\\) - that is, whether including the predictor \\(X_i\\) leads to a notable reduction in the model’s loss.\nFormally, this involves testing the following hypotheses:\n\\[\nH_0 : w_i = 0 \\\\\nH_1 : w_i \\neq 0.\n\\]\nFor clarity, let’s denote \\(L(Y, \\hat{Y}: \\theta_{\\not i})\\) and \\(L(Y, \\hat{Y}: \\theta_{i})\\) as the sum of squared residuals for models excluding and including the \\(i\\)-th predictor, respectively. Assuming independence and homoscedasticity of the model’s parameters, the significance of the \\(i\\)-th predictor can be assessed using the F-test:\n\\[\nF = \\frac{ \\frac{L(Y, \\hat{Y}: \\theta_{\\not i}) - L(Y, \\hat{Y}: \\theta_{i})}{p_2} }{ \\frac{L(Y, \\hat{Y}: \\theta_{i})}{n - p} } .\n\\]\nHere, represent the degrees of freedom for the overall model and the model containing only the \\(i\\)-th predictor; while \\(n\\) is the number of training examples.\nThe numerator of \\(F\\)-test represents the reduction in the residual sum of squares per additional degree of freedom utilized. The denominator is an estimate of the residual variance, serving as a measure of the model’s inherent noise. An \\(F\\)-ratio of one suggests that the predictors merely contribute noise. A ratio greater than one implies meaningful contribution, or signal, from the predictors. Typically, we reject \\(H_0\\) and conclude that the \\(i\\)-th variable significantly impacts the response if the \\(F\\)-statistic exceeds the 95th percentile of the \\(F\\)-distribution with \\(p_2\\) and \\((n-p)\\) degrees of freedom. A full derivation of this result is available here.\n\n\n\nThe \\(R^2\\) metric, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a regression model. It represents the proportion of the variance in the dependent variable that is predictable from the independent variables. In simpler terms, \\(R^2\\) indicates how well the data fit the regression model (the closer the value of \\(R^2\\) is to 1, the better the fit) and can be computed as:\n\\[\nR^2 = \\frac{\\mathcal(Y, \\hat{Y}; \\varnothing) - \\mathcal(Y, \\hat{Y}; \\theta)}{\\mathcal(Y, \\hat{Y}; \\varnothing)}\n\\]\nhere \\(\\mathcal(Y, \\hat{Y}; \\varnothing)\\) represent the sum of squared residuals of a model wihtout parameters, a.k.a a model that alwasy predict the mean of the response variable. Similarly, \\(\\mathcal(Y, \\hat{Y}; \\theta)\\) is the linear regression developed.\nThe \\(R^2\\) value ranges from 0 to 1. A vale of 0 means that the model does not explain any of the variability of the response data around its mean. On the other hand, an \\(R^2\\) of 1 indicates that the model explains all the variability of the response data around its mean. In conclusion \\(R^2\\) is a simple to compute, yet informative metric to determin how much our variance our model is able to predict correctly.\n\n\n\nIn general it is possible to rank the model performance in terms of \\(\\mathcal{L}\\). Thus, here are useful methods to reduce the sum of squared residuals:\n\nFeature Selection: Choose relevant and significant variables to include in the model.\nTransformation of Variables: Apply transformations (like log, square root, or inverse) to make the relationship more linear.\nPolynomial Regression: Use higher-order terms (quadratic, cubic, etc.) if the relationship is not purely linear.\nInteraction Terms: Include interaction terms if the effect of one variable depends on another. An interaction variable in a linear model represents the combined effect of two or more variables on the dependent variable, where the effect of one variable depends on the level of another variable. In other words, it’s used to capture situations where the relationship between a predictor and the outcome changes based on the value of another predictor. Formally, an interaction variable in alinear model is defined as:\n\n\\[\nY = \\epsilon + w_1 X_1 + ... + w_n X_n + w_{n+1} (X_i \\cdot X_j)\n\\]\n\nRegularization Techniques: Methods like Ridge, Lasso, or Elastic Net can help in reducing overfitting and improving prediction.\nResidual Plots: Use residual plots to check for non-linearity, unequal error variances, and outliers.\nInfluence Measures: Identify and investigate influential observations that might disproportionately affect the model’s performance.\nHomoscedasticity Testing: Ensure that residuals have constant variance across different levels of predictors."
  },
  {
    "objectID": "posts/linear_regression/index.html#residual-plot",
    "href": "posts/linear_regression/index.html#residual-plot",
    "title": "Linear Regression",
    "section": "",
    "text": "Residuals plots [2] [3] are one of the most common methods to validate the presence of Homoscedasticity. As shonw in Figure 2, residuals plots disply the residual values of a regression as a function of the predicted values and helps in understanding if the model is a good fit for the given data. Namelly residuals plot can be used for:\n\nChecking Homoscedasticity: A key assumption in linear regression is that the residuals have constant variance at every level of the independent variable(s) (homoscedasticity). If the residuals fan out or form a pattern as the predicted values increase, this indicates heteroscedasticity, which can lead to inefficient estimates and affect hypothesis testing.\nIdentifying Non-Linearity: If the relationship between the variables is not linear, the residuals will often display a systematic pattern. A residuals plot can help identify such non-linear relationships, suggesting that a different model or a transformation of variables might be more appropriate.\nDetecting Outliers: Residuals plots can reveal outliers – points that have a much larger error than the other observations. These outliers can disproportionately influence the model fit and can be indicative of data entry errors, or that the outlier is from a different population.\nEvaluating Model Fit: If a model is a good fit for the data, the residuals should be randomly scattered around zero. Patterns or trends in the residuals suggest that the model is not capturing some aspect of the data, which could lead to biased or inaccurate predictions.\nChecking Independence of Residuals: For a good model, the residuals should be independent of each other. If there’s a pattern over time (in time series data) or a pattern associated with another variable, this is a sign that the model is missing a key piece of information.\nVerifying Normal Distribution of Residuals: In many forms of regression analysis (like linear regression), it’s assumed that residuals are normally distributed. By plotting the residuals and visually checking their distribution (or using statistical tests), we can validate this assumption. A common method is to use a Q-Q plot (quantile-quantile plot) to compare the distribution of the residuals to a normal distribution.\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 4))\n\nax.scatter(y_pred, residual, label=\"residual\", color=\"gray\", edgecolors=\"k\")\nax.hlines(0, 0, 15, colors=\"k\", linestyles=\"dashed\")\n\nz = np.polyfit(y_pred, residual, 1)\nresidual_pred = (z[0] * y_pred) + z[1]\nax.plot(y_pred, residual_pred, color=\"red\", label=\"residual fit\")\n\nax.grid(True)\nax.set_xlabel(\"prediction\")\nax.set_ylabel(\"residual\")\nax.legend()\n\n\n\n\n\n\n\n\nFigure 2: Residual plot. As you can see, the residual are simmetrically randomly distributed and in general, there aren’t any clear patterns. Moreover, the best-fit line to the residual is almost identical to the x-axis (y=0) indicating independence bewteen residuals and predicted values."
  },
  {
    "objectID": "posts/linear_regression/index.html#robust-linear-model",
    "href": "posts/linear_regression/index.html#robust-linear-model",
    "title": "Linear Regression",
    "section": "",
    "text": "In the real world, datasets often suffer from multicollinearity among independent variables. One effective method to mitigate this issue is through Ridge Regression, described by the formula:\n\\[\nY = \\epsilon + w_1 X_1 + ... + w_n X_n + \\lambda \\sum_{1 \\leq i \\leq n} w_i^2.\n\\]\nRidge Regression is essentially linear regression augmented with a penalty term. This term comprises the squared coefficients of the model, effectively minimizing their magnitude. This added penalty increases the model’s bias but decreases prediction variance by imposing a normal distribution prior on the model parameters.\n\nOn the other hand, Lasso Regression is another widely-used variation of linear regression. Similar to Ridge, Lasso adds a penalty to the loss function of the linear model. However, Lasso minimizes the absolute value of the coefficients rather than their square:\n\\[\nY = \\epsilon + w_1 X_1 + ... + w_n X_n + \\lambda \\sum_{1 \\leq i \\leq n} |w_i|.\n\\]\nThe key distinction is that Lasso regression can reduce some coefficients to zero, producing a sparse model, whereas Ridge regression only reduces them to near zero.\nIn conclusion, both these regularization approaches generate a model that has a bigger bias, but better generalization capability."
  },
  {
    "objectID": "posts/linear_regression/index.html#parameters-analysis",
    "href": "posts/linear_regression/index.html#parameters-analysis",
    "title": "Linear Regression",
    "section": "",
    "text": "There are instances when we seek to comprehend how one of our predictors influences the dependent variable. Specifically, our interest lies in determining whether the parameter \\(w_i\\) significantly affects the response variable \\(Y\\) - that is, whether including the predictor \\(X_i\\) leads to a notable reduction in the model’s loss.\nFormally, this involves testing the following hypotheses:\n\\[\nH_0 : w_i = 0 \\\\\nH_1 : w_i \\neq 0.\n\\]\nFor clarity, let’s denote \\(L(Y, \\hat{Y}: \\theta_{\\not i})\\) and \\(L(Y, \\hat{Y}: \\theta_{i})\\) as the sum of squared residuals for models excluding and including the \\(i\\)-th predictor, respectively. Assuming independence and homoscedasticity of the model’s parameters, the significance of the \\(i\\)-th predictor can be assessed using the F-test:\n\\[\nF = \\frac{ \\frac{L(Y, \\hat{Y}: \\theta_{\\not i}) - L(Y, \\hat{Y}: \\theta_{i})}{p_2} }{ \\frac{L(Y, \\hat{Y}: \\theta_{i})}{n - p} } .\n\\]\nHere, represent the degrees of freedom for the overall model and the model containing only the \\(i\\)-th predictor; while \\(n\\) is the number of training examples.\nThe numerator of \\(F\\)-test represents the reduction in the residual sum of squares per additional degree of freedom utilized. The denominator is an estimate of the residual variance, serving as a measure of the model’s inherent noise. An \\(F\\)-ratio of one suggests that the predictors merely contribute noise. A ratio greater than one implies meaningful contribution, or signal, from the predictors. Typically, we reject \\(H_0\\) and conclude that the \\(i\\)-th variable significantly impacts the response if the \\(F\\)-statistic exceeds the 95th percentile of the \\(F\\)-distribution with \\(p_2\\) and \\((n-p)\\) degrees of freedom. A full derivation of this result is available here."
  },
  {
    "objectID": "posts/linear_regression/index.html#r-squared",
    "href": "posts/linear_regression/index.html#r-squared",
    "title": "Linear Regression",
    "section": "",
    "text": "The \\(R^2\\) metric, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a regression model. It represents the proportion of the variance in the dependent variable that is predictable from the independent variables. In simpler terms, \\(R^2\\) indicates how well the data fit the regression model (the closer the value of \\(R^2\\) is to 1, the better the fit) and can be computed as:\n\\[\nR^2 = \\frac{\\mathcal(Y, \\hat{Y}; \\varnothing) - \\mathcal(Y, \\hat{Y}; \\theta)}{\\mathcal(Y, \\hat{Y}; \\varnothing)}\n\\]\nhere \\(\\mathcal(Y, \\hat{Y}; \\varnothing)\\) represent the sum of squared residuals of a model wihtout parameters, a.k.a a model that alwasy predict the mean of the response variable. Similarly, \\(\\mathcal(Y, \\hat{Y}; \\theta)\\) is the linear regression developed.\nThe \\(R^2\\) value ranges from 0 to 1. A vale of 0 means that the model does not explain any of the variability of the response data around its mean. On the other hand, an \\(R^2\\) of 1 indicates that the model explains all the variability of the response data around its mean. In conclusion \\(R^2\\) is a simple to compute, yet informative metric to determin how much our variance our model is able to predict correctly."
  },
  {
    "objectID": "posts/linear_regression/index.html#improvements",
    "href": "posts/linear_regression/index.html#improvements",
    "title": "Linear Regression",
    "section": "",
    "text": "In general it is possible to rank the model performance in terms of \\(\\mathcal{L}\\). Thus, here are useful methods to reduce the sum of squared residuals:\n\nFeature Selection: Choose relevant and significant variables to include in the model.\nTransformation of Variables: Apply transformations (like log, square root, or inverse) to make the relationship more linear.\nPolynomial Regression: Use higher-order terms (quadratic, cubic, etc.) if the relationship is not purely linear.\nInteraction Terms: Include interaction terms if the effect of one variable depends on another. An interaction variable in a linear model represents the combined effect of two or more variables on the dependent variable, where the effect of one variable depends on the level of another variable. In other words, it’s used to capture situations where the relationship between a predictor and the outcome changes based on the value of another predictor. Formally, an interaction variable in alinear model is defined as:\n\n\\[\nY = \\epsilon + w_1 X_1 + ... + w_n X_n + w_{n+1} (X_i \\cdot X_j)\n\\]\n\nRegularization Techniques: Methods like Ridge, Lasso, or Elastic Net can help in reducing overfitting and improving prediction.\nResidual Plots: Use residual plots to check for non-linearity, unequal error variances, and outliers.\nInfluence Measures: Identify and investigate influential observations that might disproportionately affect the model’s performance.\nHomoscedasticity Testing: Ensure that residuals have constant variance across different levels of predictors."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Linear Regression\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nNormalizing Flows\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nFinetune Large Models\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient and Scalable Machine Learning Pipelines\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nData Structures\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\n\n\n\nOct 9, 2022\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation Metrics for Ads Ranking Systems\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nVariational Autoencoders\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nJan 10, 2021\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Principles of Linear Algebra\n\n\n\n\n\n\nLinear Algebra\n\n\n\n\n\n\n\n\n\nDec 23, 2020\n\n\nSandro Cavallari\n\n\n\n\n\n\n\n\n\n\n\n\nGradinet Descent and Backpropagation\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nDec 22, 2020\n\n\nSandro Cavallari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/finetune/index.html",
    "href": "posts/finetune/index.html",
    "title": "Finetune Large Models",
    "section": "",
    "text": "Nowadays, it is common practice to develop new machine learning projects starting from a large pre-trained model and fine-tuning it to the task at hand. Sam Altman, at the time of writing the CEO of OpenAI, recently mentioned that he envisions a feature where the most valuable startups are the ones capable of adapting publicly available foundation models to specific domains; rather than training a proprietary model from scratch. Thus, tuning or fine-tuning large models is a key capability that machine learning practitioners need to learn as much as being able to train a deep neural network was a major skill that each scientist had perfected in the last few years.\nOn the web, there are a plethora of recipes for training neural networks (thanks Karpathy you saved me multiple times). Instead, there are only limited amount of resources tackling the fine-tuning problem. To this end, this article aims at describing the strategy that I adopt when fine-tuning a large Transformer model."
  },
  {
    "objectID": "posts/finetune/index.html#dataset-preprocessing",
    "href": "posts/finetune/index.html#dataset-preprocessing",
    "title": "Finetune Large Models",
    "section": "1. Dataset Preprocessing",
    "text": "1. Dataset Preprocessing\nOften catastrophic forgettin is caused by a bad implementation of the preprocessing steps applied to the fine-tuning dataset.\nOn the one hand, for natural language processing tasks additional cares need to be taken during the tokenization of the input text. Specifically, always double-check that the correct tokenizer is applied, and that the correct PAD-token is adopted. HuggingFace made a terrific effort in providing a bug-free implementation of most Transformers models, but sub-word tokenizers remain a challenging module to operate. Another error-prone transformation is the creation of the input mask: always assert that causal and padding masks are correctly combined and applied in the correct layer.\nOn the other hand, in computing vision tasks, ensure that the proper transformations are applied to the input images. Order of the input channel, and normalization and interpolation strategy for the resizing steps are among the most error-prone functionality to correctly re-create.\nFinally, if you are working with graph structures, double-check how you uniformize to a fixed size a batch of nodes having a different amount of neighbours."
  },
  {
    "objectID": "posts/finetune/index.html#optimizer",
    "href": "posts/finetune/index.html#optimizer",
    "title": "Finetune Large Models",
    "section": "2. Optimizer",
    "text": "2. Optimizer\nAccording to the key principle of this article, the fine-tuning procedure should use the same optimizer adopted during training. While this is not always possible, most of the foundation models [1] are trained by applying the AdamW optimize [2]. AdamW is a variation of the well-known Adam algorithm that better generalize to unknown examples thanks to the decoupling of the main loss and the regularization term known as weight decay. Note that back in the day, PyTorch did not provide a proper implementation of the AdamW. Thus, multiple open-source projects provided their implementation. Across the many, the one provided by Meta’s fairseq is highly reliable and I still use it nowadays."
  },
  {
    "objectID": "posts/finetune/index.html#weight-decay",
    "href": "posts/finetune/index.html#weight-decay",
    "title": "Finetune Large Models",
    "section": "3. Weight Decay",
    "text": "3. Weight Decay\nBy default, AdamW will apply the weight decay mechanism to all models’ parameters, yet in most cases, weight decay is NOT applied to all weights. The weight decay objective is to regularise the training process toward learning models with weights smaller in magnitude. To this end, biases and gains are usually not included in the weight decay loss for multiple reasons:\n\nBiases are used to shift the activation function of a neuron and do not typically have a significant impact on the overall complexity of the model. As a result, applying weight decay to the biases would not have a significant effect on the model’s ability to overfit.\nGains are commonly used in normalization layers to enable high expressivity while ensuring gaussian-like activations. Thus, biases do not typically have a significant impact on the model complexity. On the contrary, weight decay applied to gains might limit the expressivity of a model resulting in underfitting.\nOverall, weight decay is typically applied only to the weights of a model, rather than the biases or gains, to encourage the model to use small weights and reduce the risk of overfitting. In PyTorch this is commonly achieved by creating two groups of parameters for the optimizers.\n\ndef get_group_params(\n    model: nn.Module,\n    weight_decay: float = 0.01,\n    no_decay_patterns: Optional[List[str]] = [\"bias\", \"ln_.+weight\", \"ln_.+bias\"],\n) -&gt; list[dict[str, Any]]:\n    \"\"\"function generating the appropriate group's parameters to optimize\n\n    :param model: model to train\n    :type model: nn.Module\n    :param weight_decay: weight decay factor, defaults to 0.01 \n    :type weight_decay: float\n    :param no_decay_patterns: regex patterns used to identify parameters for witch no decay is used, defaults to [\"bias\", \"ln_.+weight\", \"ln_.+bias\"]\n    :type no_decay_patterns: Optional[List[str]], optional\n    :return: two groups of parameters to optimize\n    :rtype: list[dict[str, Any]]\n    \"\"\"\n    optimizer_grouped_parameters = [\n        dict(\n            params=[\n                # parameters with weight decay\n                p for n, p in model.named_parameters() if not any(\n                    [re.search(pattern, n) for pattern in no_decay_patterns]\n                )\n            ],\n            weight_decay=weight_decay,\n        ),\n        dict(\n            params=[\n                # parameters without weight decay\n                p for n, p in model.named_parameters() if any(\n                    [re.search(pattern, n) for pattern in no_decay_patterns]\n                )\n            ],\n            weight_decay=0.,\n        )\n    ]\n    return optimizer_grouped_parameters\nIf you don’t like regex patterns, karpathy provided a nite alternative implementation in its miniGPT repository."
  },
  {
    "objectID": "posts/finetune/index.html#learning-rate",
    "href": "posts/finetune/index.html#learning-rate",
    "title": "Finetune Large Models",
    "section": "4. Learning Rate",
    "text": "4. Learning Rate\nWhile finetuning your models, you will be surprised but you will find out that the best-performing learning rate will be quite small. A sound starting point is a learning rate of 5e-5. As overmentioned, such a learning rate is extremely small compared to when you train a neural network from scratch, but it will fit the new data well and prevent catastrophic forgetting.\nMoreover, in many cases, it is beneficial to use a learning rate scheduler while fine-tuning your model. Across the many schedulers a linear decay scheduler with a warmup phase works well while being simple to implement.\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.optim.optimizer import Optimizer\n\ndef get_linear_scheduler_with_warmup(\n    optim: Optimizer,\n    num_warmup_step: float,\n    num_training_step: int,\n    last_epoch: int = -1,\n):\n    \"\"\"\n    get a scheduler that linearly increase the learning data between [0, num_warmup_steps) and then linearly decrease\n    it between [num_warmup_steps, num_training_step).\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_step:\n            return float(current_step) / float(max(1.0, num_warmup_step))\n        return max(\n            0.0,\n            float(num_training_step - current_step) /\n            float(max(1.0, num_training_step - num_warmup_step)),\n        )\n\n    return LambdaLR(optim, lr_lambda, last_epoch)\nThe assumption is to linearly increase the learning rate during the warmup phase (usually lasting for one epoch) to align the model’s parameters to the new task. Afterwards, the learning rate is reduced to allow for fine-grained adjustments of the weights. Figure 2 shows how the learning rate is adjusted across different epochs.\nNote that adopting a learning rate scheduler is beneficial, but it introduces many challenges for reproducibility. Schedulers adapt the learning rate according to the epoch and the dataset size. Thus, by simply modifiing the dataset you will likely change your scheduler. Be carefull !!!\n\n\n\n\n\n\nFigure 2: Example of linear scheduler with warmup."
  },
  {
    "objectID": "posts/finetune/index.html#parameter-feezing",
    "href": "posts/finetune/index.html#parameter-feezing",
    "title": "Finetune Large Models",
    "section": "5. Parameter feezing",
    "text": "5. Parameter feezing\nTo achieve good transferability and avoiding chatastorfic forgetting it is key to implement a proper parameter freezing stragegy. As over-mentioned it is important to minimise the number of parameter to ajdust during the fine-tuining phase to guarantee transferability across tasks. To this end, [3] proposed to only adjust bias terms to achieve a good tradeoff between learning speed and transferabilty performance. Similarly, in a multilingual setting it is common to keep frozen the embedding layers as well as the first few layers of a multilingual-BERT models ([4]) demonstrated that the first 8 layers need to be keep frozen, while the remaining are tasks-specific). Finally, the same effect is also observable in computer vision assignment: in [5] it is reported how freezing the lowest layers is key to obtain good generalization.\nFreezing parameters is extramly easy in PyTorch thanks to the requires_grad flag associated to each model’s parameter:\ndef unfreeze_layer_params(\n    model: nn.Module,\n    freeze_patterns: list[str] = [\".*bias$\"],\n):\n    for n, p in model.named_parameters():\n        \n        if any([re.search(pattern, n) for pattern in freeze_patterns]):\n            # unfreeze biases parameter\n            p.requires_grad = True\n            print(f\"unfreeze -&gt; {n}\")\n\n        else:\n            # freeze remaining parameters\n            p.requires_grad = False\n            print(f\"FREEZE -&gt; {n}\")"
  },
  {
    "objectID": "posts/finetune/index.html#overfit",
    "href": "posts/finetune/index.html#overfit",
    "title": "Finetune Large Models",
    "section": "6. Overfit",
    "text": "6. Overfit\nFinally, it is recommended to evaluate the training scripts on a single batch before executing the fine-tuning procedure. The benefits of this exercise are twofold:\n\nit enables us to detect errors or hardware limitations in the fine-tuning procedure at a low cost;\nit ensures that our model will be able to fit the fine-tuning dataset.\n\nIf the implementation is correct we expect to overfit the training batch in a few gradient updates: resulting in 0 training loss and 100 % training accuracy. Similarly, as the model keep overfitting a single training batch we expect the validation performances to degrade progressively. Figure 3 displays a good example of how the metrics should look at this stage.\n\n\n\n\n\n\n\n\n\n\n\n(a) Training loss and accuracy for a single batch.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Validation loss and accuracy for a single batch.\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "posts/normalization_flow/index.html",
    "href": "posts/normalization_flow/index.html",
    "title": "Normalizing Flows",
    "section": "",
    "text": "Normalizing Flows (NF) represent a potent technique that facilitates the learning and sampling from intricate probability distributions [1] [2]. These models, categorized as generative models, enable the precise estimation of likelihood for continuous input data, denoted as \\(p(x)\\). In contrast to methods such as variational inference that rely on approximations, normalizing flows function by transforming samples from a simple distribution, denoted as \\(z \\sim p(z)\\), into samples from a more complex distribution using the following transformation:\n\\[\nx = f_{\\theta}(z), ~~ z \\sim p(z; \\psi).\n\\]\nHere, \\(f_{\\theta}(\\cdot)\\) is a mapping function from \\(z\\) to \\(x\\), parametrized by \\(\\theta\\), and \\(p(z; \\psi)\\) is the base distribution (sometimes referred to as the prior distribution), parametrized by \\(\\psi\\), from which samples can be drawn. The essential properties defining a normalizing flow include:\nThe defining propertires of a normalizing flow are:\nAdhering to these constraints ensures the well-defined density of \\(x\\), as established by the change-of-variable theorem [3]:\n\\[\n\\begin{align*}\n    \\int p(x) \\partial x &= \\int p(z) \\partial z = 1 \\\\\n    \\implies p(x) & = p(z) \\cdot |\\frac{\\partial z}{\\partial x}| \\\\\n    & = p\\big(f_{\\theta}^{-1}(x)\\big) \\cdot |\\frac{\\partial f_{\\theta}^{-1}(x)}{\\partial x}| \\\\\n\\end{align*}\n\\]\nIn its definition, \\(\\partial x\\) represents the width of an infinitesimally small rectangle with height \\(p(x)\\). Consequently, \\(\\frac{\\partial f_{\\theta}^{-1}(x)}{\\partial x}\\) denotes the ratio between the areas of rectangles defined in two distinct coordinate systems: one in terms of \\(x\\) and the other in terms of \\(z\\).\nFor illustrative purposes, consider Figure 1, which depicts how the affine transformation \\(f_{\\theta}^{-1}(x) = (5 \\cdot x) - 2\\) maps the Normal distribution \\(p(x; \\mu=0, \\sigma=1)\\) to another Gaussian distribution \\(p(z; \\mu=-2, \\sigma=5)\\). With \\(\\frac{\\partial z}{\\partial x} = 5\\), the area \\(\\partial x\\) undergoes a stretching factor of 5 when transformed into the variable \\(z\\). Consequently, \\(p(z)\\) must be lowered by a factor of 5 to maintain its validity as a probability density function, satisfying the condition \\(\\int p(z) \\partial z = 1\\):\n\\[\np(z) = \\frac{p(x)}{\\frac{\\partial f_{\\theta}^{-1}(x)}{\\partial x}} = \\frac{p(x)}{f_{\\theta}^{-1'}(x)}.\n\\]\nIn the preceding paragraph, we introduced the concept of area-preserving transformations. Extending this notion to the multidimensional space involves considering \\(\\frac{\\partial z}{\\partial x}\\) not as a simple derivative but as the Jacobian matrix:\n\\[\nJ_{z}(x) = \\begin{bmatrix}\n    \\frac{\\partial z_1}{\\partial x_1} & \\dots & \\frac{\\partial z_1}{\\partial x_D}\\\\\n    \\vdots & \\ddots &  \\vdots \\\\\n    \\frac{\\partial z_D}{\\partial x_1} & \\dots & \\frac{\\partial z_D}{\\partial x_D}\n\\end{bmatrix}.\n\\]\nIn the multidimensional setting, the difference in areas translates to a difference in volumes quantified by the determinant of the Jacobian matrix, denoted as \\(det(J_{z}(x)) \\approx \\frac{Vol(z)}{Vol(x)}\\). Consolidating these concepts, we can formalize a multidimensional normalization flow as follows:\n\\[\n\\begin{align*}\np(x) & = p(z) \\cdot |det(\\frac{\\partial z}{\\partial x})| \\\\\n& = p(f_{\\theta}^{-1}(x)) \\cdot |det(\\frac{\\partial f_{\\theta}^{-1}(x)}{\\partial x})| \\\\\n& = p(f_{\\theta}^{-1}(x)) \\cdot |det(J_{f_{\\theta}^{-1}}(x))|.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/normalization_flow/index.html#generative-process-as-finate-composition-of-transformations",
    "href": "posts/normalization_flow/index.html#generative-process-as-finate-composition-of-transformations",
    "title": "Normalizing Flows",
    "section": "Generative Process as Finate Composition of Transformations",
    "text": "Generative Process as Finate Composition of Transformations\nIn the general case, the transformations \\(f_{\\theta}(\\cdot)\\) and \\(f_{\\theta}^{-1}(\\cdot)\\) are defined as finite compositions of simpler transformations \\(f_{\\theta_i}\\):\n\\[\n\\begin{align*}\nx & = z_{K} = f_{\\theta}(z_0) = f_{\\theta_K} \\dots f_{\\theta_2} \\circ f_{\\theta_1}(z_0) & \\\\\np(x) & = p_K(z_{k}) = p_{K-1}(f_{\\theta_K}^{-1}(z_{k})) \\cdot \\Big| det\\Big(J_{f_{\\theta_K}^{-1}}(z_{k})\\Big)\\Big| & \\\\\n& = p_{K-1}(z_{K-1}) \\cdot \\Big| det\\Big(J_{f_{\\theta_K}^{-1}}(z_k)\\Big)\\Big| & \\text{Due to the definition of } f_{\\theta_K}(z_{K-1}) = z_K\\\\\n& = p_{K-1}(z_{K-1}) \\cdot \\Big| det\\Big( J_{f_{\\theta_K}(z_{K-1})} \\Big)^{-1}\\Big| & \\text{As: } J_{f_{\\theta_K}^{-1}}(x) = \\frac{f_{\\theta_K}^{-1}(z_k)}{\\partial z_K} \\\\\n& & = \\Big(\\frac{\\partial z_K}{\\partial f_{\\theta_k}^{-1}(z_k)} \\Big)^{-1} \\\\\n& & = \\Big(\\frac{\\partial f_{\\theta_{k}}(z_{K-1})}{\\partial z_{K-1}}\\Big)^{-1} \\\\\n& & = \\Big( J_{f_{\\theta_K}(z_{K-1})} \\Big)^{-1} \\\\\n& = p_{K-1}(z_{K-1}) \\cdot \\Big| det\\Big( J_{f_{\\theta_K}}(z_{K-1}) \\Big)\\Big|^{-1}. \\\\\n\\end{align*}\n\\]\nBy this process, \\(p(z_i)\\) is fully described by \\(z_{i-1}\\) and \\(f_{\\theta_i}\\), allowing the extension of the previous reasoning to all i-steps of the overall generative process:\n\\[\n\\begin{equation}\np(x) = p(z_0) \\cdot \\prod_{i=1}^k \\Big| det \\big( J_{f_{\\theta_i}}(z_{i-1}) \\big) \\Big|^{-1}.\n\\end{equation}\n\\tag{1}\\]\nIt is noteworthy that in the context of generative models, \\(f_{\\theta}\\) is also referred to as a pushforward mapping from a simple density \\(p(z)\\) to a more complex \\(p(x)\\). On the other hand, the inverse transformation \\(f_{\\theta}^{-1}\\) is known as the normalization function, as it systematically “normalizes” a complex distribution into a simpler one, one step at a time."
  },
  {
    "objectID": "posts/normalization_flow/index.html#training-procedures",
    "href": "posts/normalization_flow/index.html#training-procedures",
    "title": "Normalizing Flows",
    "section": "Training Procedures",
    "text": "Training Procedures\nAs previously mentioned, NFs serve as efficient models for both sampling from and learning complex distributions. The primary applications of NFs lie in density estimation and data generation.\nDensity estimation proves valuable for computing statistical quantities over unseen data, as demonstrated in works such as [4] and [5], where NF models effectively estimate densities for tabular and image datasets. Additionally, NFs find application in anomaly detection [6], although requiring careful tuning for out-of-distribution detection [7].\nOn the flip side, data generation stands out as the central application for NFs. As mentioned earlier, NFs, under mild assumptions, can sample new data points from a complex distribution \\(p(x)\\). Exemplifying this, [8] showcases NFs applied to image generation, while [9] and [10] demonstrate successful learning of audio signals through NFs.\nA key advantage of NFs over other probabilistic generative models lies in their ease of training, achieved by minimizing a divergence metric between \\(p(x; \\theta)\\) and the target distribution \\(p(x)\\). In most cases, NFs are trained by minimizing the Kullback-Leibler (KL) divergence between these two distributions:\n\\[\n\\begin{align*}\n    \\mathcal{L}(\\theta) & = D_{KL}[p(x) || p(x; \\theta)] \\\\\n        & = - \\sum_{x \\sim p(x)} p(x) \\cdot \\log \\frac{p(x; \\theta)}{p(x)} \\\\\n        & = - \\sum_{x \\sim p(x)} p(x) \\cdot \\log p(x; \\theta) + \\sum_{x \\sim p(x)} p(x) \\cdot \\log p(x) \\\\\n        & = - \\mathbb{E}_{x \\sim p(x)} \\Big[ \\log p(x; \\theta) \\Big] + \\mathbb{E}_{x \\sim p(x)} \\Big[ \\log p(x) \\Big] \\\\\n        & = - \\mathbb{E}_{x \\sim p(x)}\\Big[\\log p(x; \\theta)\\Big] + const. ~~~ \\text{As it does not depend on $\\theta$} \\\\\n        & = - \\mathbb{E}_{x \\sim p(x)}\\Big[\\log p\\big(f_{\\theta}^{-1}(x)\\big) + \\sum_{i=1}^{K} \\log \\Big| det\\big( J_{f_{\\theta_i}^{-1}}(z_{i}) \\big)\\Big| \\Big] + const.\n\\end{align*}\n\\]\nHere, \\(p(f_{\\theta}^{-1}(x)) = p(z_0)\\), and \\(z_K\\) is equal to \\(x\\). For a fixed training set \\(X_N = \\\\{ x_n \\\\}_{n=1}^N\\), the loss function is derived as the negative log-likelihood typically optimized using stochastic gradient descent:\n\\[\n\\begin{equation}\n\\mathcal{L}(\\theta) = - \\frac{1}{N} \\sum_{n=1}^N \\log p\\big(f_{\\theta}^{-1}(x)\\big) + \\sum_{i=1}^{K} \\log \\Big| det\\big( J_{f_{\\theta_i}^{-1}}(z_{i}) \\big)\\Big|.\n\\end{equation}\n\\tag{2}\\]\nIt is important to note that the loss function (Equation 2) is computed by starting from a datapoint \\(x\\) and reversing it to a plausible latent variable \\(z_0\\). Consequently, the structural formulation of \\(p(z_0)\\) plays a critical role in defining the training signals: if \\(p(z_0)\\) is too lax, the training process lacks substantial information; if it is too stringent, the training process may become overly challenging. Furthermore, the training process is the inverse of the generative process defined in Equation 1, emphasizing the importance of the sum of determinants. Achieving computationally efficient training requires the efficient computation of determinants of \\(J_{f_{\\theta_i}^{-1}}\\). While auto-diff libraries can compute gradients with respect to \\(\\theta_i\\) of the Jacobian matrix and its determinant, such computations are computationally expensive (\\(O(n)^3\\)). Therefore, significant research efforts have focused on designing transformations with efficient Jacobian determinant formulations.\n\nTraining Example\nAs previously mentioned, the training process of a NF involves mapping a given input data \\(x\\) to a specific base distribution \\(p(z_0)\\). Typically, the base distribution is a well-known distribution such as a multivariate Gaussian, Uniform, or any other exponential distribution. Similarly, the mapping function is usually implemented as a neural network.\nStarting from first principles, any NF model can be specified as comprising a base distribution and a series of flows that map \\(x\\) to \\(z_0\\). Here is a Python implementation:\nclass NormalizingFlow(nn.Module):\n    def __init__(\n        self,\n        prior: Distribution,\n        flows: nn.ModuleList,\n    ):\n        super().__init__()\n        self.prior = prior\n        self.flows = flows\n\n    def forward(self, x: Tensor):\n        bs = x.size(0)\n        zs = [x]\n        sum_log_det = torch.zeros(bs).to(x.device)\n        for flow in self.flows:\n            z = flow(x)\n            log_det = flow.log_abs_det_jacobian(x, z)\n            zs.append(z)\n            sum_log_det += log_det\n            x = z\n\n        prior_logprob = self.prior.log_prob(z).view(bs, -1).sum(-1)\n        log_prob = prior_logprob + sum_log_det\n\n        intermediat_results = dict(\n            prior_logprob=prior_logprob,\n            sum_log_det=sum_log_det,\n            zs=zs,\n        )\n        return log_prob, intermediat_results\nwhere the prior can be any base distribution inplemented in torch.distributions, and flows can be any module that statisfy the NF’s properties.\nSuppose we are given a 1D dataset, as shown in Fig. [2.a]. We can fit an NF to the underlying probability distribution \\(p(x)\\) of the dataset. To successfully learn the density of the dataset, we need a base distribution (let’s say a Beta distribution parameterized by \\(\\alpha = 2\\) and \\(\\beta = 5\\)) and a functional definition for our flow. In this case, let’s use the cumulative distribution function of a Gaussian Mixture Model (GMM) with 4 different components:\nmodel = NormalizingFlow(\n    prior=Beta(2.0, 5.0),\n    flows=nn.ModuleList([\n        GMMFlow(n_components=4, dim=1)\n    ]),\n)\noptimizer = optim.AdamW(model.parameters(), lr=5e-3, weight_decay=1e-5)\nWith these ingredients, we can train the model by minimizing the negative log-likelihood using stochastic gradient descent (SGD):\nfor epoch in range(epochs):\n    model.train()\n\n    for idx, (x, y) in enumerate(dataloader):\n        optimizer.zero_grad()\n        log_prob, _ = model(x)\n        loss = -log_prob.mean()  # nll\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1)\n        optimizer.step()\nNote that no labels are used for training, as the objective is to directly maximize the predicted density of the dataset.\nFigure 2 shows the learned density and how the 4 different components are used to correctly model \\(p(x)\\). While the same results might be achieved by using only 2 components, in the general case, the minimum number of needed components is not known a priori; thus using a larger number of components is a good practice.\nFinally, Figure 4 demonstrates how the learned model is able to map a dataset coming from an unknown density to the Beta distributin over-defined.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Training dataset build by sampling 750 elements from two distinct gaussian distributions\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: A normalizing flow fitted to the given dataset to learn p(x). The normalizing flow is composed by a beta distributon as a prior and as a gaussian mixture model with 4 different component as a flow.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Learned normalizing flow from the unknown distribution \\(p(x)\\) to the choosen prior \\(p(z)\\).\n\n\n\nFull code is contained in the following notebook.\n\n\n2D Training Example\nConsider a more intricate dataset, such as the famous 2 Moon dataset depicted in Fig. [3.a]. The objective here is to map samples from this dataset into a latent variable that conforms to a Gaussian distribution.\nIn this context, relying solely on the cumulative distribution function of a Gaussian Mixture model as NF formulation may not provide the necessary expressiveness. While Neural Networks serve as powerful function approximators, they do not inherently guarantee the conditions required by a normalizing flow. Furthermore, computing the determinant of a linear layer within a neural network is computationally expensive.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: 2D Moon dataset.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Gif of all the steps needed by the normalization flow to map the 2 Moon dataset into a Gaussian distribution.\n\n\n\n\n\n\nIn recent years, Coupling layers [4] have emerged as effective solutions for Normalizing Flows. They prove efficient both during sampling and training, while delivering competitive performances. The fundamental idea involves splitting the input variables of the i-th layer into equally sized groups:\n\nThe first group of input variables (\\(z_i[0], ..., z_i[d]\\)) is considered constant during the i-th layer1.\nThe second group of parameters (\\(z_{i}[d+1], ..., z_{i}[D]\\)) undergoes transformation by a Neural Network that depends solely on \\(z_{i}[\\leq d]\\).\n\nMathematically, we can represent the transformation applied to all input variables in the i-th layer as:\n\\[\n\\begin{align*}\n    z_{i+1}[0], ..., z_{i+1}[d] & = z_{i}[0], ..., z_{i}[d] \\\\\n    d_{i}[d+1], ..., d_{i}[D], t_{i}[d+1], ..., t_{i}[D] & = f(z_{i}[0], ..., z_{i}[d]; \\theta_{i}) \\\\\n    z_{i+1}[d+1], ..., z_{i+1}[D] & = (z_{i}[d+1] \\cdot d_{i}[d+1]) + t_{i}[d+1], ..., (z_{i}[D] \\cdot d_{i}[D]) + t_{i}[D]\n\\end{align*}\n\\]\nwhere \\(f(\\cdot; \\theta_i)\\) is any neural network. Intuitively, a coupling layer is akin to an autoregressive layer, where the autoregressive mask only permits \\(z_{i+1}[&gt;d]\\) to depend on \\(z_{i}[\\leq d]\\).\nAs shown in Figure 7 and Figure 8, the beauty of coupling layers lies in the ease of inverting their transformation. Given the initial conditiokn \\(z_{i+1}[\\leq d] = z_{i}[\\leq d]\\), it is possible to derive the affine parameters \\(d_{i}[&gt; d]\\) and \\(t_{i}[&gt; d]\\) by directly applying \\(f(\\cdot; \\theta_i)\\) to \\(z_{i+1}[\\leq d]\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Coupling layer forward pass.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Coupling layer backward pass.\n\n\n\n\n\n\nBy construction, the Jacobian matrix of any such layer is lower triangular, following the structure:\n\\[\nJ_{z_{i+1}}(z_{i}) = \\begin{bmatrix}\n    \\mathbf{I} & \\mathbf{O} \\\\\n    \\mathbf{A} & \\mathbf{D} \\\\\n\\end{bmatrix}.\n\\]\nHere, \\(\\mathbf{I}\\) is an identity matrix of size \\(d \\times d\\), \\(\\mathbf{O}\\) is a zeros matrix of size \\(d \\times (D-d)\\), \\(\\mathbf{A}\\) is a full matrix of size \\((D-d) \\times d\\) and \\(\\mathbf{D}\\) is a diagonal matrix of shape \\((D-d) \\times (D-d)\\). The determinant of such a matrix is formed by the product of the diagonal elements of \\(\\mathbf{D}\\), making it efficient to compute.\nFigure 9 illustrates the dynamics of a NF trained on a 2 Moon dataset. Note how the final latent space (step 4) conforms to a Gaussian distribution.\n\n\n\n\n\n\nFigure 9: Normalizing flow from a 2 moon dataset to the guassian prior visualized step by step. Bottom right picture shows the distribution of the final latent variable extracted by the flow, demonstrating that it is clrearly Gaussian.\n\n\n\nFinally, a simple implementation of a coupling layer in pytorch is proviceded as follow:\nclass CouplingFlow(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        mask: Tensor,\n    ) -&gt; None:\n        super().__init__()\n        assert dim % 2 == 0, \"dim must be even\"\n        assert dim == mask.size(-1), \"mask dimension must equal dim\"\n\n        self.dim = dim\n        self.hidden_dim = hidden_dim\n        self.register_buffer(\"mask\", mask)\n\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.LeakyReLU(),\n            nn.Linear(hidden_dim, dim),\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x_masked = x * self.mask\n        output = self.net(x_masked)\n        log_d, t = output.chunk(2, dim=-1)\n        z = x_masked + ((1 - self.mask) * (x * torch.exp(log_d) + t))\n        return z\n\n    def log_abs_det_jacobian(\n        self,\n        x: Tensor,\n        z: Tensor,\n    ) -&gt; Tensor:\n        x_masked = x * self.mask\n        log_d, t = self.net(x_masked).chunk(2, dim=-1)\n        return log_d.sum(dim=-1)"
  },
  {
    "objectID": "posts/normalization_flow/index.html#footnotes",
    "href": "posts/normalization_flow/index.html#footnotes",
    "title": "Normalizing Flows",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we introduce the notation \\(z_{i}[d]\\) as indicating the \\(d\\) dimention of the latent variable at the i-th layer of a flow (\\(z_{i}\\)).↩︎"
  },
  {
    "objectID": "posts/vea/index.html",
    "href": "posts/vea/index.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Introduction\nIn this article, I will delve into the theoretical foundations of Variational Autoencoders (VAE). You can find the code used for both Convolutional Neural Network (CNN) and normal feedforward autoencoder trained on the MNIST dataset on my GitHub repository.\nVAEs are generative models grounded in Bayesian inference theory and variational inference. The underlying concept involves generating data points from a given latent variable that encodes the characteristics of the desired data. To illustrate, consider the task of dwaring an animal. Initially, we conceptualize the animal with specific criteria, such as having four legs and the ability to swim. With these criteria, we can draw the animal by sampling from the animal kingdom.\nLet use define some notation:\n\n\\(x\\) represents a data point.\n\\(z\\) is a latent variable.\n\\(p(x)\\) denotes the probability distribution of the data.\n\\(p(z)\\) signifies the probability of the latent variable indicating the type of generated data.\n\\(p(x|z)\\) represents the distribution of the generated data based on a latent variable. Analogously, it is akin to transforming imagination into reality.\n\\(D_{KL}\\big(p(X) \\parallel q(X)\\big) = \\sum_{x_i \\in X} p(x_i) \\log \\frac{p(x_i)}{q(x_i)} = - \\sum_{x_i \\in X} p(x_i) \\log \\frac{q(x_i)}{p(x_i)}\\) is the Kullback-Leibler (KL) divergence between two discrete distributions.\n\nKL divergence possesses notable properties: firstly, \\(D_{KL}\\big(p(x) \\parallel q(x)\\big)\\) is not equal to \\(D_{KL}\\big(q(x) \\parallel p(x)\\big)\\), indicating its asymmetric nature. Secondly, \\(D_{KL} &gt; 0\\).\n\n\nVariationa Autoencoders\nVariational Autoencoders function as generative models, enabling the sampling of new data points from such a model. In general, generative models learn a functional form of \\(p(x)\\) that allows for sampling. However, \\(p(x)\\)is often unknown, and only a dataset \\(\\hat{X} = (x_i)^N_{i=1}\\) comprising some samples from \\(p(x)\\) is provided.\nVAEs overcome this challenge by leveraging the concept that high-dimensional data is generated based on a low-dimensional latent variable \\(z\\); thus, the joint distribution can be factorized as \\(p(x,z)=p(x∣z)p(z)\\). Ultimately, through marginalization, we can define \\(p(x)\\) as:\n\\[\np(x) = \\int p(x|z) p(z) \\partial z.\n\\]\nIn our earlier analogy, \\(z\\) represents the imagined concept, while \\(x\\) is the realization of all the selected concepts. As mentioned before, during the training phase of VAEs, access is neither given to \\(p(x)\\) nor to the latent variable \\(z\\) used to generate the dataset. However, throughout the training process, a reasonable posterior distribution \\(p(z∣x)\\) is learned. This approach makes sense, as the goal is to make the latent variable likely under the observed data, thereby generating plausible data.\nAccording to Bayesian theory:\n\\[\np(z|x) = \\frac{p(x|z)\\cdot p(z)}{p(x)} = \\frac{p(x, z)}{p(x)}.\n\\]\nAs mentioned earlier, \\(p(x)\\) can be expressed through marginalization over \\(z\\); however, such computation is typically intractable as it involves integrating over all latent dimensions:\n\\[\np(x) \\int ... \\int \\int p(x|z)\\cdot p(z) \\partial z_i.\n\\]\nTo overcome this computational challenge, variational inference suggests approximating \\(p(z∣x)\\) with a simpler distribution \\(q(z∣x)\\). By assigning a tractable form to \\(q(z∣x)\\), such as a Gaussian distribution, and adjusting its parameters to closely match \\(p(z∣x)\\), we can overcome the intractability issue.\nFormally, we can rewrite our goal as: \\[\n\\begin{align*}\n\\min D_{KL}\\big(q(z|x) || p(z|x)\\big) & = - \\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{p(z|x)}{q(z|x)}  \\\\\n& = - \\sum_{x \\in \\hat{X}} q(z|x) \\log \\Big(\\frac{p(x, z)}{q(z|x)}  \\cdot \\frac{1}{p(x)} \\Big) \\\\\n& = - \\sum_{x \\in \\hat{X}} q(z|x) \\log \\Big(\\frac{p(x, z)}{q(z|x)}  - \\log p(x) \\Big)  \\\\\n& = - \\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{p(x, z)}{q(z|x)} + \\sum_{x \\in \\hat{X}} q(z|x) \\log p(x) \\\\\n& = \\log p(x) - \\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{p(x, z)}{q(z|x)} ~~~~ \\small{\\text{:as $\\sum_{x \\in \\hat{X}} q(z|x) = 1$ and $p(x)$ do not depend on $z$}}\n\\end{align*}\n\\]\nBy rearranging the above equation we can state that: \\[\n\\log p(x) = D_{KL}\\big(q(z|x) || p(z|x)\\big) + \\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{p(x, z)}{q(z|x)}.\n\\]\nHowever, \\(p(x)\\) is constant for a given dataset \\(\\hat{X}\\), thus minimizing \\(D_{KL}\\big(q(z|x) || p(z|x)\\big)\\) is equivalent to maximise \\(\\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{p(x, z)}{q(z|x)}\\) up to a constant factor. Such formulation of the KL-divergenve is also known as the Evidence Lower Bound (ELBO) and it is tractable:\n\\[\n\\begin{align*}\n\\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{p(x, z)}{q(z|x)} & = \\sum_{x \\in \\hat{X}} q(z|x) \\log \\big( \\frac{p(x|z) p(z)}{q(z|x)}\\big)  \\\\\n& = \\sum_{x \\in \\hat{X}} q(z|x) \\Big(\\log p(x|z) + \\log \\frac{p(z)}{q(z|x)} \\Big)  \\\\\n& = \\sum_{x \\in \\hat{X}} q(z|x) \\log p(x|z) + \\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{p(z)}{q(z|x)} \\\\\n& = \\mathbb{E}_{z \\sim q(z|x)} \\big[ \\log p(x|z) \\big] - \\sum_{x \\in \\hat{X}} q(z|x) \\log \\frac{q(z|x)}{p(z)} \\\\\n& = \\mathbb{E}_{z \\sim q(z|x)} \\big[ \\log p(x|z) \\big] - \\mathbb{E}_{z \\sim q(z|x)} \\big[ \\log q(z|x) - \\log p(z) \\big] \\\\\n& = \\mathbb{E}_{z \\sim q(z|x)} \\big[ \\log p(x|z) \\big] - D_{KL}\\big( q(z|x) \\parallel p(z) \\big).\n\\end{align*}\n\\]\nThe initial component of the ELBO, denoted as \\(\\mathbb{E}_{z \\sim q(z|x)} \\big[ \\log p(x|z) \\big]\\) , is commonly known as the (negative) reconstruction error. This is because it involves encoding \\(x\\) into \\(z\\) and then decoding it back. The second segment, \\(D_{KL}\\big( q(z|x)\\parallel p(z) \\big)\\) can be viewed as a regularization term that imposes a specific distribution on \\(q\\).\n\n\nResults\nBased on the code, we have trained a CNN-based Variational Autoencoder on the MNIST dataset. Figure 1 report the training loss, while Figure 2 shows us some generated example. As it is possible to see, there are still some artifact. Maybe a better activation function would provide better results.\n\n\n\n\n\n\nFigure 1: Training loss of a CNN based VAE on the MNIST dataset.\n\n\n\n\n\n\n\n\n\nFigure 2: Generated examples."
  }
]