[
  {
    "objectID": "notebooks/interactive_marginal_joint_conditional.html#interactive-demonstration-of-marginal-joint-and-conditional-probabilities-and-distributions",
    "href": "notebooks/interactive_marginal_joint_conditional.html#interactive-demonstration-of-marginal-joint-and-conditional-probabilities-and-distributions",
    "title": "Sandro Cavallari",
    "section": "Interactive Demonstration of Marginal, Joint and Conditional Probabilities and Distributions",
    "text": "Interactive Demonstration of Marginal, Joint and Conditional Probabilities and Distributions\n\nMichael J. Pyrcz, Professor, The University of Texas at Austin\nNovel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions\n\nLoad and Configure the Required Libraries\nThe following code loads the required libraries and sets a plotting default.\n\nimport os  # operating system\nimport numpy as np\nimport matplotlib.pyplot as plt  # plotting\nfrom matplotlib.ticker import (\n    MultipleLocator,\n    AutoMinorLocator,\n    AutoLocator,\n)  # control of axes ticks\n\nplt.rc(\"axes\", axisbelow=False)  # set axes and grids in the background for all plots\nimport matplotlib.patches as patches\nfrom ipywidgets import interactive  # widgets and interactivity\nfrom ipywidgets import widgets\nfrom ipywidgets import Layout\nfrom ipywidgets import Label\nfrom ipywidgets import VBox, HBox\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")  # supress warnings\n\n\n\nSet the working directory\nI always like to do this so I don’t lose files and to simplify subsequent read and writes (avoid including the full address each time). Also, in this case make sure to place the required (see below) data file in this directory. When we are done with this tutorial we will write our new dataset back to this directory.\n\n# os.chdir(\"c:/PGE383\")                           # set the working directory\n\n\n\nCalculate a Synthetic Mining Grade Bivariate Dataset\nLet’s make a bivariate distribution with the following steps:\n\ndraw values from a bivariate Guassian distribution with some correlation\nimpose a nonlinear relationship with a specified weight\ncorrect the mean and standard deviations of each feature\n\nThen we scatter plot to check the data visually.\n\nseed = 73073\nn = 5000\nAg_mean = 125\nAu_mean = 8\nAg_stdev = 20\nAu_stdev = 1\nwt_poly = 0.002\nnp.random.seed(seed=seed)\nX = np.random.multivariate_normal([Au_mean, Ag_mean], [[9, 0], [0, 400]], size=n)\nX[:, 0] = (1.0 - wt_poly) * X[:, 0] + wt_poly * (-0.5 * np.power(X[:, 1] - 200, 2) + 10)\nX[:, 0] = (X[:, 0] - np.mean(X[:, 0])) * Au_stdev / np.std(X[:, 0]) + Au_mean\n\nplt.subplot(111)\nplt.scatter(X[:, 1], X[:, 0], c=\"darkorange\", s=20, alpha=0.2, edgecolor=\"black\")\nplt.xlabel(\"Silver, Ag, Grade (g/t)\")\nplt.ylabel(\"Gold, Au, Grade (g/t)\")\nplt.title(\"Mineral Grades\")\nplt.xlim([60, 200])\nplt.ylim([4, 11])\n\nplt.gca().grid(True, which=\"major\", linewidth=1.0)\nplt.gca().grid(True, which=\"minor\", linewidth=0.2)  # add y grids\nplt.gca().tick_params(which=\"major\", length=7)\nplt.gca().tick_params(which=\"minor\", length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator())\nplt.gca().yaxis.set_minor_locator(AutoMinorLocator())  # turn on minor ticks\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1)\nplt.show()  # set plot size\n\n\n\n\n\n\n\n\n\n\nBuild the Interactive Dashboard to Demonstrate Joint Probability\nThe following code:\n\ntake the previous synthetic dataset and specifiy events:\n\nA - silver grade within a bin\nB - gold grade within a bin\n\ncalculate conditional probability, \\(P\\{B , A\\}\\)\n\n\nl = widgets.Text(\n    value=\"                               Joint Probability Demo, Prof. Michael Pyrcz, The University of Texas at Austin\",\n    layout=Layout(width=\"750px\", height=\"30px\"),\n)\n\nn = widgets.IntSlider(\n    min=0,\n    max=5000,\n    value=1000,\n    step=100,\n    description=\"$n$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\nix = widgets.IntSlider(\n    min=0,\n    max=6,\n    value=1,\n    step=1,\n    description=r\"Silver Bin, $i_{\\theta_{Ag}}$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\niy = widgets.IntSlider(\n    min=0,\n    max=6,\n    value=1,\n    step=1,\n    description=r\"Gold Bin, $i_{\\theta_{Au}}$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\n\nui = widgets.HBox(\n    [n, ix, iy],\n)\nui2 = widgets.VBox(\n    [l, ui],\n)\n\n\ndef run_plot(n, ix, iy):\n    seed = 73073\n    Ag_mean = 125\n    Au_mean = 8\n    Ag_stdev = 20\n    Au_stdev = 1\n    wt_poly = 0.002\n    np.random.seed(seed=seed)\n    X = np.random.multivariate_normal([Au_mean, Ag_mean], [[9, 0], [0, 400]], size=n)\n    X[:, 0] = (1.0 - wt_poly) * X[:, 0] + wt_poly * (\n        -0.5 * np.power(X[:, 1] - 200, 2) + 10\n    )\n    X[:, 0] = (X[:, 0] - np.mean(X[:, 0])) * Au_stdev / np.std(X[:, 0]) + Au_mean\n\n    xsiz = 20\n    ysiz = 1\n    x_start = ix * xsiz + 60\n    x_end = (ix + 1) * xsiz + 60\n    y_start = iy * ysiz + 4\n    y_end = (iy + 1) * ysiz + 4\n\n    count = (\n        (y_start &lt; X[:, 0])\n        & (X[:, 0] &lt; y_end)\n        & (x_start &lt; X[:, 1])\n        & (X[:, 1] &lt; x_end)\n    ).sum()\n\n    Xsub = X[\n        (\n            (y_start &lt; X[:, 0])\n            & (X[:, 0] &lt; y_end)\n            & (x_start &lt; X[:, 1])\n            & (X[:, 1] &lt; x_end)\n        )\n    ]\n\n    fig, axs = plt.subplots(\n        1, 2, figsize=(16, 7), gridspec_kw={\"width_ratios\": [0.8, 1]}\n    )\n\n    axs[0].scatter(X[:, 1], X[:, 0], c=\"r\", s=20, alpha=0.2, edgecolor=\"black\")\n    axs[0].scatter(\n        Xsub[:, 1], Xsub[:, 0], c=\"black\", s=20, alpha=0.6, edgecolor=\"black\"\n    )\n    axs[0].set_xlabel(\"Silver, Ag, Grade (g/t)\")\n    axs[0].set_ylabel(\"Gold, Au, Grade (g/t)\")\n    axs[0].set_title(\"Mineral Grades Scatter Plot\")\n    axs[0].set_xlim([60, 200])\n    axs[0].set_ylim([4, 11])\n\n    axs[0].grid(True, which=\"major\", linewidth=1.0)\n    axs[0].grid(True, which=\"minor\", linewidth=0.2)  # add y grids\n    axs[0].tick_params(which=\"major\", length=7)\n    axs[0].tick_params(which=\"minor\", length=4)\n    axs[0].xaxis.set_minor_locator(AutoMinorLocator(n=10))\n    axs[0].yaxis.set_minor_locator(AutoMinorLocator(n=10))  # turn on minor ticks\n    axs[0].xaxis.set_major_locator(MultipleLocator(xsiz))\n    axs[0].yaxis.set_major_locator(MultipleLocator(ysiz))  # turn on minor ticks\n\n    rect = patches.Rectangle(\n        (x_start, 4), xsiz, 7 * ysiz, linewidth=3, edgecolor=\"green\", facecolor=\"none\"\n    )\n    axs[0].add_patch(rect)\n    rect = patches.Rectangle(\n        (60, y_start), 7 * xsiz, ysiz, linewidth=3, edgecolor=\"blue\", facecolor=\"none\"\n    )\n    axs[0].add_patch(rect)\n    rect = patches.Rectangle(\n        (x_start, y_start), xsiz, ysiz, linewidth=5, edgecolor=\"black\", facecolor=\"none\"\n    )\n    axs[0].add_patch(rect)\n\n    axs[0].annotate(\n        r\"A = [\" + str(x_start) + r\" ≤ $\\bf{\\theta_{Ag}}$ ≤ \" + str(x_end) + \"]\",\n        (150, 4.6),\n        color=\"green\",\n        weight=\"bold\",\n    )\n\n    axs[0].annotate(\n        r\"B = [\" + str(y_start) + r\" ≤ $\\bf{\\theta_{Au}}$ ≤ \" + str(y_end) + \"]\",\n        (150, 4.3),\n        color=\"blue\",\n        weight=\"bold\",\n    )\n\n    axs[0].annotate(r\"$\\bf{A}$\", (x_start + 0.4 * xsiz, 10.7), size=15, color=\"green\")\n    axs[0].annotate(r\"$\\bf{B}$\", (61.5, y_start + 0.4 * ysiz), size=15, color=\"blue\")\n    axs[0].annotate(\n        r\"$\\bf{A \\bigcap B}$\",\n        (x_start + 0.2 * xsiz, y_start + 0.4 * ysiz),\n        size=10,\n        color=\"black\",\n    )\n\n    if ix &lt;= 3:\n        axs[0].annotate(\n            r\"n{A $\\bigcap$ B} = \" + str(count),\n            (x_end + 0.2 * xsiz, y_start + ysiz * 0.5),\n            color=\"black\",\n            weight=\"bold\",\n        )\n    else:\n        axs[0].annotate(\n            r\"n{A $\\bigcap$ B} = \" + str(count),\n            (x_end - 2.8 * xsiz, y_start + ysiz * 0.5),\n            color=\"black\",\n            weight=\"bold\",\n        )\n\n    hist2d = axs[1].hist2d(\n        X[:, 1],\n        X[:, 0],\n        bins=7,\n        range=[[60, 200], [4, 11]],\n        density=False,\n        weights=None,\n        cmin=None,\n        cmax=None,\n        cmap=plt.cm.Reds,\n    )\n    axs[1].set_xlabel(\"Silver, Ag, Grade (g/t)\")\n    axs[1].set_ylabel(\"Gold, Au, Grade (g/t)\")\n    axs[1].set_title(\"Mineral Grades Frequency Table\")\n    axs[1].set_xlim([60, 200])\n    axs[1].set_ylim([4, 11])\n\n    cbar = plt.colorbar(hist2d[3], orientation=\"vertical\")\n    cbar.set_label(\"Frequency\", rotation=270, labelpad=20)\n\n    axs[1].grid(True, which=\"major\", linewidth=1.0)\n    axs[1].grid(True, which=\"minor\", linewidth=0.2)  # add y grids\n    axs[1].tick_params(which=\"major\", length=7)\n    axs[1].tick_params(which=\"minor\", length=4)\n    axs[1].xaxis.set_minor_locator(AutoMinorLocator(n=10))\n    axs[1].yaxis.set_minor_locator(AutoMinorLocator(n=10))  # turn on minor ticks\n    axs[1].xaxis.set_major_locator(MultipleLocator(xsiz))\n    axs[1].yaxis.set_major_locator(MultipleLocator(ysiz))  # turn on minor ticks\n\n    rect = patches.Rectangle(\n        (x_start, 4), xsiz, 7 * ysiz, linewidth=3, edgecolor=\"green\", facecolor=\"none\"\n    )\n    axs[1].add_patch(rect)\n    rect = patches.Rectangle(\n        (60, y_start), 7 * xsiz, ysiz, linewidth=3, edgecolor=\"blue\", facecolor=\"none\"\n    )\n    axs[1].add_patch(rect)\n    rect = patches.Rectangle(\n        (x_start, y_start), xsiz, ysiz, linewidth=5, edgecolor=\"black\", facecolor=\"none\"\n    )\n    axs[1].add_patch(rect)\n\n    axs[1].scatter(X[:, 1], X[:, 0], c=\"black\", s=20, alpha=0.1, edgecolor=\"black\")\n    axs[1].scatter(\n        Xsub[:, 1], Xsub[:, 0], c=\"black\", s=20, alpha=0.3, edgecolor=\"black\"\n    )\n\n    axs[1].annotate(\n        r\"A = [\" + str(x_start) + r\" ≤ $\\bf{\\theta_{Ag}}$ ≤ \" + str(x_end) + \"]\",\n        (150, 4.6),\n        color=\"green\",\n        weight=\"bold\",\n    )\n\n    axs[1].annotate(r\"$\\bf{A}$\", (x_start + 0.4 * xsiz, 10.7), size=15, color=\"green\")\n    axs[1].annotate(r\"$\\bf{B}$\", (61.5, y_start + 0.4 * ysiz), size=15, color=\"blue\")\n    axs[1].annotate(\n        r\"$\\bf{A \\bigcap B}$\",\n        (x_start + 0.2 * xsiz, y_start + 0.4 * ysiz),\n        size=10,\n        color=\"black\",\n    )\n\n    axs[1].annotate(\n        r\"B = [\" + str(y_start) + r\" ≤ $\\bf{\\theta_{Au}}$ ≤ \" + str(y_end) + \"]\",\n        (150, 4.3),\n        color=\"blue\",\n        weight=\"bold\",\n    )\n\n    if ix &lt;= 3:\n        z = axs[1].annotate(\n            r\"P{A $\\bigcap$ B} = \" + str(count) + \"/\" + str(n),\n            (x_end + 0.2 * xsiz, y_start + ysiz * 0.5),\n            color=\"black\",\n            weight=\"bold\",\n        )\n    else:\n        z = axs[1].annotate(\n            r\"P{A $\\bigcap$ B} = \" + str(count) + \"/\" + str(n),\n            (x_end - 3.5 * xsiz, y_start + ysiz * 0.5),\n            color=\"black\",\n            weight=\"bold\",\n        )\n\n\n# connect the function to make the samples and plot to the widgets\ninteractive_plot = widgets.interactive_output(\n    run_plot,\n    {\n        \"n\": n,\n        \"ix\": ix,\n        \"iy\": iy,\n    },\n)\ninteractive_plot.clear_output(wait=True)  # reduce flickering by delaying plot updating\n\n\n\n\nInteractive Joint Probability Demonstation\nThe joint probability distribution of two or more random variables \\(X\\) and \\(Y\\) represent the likelyhood of observing in out dataset the specific combination of \\(X=x_i\\) and \\(Y=y_j\\) that is \\(P(X=x_i, Y=y_j)\\).\nThe joint probability must satisfay: - $ 0 P(X=x_i, Y=Y_j)$ - $ {i=1}^N {j=1}^M P(X=x_i, Y=y_j) = 1 $\nIn the continuosu case we can just replase summations by integrals over the domains.\n\nMichael Pyrcz, Professor, The University of Texas at Austin\nChange the number of sample data, select silver and gold bins.\n\n\n\nThe Inputs\n\nn - number of data, \\(i_{\\theta_{Ag}}\\) - silver bin, and \\(i_{\\theta_{Au}}\\) - gold bin\n\n\ndisplay(ui2, interactive_plot)  # display the interactive plot\n\n\n\n\n\n\n\n\nrun_plot(\n    n=1000,\n    ix=3,\n    iy=3,\n)\n\n\n\n\n\n\n\n\n\nBuild the Interactive Dashboard to Demonstrate Conditional Probability\n\nBy Michael Pyrcz, Professor, The University of Texas at Austin\nThe following code:\n\ntake the previous synthetic dataset and specifiy events:\n\nA - silver grade within a bin\nB - gold grade within a bin\n\ncalculate conditional probability, \\(P\\{B | A\\} = \\frac{P(B \\cap A)}{P(A)} = \\frac{P(A, B)}{P(A)}\\)\n\nChange the number of sample data, select silver and gold bins.\n\n\n\n\nThe Inputs\n\nn - number of data, \\(i_{\\theta_{Ag}}\\) - silver bin, and \\(i_{\\theta_{Au}}\\) - gold bin\n\n\nl = widgets.Text(\n    value=\"                               Conditional Probability Demo, Prof. Michael Pyrcz, The University of Texas at Austin\",\n    layout=Layout(width=\"750px\", height=\"30px\"),\n)\n\nn = widgets.IntSlider(\n    min=0,\n    max=5000,\n    value=1000,\n    step=100,\n    description=\"$n$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\nix = widgets.IntSlider(\n    min=0,\n    max=6,\n    value=1,\n    step=1,\n    description=r\"Silver Bin, $i_{\\theta_{Ag}}$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\niy = widgets.IntSlider(\n    min=0,\n    max=6,\n    value=1,\n    step=1,\n    description=r\"Gold Bin, $i_{\\theta_{Au}}$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\n\nui = widgets.HBox(\n    [n, ix, iy],\n)\nui3 = widgets.VBox(\n    [l, ui],\n)\n\n\ndef run_plot(n, ix, iy):\n    seed = 73073\n    Ag_mean = 125\n    Au_mean = 8\n    Ag_stdev = 20\n    Au_stdev = 1\n    wt_poly = 0.002\n    np.random.seed(seed=seed)\n    X = np.random.multivariate_normal([Au_mean, Ag_mean], [[9, 0], [0, 400]], size=n)\n    X[:, 0] = (1.0 - wt_poly) * X[:, 0] + wt_poly * (\n        -0.5 * np.power(X[:, 1] - 200, 2) + 10\n    )\n    X[:, 0] = (X[:, 0] - np.mean(X[:, 0])) * Au_stdev / np.std(X[:, 0]) + Au_mean\n\n    xsiz = 20\n    ysiz = 1\n    x_start = ix * xsiz + 60\n    x_end = (ix + 1) * xsiz + 60\n    y_start = iy * ysiz + 4\n    y_end = (iy + 1) * ysiz + 4\n\n    count = (\n        (y_start &lt; X[:, 0])\n        & (X[:, 0] &lt; y_end)\n        & (x_start &lt; X[:, 1])\n        & (X[:, 1] &lt; x_end)\n    ).sum()\n    count_a = ((x_start &lt; X[:, 1]) & (X[:, 1] &lt; x_end)).sum()\n    count_b = ((y_start &lt; X[:, 0]) & (X[:, 0] &lt; y_end)).sum()\n\n    Xsub_a = X[((x_start &lt; X[:, 1]) & (X[:, 1] &lt; x_end))]\n    Xsub = X[\n        (\n            (y_start &lt; X[:, 0])\n            & (X[:, 0] &lt; y_end)\n            & (x_start &lt; X[:, 1])\n            & (X[:, 1] &lt; x_end)\n        )\n    ]\n\n    fig, axs = plt.subplots(\n        1, 2, figsize=(16, 7), gridspec_kw={\"width_ratios\": [0.8, 1]}\n    )\n\n    axs[0].scatter(\n        Xsub_a[:, 1], Xsub_a[:, 0], c=\"black\", s=20, alpha=0.4, edgecolor=\"black\"\n    )\n    axs[0].scatter(X[:, 1], X[:, 0], c=\"r\", s=20, alpha=0.2, edgecolor=\"black\")\n    axs[0].scatter(\n        Xsub[:, 1], Xsub[:, 0], c=\"black\", s=20, alpha=0.8, edgecolor=\"black\"\n    )\n    axs[0].set_xlabel(\"Silver, Ag, Grade (g/t)\")\n    axs[0].set_ylabel(\"Gold, Au, Grade (g/t)\")\n    axs[0].set_title(\"Mineral Grades Scatter Plot\")\n    axs[0].set_xlim([60, 200])\n    axs[0].set_ylim([4, 11])\n\n    axs[0].grid(True, which=\"major\", linewidth=1.0)\n    axs[0].grid(True, which=\"minor\", linewidth=0.2)  # add y grids\n    axs[0].tick_params(which=\"major\", length=7)\n    axs[0].tick_params(which=\"minor\", length=4)\n    axs[0].xaxis.set_minor_locator(AutoMinorLocator(n=10))\n    axs[0].yaxis.set_minor_locator(AutoMinorLocator(n=10))  # turn on minor ticks\n    axs[0].xaxis.set_major_locator(MultipleLocator(xsiz))\n    axs[0].yaxis.set_major_locator(MultipleLocator(ysiz))  # turn on minor ticks\n\n    rect = patches.Rectangle(\n        (x_start, 4), xsiz, 7 * ysiz, linewidth=3, edgecolor=\"green\", facecolor=\"none\"\n    )\n    axs[0].add_patch(rect)\n    rect = patches.Rectangle(\n        (60, y_start),\n        7 * xsiz,\n        ysiz,\n        linewidth=3,\n        ls=\"--\",\n        edgecolor=\"blue\",\n        alpha=0.4,\n        facecolor=\"none\",\n    )\n    axs[0].add_patch(rect)\n    rect = patches.Rectangle(\n        (x_start, y_start), xsiz, ysiz, linewidth=5, edgecolor=\"black\", facecolor=\"none\"\n    )\n    axs[0].add_patch(rect)\n\n    axs[0].annotate(\n        r\"A = [\" + str(x_start) + r\" ≤ $\\bf{\\theta_{Ag}}$ ≤ \" + str(x_end) + \"]\",\n        (150, 4.6),\n        color=\"green\",\n        weight=\"bold\",\n    )\n\n    axs[0].annotate(\n        r\"B = [\" + str(y_start) + r\" ≤ $\\bf{\\theta_{Au}}$ ≤ \" + str(y_end) + \"]\",\n        (150, 4.3),\n        color=\"blue\",\n        weight=\"bold\",\n    )\n\n    axs[0].annotate(r\"$\\bf{A}$\", (x_start + 0.4 * xsiz, 10.7), size=15, color=\"green\")\n    axs[0].annotate(r\"$\\bf{B}$\", (61.5, y_start + 0.4 * ysiz), size=15, color=\"blue\")\n    axs[0].annotate(\n        r\"$\\bf{A \\bigcap B}$\",\n        (x_start + 0.2 * xsiz, y_start + 0.4 * ysiz),\n        size=10,\n        color=\"black\",\n    )\n\n    if ix &lt;= 3:\n        axs[0].annotate(\n            r\"n{A $\\bigcap$ B} = \" + str(count),\n            (x_end + 0.2 * xsiz, y_start + ysiz * 0.6),\n            color=\"black\",\n            weight=\"bold\",\n        )\n        axs[0].annotate(\n            r\"n{A} = \" + str(count_a),\n            (x_end + 0.2 * xsiz, y_start + ysiz * 0.2),\n            color=\"black\",\n            weight=\"bold\",\n        )\n    else:\n        axs[0].annotate(\n            r\"n{A $\\bigcap$ B} = \" + str(count),\n            (x_end - 2.8 * xsiz, y_start + ysiz * 0.5),\n            color=\"black\",\n            weight=\"bold\",\n        )\n\n    hist2d = axs[1].hist2d(\n        X[:, 1],\n        X[:, 0],\n        bins=7,\n        range=[[60, 200], [4, 11]],\n        density=False,\n        weights=None,\n        cmin=None,\n        cmax=None,\n        cmap=plt.cm.Reds,\n    )\n    axs[1].set_xlabel(\"Silver, Ag, Grade (g/t)\")\n    axs[1].set_ylabel(\"Gold, Au, Grade (g/t)\")\n    axs[1].set_title(\"Mineral Grades Conditional Frequency Table\")\n    axs[1].set_xlim([60, 200])\n    axs[1].set_ylim([4, 11])\n\n    cbar = plt.colorbar(hist2d[3], orientation=\"vertical\")\n    cbar.set_label(\"Frequency\", rotation=270, labelpad=20)\n\n    axs[1].grid(True, which=\"major\", linewidth=1.0)\n    axs[1].grid(True, which=\"minor\", linewidth=0.2)  # add y grids\n    axs[1].tick_params(which=\"major\", length=7)\n    axs[1].tick_params(which=\"minor\", length=4)\n    axs[1].xaxis.set_minor_locator(AutoMinorLocator(n=10))\n    axs[1].yaxis.set_minor_locator(AutoMinorLocator(n=10))  # turn on minor ticks\n    axs[1].xaxis.set_major_locator(MultipleLocator(xsiz))\n    axs[1].yaxis.set_major_locator(MultipleLocator(ysiz))  # turn on minor ticks\n\n    rect = patches.Rectangle(\n        (60, 4), xsiz * ix, 7 * ysiz, linewidth=0, edgecolor=\"none\", facecolor=\"white\"\n    )\n    axs[1].add_patch(rect)\n    rect = patches.Rectangle(\n        (60 + xsiz * (ix + 1), 4),\n        60 + xsiz * (7 - (ix + 1)),\n        7 * ysiz,\n        linewidth=0,\n        edgecolor=\"none\",\n        facecolor=\"white\",\n    )\n    axs[1].add_patch(rect)\n\n    rect = patches.Rectangle(\n        (x_start, 4), xsiz, 7 * ysiz, linewidth=3, edgecolor=\"green\", facecolor=\"none\"\n    )\n    axs[1].add_patch(rect)\n    rect = patches.Rectangle(\n        (60, y_start),\n        7 * xsiz,\n        ysiz,\n        linewidth=3,\n        ls=\"--\",\n        edgecolor=\"blue\",\n        alpha=0.4,\n        facecolor=\"none\",\n    )\n    axs[1].add_patch(rect)\n    rect = patches.Rectangle(\n        (x_start, y_start), xsiz, ysiz, linewidth=5, edgecolor=\"black\", facecolor=\"none\"\n    )\n    axs[1].add_patch(rect)\n\n    axs[1].scatter(\n        Xsub_a[:, 1], Xsub_a[:, 0], c=\"black\", s=20, alpha=0.2, edgecolor=\"black\"\n    )\n    axs[1].scatter(X[:, 1], X[:, 0], c=\"black\", s=20, alpha=0.03, edgecolor=\"black\")\n    axs[1].scatter(\n        Xsub[:, 1], Xsub[:, 0], c=\"black\", s=20, alpha=0.6, edgecolor=\"black\"\n    )\n\n    axs[1].annotate(\n        r\"A = [\" + str(x_start) + r\" ≤ $\\bf{\\theta_{Ag}}$ ≤ \" + str(x_end) + \"]\",\n        (150, 4.6),\n        color=\"green\",\n        weight=\"bold\",\n    )\n\n    axs[1].annotate(r\"$\\bf{A}$\", (x_start + 0.4 * xsiz, 10.7), size=15, color=\"green\")\n    axs[1].annotate(r\"$\\bf{B}$\", (61.5, y_start + 0.4 * ysiz), size=15, color=\"blue\")\n    axs[1].annotate(\n        r\"$\\bf{A \\bigcap B}$\",\n        (x_start + 0.2 * xsiz, y_start + 0.4 * ysiz),\n        size=10,\n        color=\"black\",\n    )\n\n    axs[1].annotate(\n        r\"B = [\" + str(y_start) + r\" ≤ $\\bf{\\theta_{Au}}$ ≤ \" + str(y_end) + \"]\",\n        (150, 4.3),\n        color=\"blue\",\n        weight=\"bold\",\n    )\n\n    if ix &lt;= 3:\n        z = axs[1].annotate(\n            r\"P{B | A} = \" + str(count) + \"/\" + str(count_a),\n            (x_end + 0.2 * xsiz, y_start + ysiz * 0.5),\n            color=\"black\",\n            weight=\"bold\",\n        )\n    else:\n        z = axs[1].annotate(\n            r\"P{B | A} = \" + str(count) + \"/\" + str(count_a),\n            (x_end - 3.5 * xsiz, y_start + ysiz * 0.5),\n            color=\"black\",\n            weight=\"bold\",\n        )\n\n\n# connect the function to make the samples and plot to the widgets\ninteractive_plot2 = widgets.interactive_output(\n    run_plot,\n    {\n        \"n\": n,\n        \"ix\": ix,\n        \"iy\": iy,\n    },\n)\ninteractive_plot2.clear_output(wait=True)  # reduce flickering by delaying plot updating\n\n\ndisplay(ui3, interactive_plot2)  # display the interactive plot\n\n\n\n\n\n\n\n\nrun_plot(\n    n=1000,\n    ix=3,\n    iy=3,\n)\n\n\n\n\n\n\n\n\n\nBuild the Interactive Dashboard to Demonstrate Conditional Distributions\nMarginalizing is the process to retireve the probability of a single variable from a joint distribution. Suppose we want to compute \\(P(X=x_1)\\), if we have access to \\(P(X, Y)\\) we can just define: \\[P(X=x_1) = \\sum_{j=1}^M P(X=x_1, Y=y_j)\\]\nThis process of summing over all values of a variable to get the probability distribution of the other variable is known as marginalization.\nThe following code:\n\ntake the previous synthetic dataset and specifiy events:\n\nA - silver grade within a bin\nB - gold grade within a bin\n\ncalculate conditional distributions, \\(f_{(B | A)}\\) and \\(f_{(A | B)}\\), as histograms and compare to the marginals, \\(f_B\\) and \\(f_A\\) respectively.\n\n\nl = widgets.Text(\n    value=\"                               Conditional Distributions Demo, Prof. Michael Pyrcz, The University of Texas at Austin\",\n    layout=Layout(width=\"750px\", height=\"30px\"),\n)\n\nn = widgets.IntSlider(\n    min=0,\n    max=5000,\n    value=1000,\n    step=100,\n    description=\"$n$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\nix = widgets.IntSlider(\n    min=0,\n    max=6,\n    value=1,\n    step=1,\n    description=r\"Silver Bin, $i_{\\theta_{Ag}}$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\niy = widgets.IntSlider(\n    min=0,\n    max=6,\n    value=1,\n    step=1,\n    description=r\"Gold Bin, $i_{\\theta_{Au}}$\",\n    orientation=\"horizontal\",\n    style={\"description_width\": \"initial\"},\n    continuous_update=False,\n)\n\nui = widgets.HBox(\n    [n, ix, iy],\n)\nui5 = widgets.VBox(\n    [l, ui],\n)\n\n\ndef run_plot5(n, ix, iy):\n    seed = 73073\n    Ag_mean = 125\n    Au_mean = 8\n    Ag_stdev = 20\n    Au_stdev = 1\n    wt_poly = 0.002\n    np.random.seed(seed=seed)\n    X = np.random.multivariate_normal([Au_mean, Ag_mean], [[9, 0], [0, 400]], size=n)\n    X[:, 0] = (1.0 - wt_poly) * X[:, 0] + wt_poly * (\n        -0.5 * np.power(X[:, 1] - 200, 2) + 10\n    )\n    X[:, 0] = (X[:, 0] - np.mean(X[:, 0])) * Au_stdev / np.std(X[:, 0]) + Au_mean\n\n    xsiz = 20\n    ysiz = 1\n    x_start = ix * xsiz + 60\n    x_end = (ix + 1) * xsiz + 60\n    y_start = iy * ysiz + 4\n    y_end = (iy + 1) * ysiz + 4\n\n    count = (\n        (y_start &lt; X[:, 0])\n        & (X[:, 0] &lt; y_end)\n        & (x_start &lt; X[:, 1])\n        & (X[:, 1] &lt; x_end)\n    ).sum()\n    count_a = ((x_start &lt; X[:, 1]) & (X[:, 1] &lt; x_end)).sum()\n    count_b = ((y_start &lt; X[:, 0]) & (X[:, 0] &lt; y_end)).sum()\n\n    Xsub_a = X[((x_start &lt; X[:, 1]) & (X[:, 1] &lt; x_end))]\n    Xsub_b = X[((y_start &lt; X[:, 0]) & (X[:, 0] &lt; y_end))]\n    Xsub = X[\n        (\n            (y_start &lt; X[:, 0])\n            & (X[:, 0] &lt; y_end)\n            & (x_start &lt; X[:, 1])\n            & (X[:, 1] &lt; x_end)\n        )\n    ]\n\n    plt_scatter = plt.subplot2grid((3, 3), (1, 0), rowspan=2, colspan=2)\n    plt_x1 = plt.subplot2grid((3, 3), (0, 0), colspan=2, sharex=plt_scatter)\n    plt_x2 = plt.subplot2grid((3, 3), (1, 2), rowspan=2, sharey=plt_scatter)\n\n    # plt.plot([0,0],[1.0,1.0],color = 'black')\n    plt_scatter.scatter(\n        Xsub_a[:, 1], Xsub_a[:, 0], color=\"none\", edgecolor=\"green\", alpha=0.9\n    )\n    plt_scatter.scatter(\n        Xsub_b[:, 1], Xsub_b[:, 0], color=\"none\", edgecolor=\"blue\", alpha=0.9\n    )\n    plt_scatter.scatter(\n        X[:, 1], X[:, 0], color=\"red\", edgecolor=\"black\", alpha=0.1, label=\"samples\"\n    )\n    # plt_scatter.scatter(sample[:,0],sample[:,1],color = 'red',alpha = 0.8,edgecolors='black',label = 'Samples')\n    plt_scatter.legend(loc=\"upper left\")\n    plt_scatter.set_xlabel(\"Silver, Ag, Grade (g/t)\")\n    plt_scatter.set_ylabel(\"Gold, Au, Grade (g/t)\")\n    plt_scatter.set_title(\"Mineral Grades Conditional Frequency Table\")\n    plt_scatter.set_xlim([60, 200])\n    plt_scatter.set_ylim([4, 11])\n    plt_scatter.legend(loc=\"upper right\")\n\n    plt_scatter.grid(True, which=\"major\", linewidth=1.0)\n    plt_scatter.grid(True, which=\"minor\", linewidth=0.2)  # add y grids\n    plt_scatter.tick_params(which=\"major\", length=7)\n    plt_scatter.tick_params(which=\"minor\", length=4)\n    plt_scatter.xaxis.set_minor_locator(AutoMinorLocator(n=10))\n    plt_scatter.yaxis.set_minor_locator(AutoMinorLocator(n=10))  # turn on minor ticks\n    plt_scatter.xaxis.set_major_locator(MultipleLocator(xsiz))\n    plt_scatter.yaxis.set_major_locator(MultipleLocator(ysiz))  # turn on minor ticks\n\n    # rect = patches.Rectangle((60, 4), xsiz*ix, 7*ysiz, linewidth=0, edgecolor='none', facecolor='white'); plt_scatter.add_patch(rect)\n    # rect = patches.Rectangle((60+xsiz*(ix+1), 4), 60+xsiz*(7 - (ix+1)), 7*ysiz, linewidth=0, edgecolor='none', facecolor='white'); plt_scatter.add_patch(rect)\n\n    rect = patches.Rectangle(\n        (x_start, 4), xsiz, 7 * ysiz, linewidth=3, edgecolor=\"green\", facecolor=\"none\"\n    )\n    plt_scatter.add_patch(rect)\n    rect = patches.Rectangle(\n        (60, y_start),\n        7 * xsiz,\n        ysiz,\n        linewidth=3,\n        edgecolor=\"blue\",\n        alpha=0.4,\n        facecolor=\"none\",\n    )\n    plt_scatter.add_patch(rect)\n\n    plt_scatter.annotate(\n        r\"A = [\" + str(x_start) + r\" ≤ $\\bf{\\theta_{Ag}}$ ≤ \" + str(x_end) + \"]\",\n        (150, 4.6),\n        color=\"green\",\n        weight=\"bold\",\n    )\n\n    plt_scatter.annotate(\n        r\"B = [\" + str(y_start) + r\" ≤ $\\bf{\\theta_{Au}}$ ≤ \" + str(y_end) + \"]\",\n        (150, 4.3),\n        color=\"blue\",\n        weight=\"bold\",\n    )\n    plt_scatter.annotate(\n        r\"$\\bf{A}$\", (x_start + 0.4 * xsiz, 10.6), size=15, color=\"green\"\n    )\n    plt_scatter.annotate(\n        r\"$\\bf{B}$\", (61.5, y_start + 0.4 * ysiz), size=15, color=\"blue\"\n    )\n\n    plt_x1.hist(\n        X[:, 1],\n        density=True,\n        color=\"red\",\n        alpha=0.3,\n        edgecolor=\"black\",\n        bins=np.linspace(60, 200, 30),\n        label=\"marginal\",\n    )\n    plt_x1.hist(\n        Xsub_b[:, 1],\n        density=True,\n        color=\"blue\",\n        alpha=0.3,\n        edgecolor=\"black\",\n        bins=np.linspace(60, 200, 30),\n        label=\"conditional\",\n    )\n\n    plt_x1.set_xlim([60, 200])\n    plt_x1.legend(loc=\"upper right\")\n    plt_x1.set_xlabel(\"Silver, Ag, Grade (g/t)\")\n    plt_x1.set_ylabel(r\"Density\")\n    plt_x1.set_title(r\"Marginal and Conditional Distributions\")\n\n    plt_x2.hist(\n        X[:, 0],\n        orientation=\"horizontal\",\n        density=True,\n        color=\"red\",\n        alpha=0.3,\n        edgecolor=\"black\",\n        bins=np.linspace(4, 11, 30),\n        label=\"marginal\",\n    )\n    plt_x2.hist(\n        Xsub_a[:, 0],\n        orientation=\"horizontal\",\n        density=True,\n        color=\"green\",\n        alpha=0.3,\n        edgecolor=\"black\",\n        bins=np.linspace(4, 11, 30),\n        label=\"conditional\",\n    )\n\n    plt_x2.set_ylim([4, 11])\n    plt_x2.legend(loc=\"upper right\")\n    plt_x2.set_ylabel(\"Gold, Au, Grade (g/t)\")\n    plt_x2.set_xlabel(r\"Density\")\n\n    plt.subplots_adjust(\n        left=0.0, bottom=0.0, right=1.5, top=1.7, wspace=0.3, hspace=0.4\n    )\n    plt.show()\n\n\n# connect the function to make the samples and plot to the widgets\ninteractive_plot5 = widgets.interactive_output(\n    run_plot5,\n    {\n        \"n\": n,\n        \"ix\": ix,\n        \"iy\": iy,\n    },\n)\ninteractive_plot5.clear_output(wait=True)  # reduce flickering by delaying plot updating\n\n\ndisplay(ui5, interactive_plot5)  # display the interactive plot\n\n\n\n\n\n\n\n\nrun_plot5(\n    1000,\n    3,\n    5,\n)\n\n\n\n\n\n\n\n\n\n\nComments\nThis was a basic demonstration marginal, joint and conditional probabilities. I have many other demonstrations and even basics of working with DataFrames, ndarrays, univariate statistics, plotting data, declustering, data transformations and many other workflows available at https://github.com/GeostatsGuy/PythonNumericalDemos and https://github.com/GeostatsGuy/GeostatsPy.\n\n\nThe Author:\n\n\n\nMichael J. Pyrcz, Professor, The University of Texas at Austin\nNovel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions\nWith over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers’ and geoscientists’ impact in subsurface resource development.\nFor more about Michael check out these links:\n\nTwitter | GitHub | Website | GoogleScholar | Book | YouTube | LinkedIn\n\n\nWant to Work Together?\nI hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n\nWant to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I’d be happy to drop by and work with you!\nInterested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\nI can be reached at mpyrcz@austin.utexas.edu.\n\nI’m always happy to discuss,\nMichael\nMichael Pyrcz, Ph.D., P.Eng. Professor, The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, Jackson School of Geosciences, The University of Texas at Austin\n\n\nMore Resources Available at: Twitter | GitHub | Website | GoogleScholar | Book | YouTube | LinkedIn"
  },
  {
    "objectID": "posts/gradient_descent_and_backprop/index.html",
    "href": "posts/gradient_descent_and_backprop/index.html",
    "title": "Gradinet Descent and Backpropagation",
    "section": "",
    "text": "Most deep learning algorithm relay on the idea of learning some useful information from the data to solve a specific task. That is, instead of explicitly define every single instruction that a program has to perform, in machine learning, we specify an optimization routine that a program executes over a set of examples to improve its performances. By executing the optimization algorithm, a machine automatically navigates the solution space to find the best “program” that solve the given task starting from a random state: \\(\\mathbf{\\theta}\\). It is expectable that the initial program obtained based on the random state would not perform well on the chosen task, however, by iterating over the dataset, we can adjust \\(\\mathbf{\\theta}\\) until we obtain an optimal solution.\n\nGradient Descent\nOne of the most common learning algorithm is known as Gradient Descent or Stochastic Gradient Descent (SGD) [1]. The core idea of SGD is to iteratively evaluate the difference between the obtained prediction of the model (\\(y_{\\theta}\\)), and, the desired output (\\(y\\)) utilizing a loss function \\(\\mathcal{L}(y_{\\mathbf{\\theta}}, y)\\). Once the difference is known, it is possible to adjust \\(\\theta\\) to reduce the difference or prediction error.\nFormally, SGD is composed of 3 main steps:\n\nevaluate the loss function: \\(\\mathcal{L}(y_{\\mathbf{\\theta}}, y)\\),\ncompute the gradient of the loss function w.r.t. the model parameters: \\(\\nabla \\mathcal{L}_{\\theta} = \\frac{\\partial \\mathcal{L}(y_{\\theta}, y)}{\\partial \\theta}\\),\nupdate the model parameters (or solution) to decrease the loss function: \\(\\mathbf{\\theta} = \\mathbf{\\theta} - \\eta \\nabla \\mathcal{L}_{\\theta}\\).\n\nAs it is possible to notice, such a learning algorithm requires a loss function that is continuous and differentiable; otherwise, it is not applicable. However, over the years, many efficient and effective loss functions have been proposed.\n\n\nBackpropagation\nComputing the analytical gradients for a deep learning algorithm might not be easy, and it is definitely an error-prone procedure. Luckily, over the years mathematicians manage to programmatically compute the derivate of most of the functions with a procedure known as algorithmic differentiation. The application of algorithmic differentiation to compute the SGD is known as backpropagation.\nSupposing to have the current function \\(f(x,y,z) = (x + y) \\cdot z\\). It is possible to simplify it’s computation defining an intermediate function:\n\\[\nq(x, y) = x + y \\Rightarrow f(q, z) = q \\cdot z.\n\\]\nKnowing that:\n\n\\(\\frac{\\partial f}{\\partial q} = z\\)\n\\(\\frac{\\partial f}{\\partial z} = q\\)\n\\(\\frac{\\partial q}{\\partial x} = 1\\)\n\\(\\frac{\\partial q}{\\partial y} = 1\\)\n\nwe can compute \\(\\frac{\\partial f}{\\partial x}\\) by chain rule:\n\\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x}.\n\\]\nThis operation can be seen even as a computational graph, where each node represent an operation ; and using backpropagation it is possible to compute the gradient of function \\(f\\) w.r.t. its input variable \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\n\nFigure 1: The forward and backward pass of the computational graph for the function $ f(x,y,z) = (x + y) z $. (Image taken from Andrej Karpathy slides, CS231n.)\n\n\n\nIt has to be noted that, backpropagation is a local and global process. It is local since a gate, during the forward pass, can compute:\n\nits output value: \\(q = x + y = 3\\),\nas well as its local gradient (the gradient of its input w.r.t. its output): \\(\\frac{\\partial q}{\\partial x} = 1\\) and \\(\\frac{\\partial q}{\\partial y} = 1\\).\n\nIt is global since, a gate need to know the gradient of its output node in order to evaluate the chain rules: \\(\\frac{\\partial f}{\\partial q}\\). The gradient of its ouput is known only during the backward pass, thus all the local computations need to be stored in memory; thus require a lot of memory.\nThe backward pass start by computing: \\(\\frac{f}{\\partial f} = 1\\). Then, knowing that \\(\\frac{\\partial f}{\\partial q} = z\\) and \\(\\frac{\\partial f}{\\partial q} = \\frac{f}{\\partial f} \\frac{\\partial f}{\\partial q} = 1 \\cdot -4 = -4\\). Similarly, \\(\\frac{\\partial f}{\\partial z} = q\\) and \\(\\frac{\\partial f}{\\partial z} = \\frac{f}{\\partial f} \\frac{\\partial f}{\\partial z} = 3\\). Finally, our goal is to goal is to compute: \\[\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = -4 \\cdot 1 = -4\n\\] and, \\[\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = -4 \\cdot 1 = -4\n\\].\n\n\nWeight Decay\nTo achieve better generaliation performance it is well known that graient updates needs to be regularized so to have sparse or force small weights magnitude. The two most common regularizations for gradiens are L1-regularization or weight decay [2] (equivalent to the L2-regularization):\n\\[\n\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\partial f(x; \\theta_t)}{\\partial \\theta_t} - \\lambda \\theta_t\n\\]\nwhere \\(\\lambda \\theta_t\\) stand for weight decay or L2-regularization. However, weight dacay and L2-regularization are equivalent only for SDG, but not for Adam or other adaptive optimizers. Instead of applying the same learning rate to all parameters, Adam apply a different learning rate to each parameters proportional to the update signals they recently recevied (a.k.a proportional to the recent gradients). As Adam uses a different learning rate per each parameters, it means that L2-regularization is not only affected by \\(\\lambda\\) but also from the learning rate and the momentum. Thus, Adam requires a bigger regularizer coefficent to achieve comparable performance as SGD.\n\n\nReferences\n\n\n1. Hanson S, Pratt L (1988) Comparing biases for minimal network construction with back-propagation. Advances in neural information processing systems 1\n\n\n2. Gugger S, Howard J Fast.ai - AdamW and Super-convergence is now the fastest way to train neural nets — fast.ai"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Gradinet Descent and Backpropagation\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nDec 22, 2020\n\n\nSandro Cavallari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is my personal space where I keep all my notes from my ML experience. I’m mostly interested in NLP and graph embeddings/analytics. Thus expect most of the material to cover:\n\nFundamentals of Deep Learning;\nFundamentals of Reinforcement Learning;\nBasic Stats;\nBasic of Neural ODE.\n\nPlease note that this website is ment to keep only my personal material, this it my contains errors and typos.\n\nHere are some working principles I try to apply during my working expeience.\n\n\n\nThe Skunk Works workers must be delegated practically complete control of his program in all aspects.\nThe number of people having any connection with the project must be restricted in an almost vicious manner. Use a small number of good people (10% to 25% compared to the so-called normal systems).\nA very simple drawing and drawing release system with great flexibility for making changes must be provided.\nThere must be a minimum number of reports required, but important work must be recorded thoroughly.\nAccess by outsiders to the project and its personnel must be strictly controlled by appropriate security measures.\nBecause only a few people will be used in engineering and most other areas, ways must be provided to reward good performance by pay not based on the number of personnel supervised.\n\n\n\n\n\nMake the requirements less dumb. “Your requirements are definitely dumb, it does not matter who gave them to you.” Uses this step to test assumptions and question-the-question, especialy if it come from a ‘smart person’ since you might not question them enough.\nDelete the part or process. “If you’re not adding things back in at least 10% of the time, you’re clearly not deleting enough.” Starting lean and building up when and if required, but be strongly bias throward removing thinks. Each requirement or constraint must be accountable to a person, so you can ask that person about its relevance and purpose.\nSimplify or optimise the design. “Possibly the most common error of a smart engineer is to optimise a thing that should not exist”. Do your work and go through the first two steps before trying to optimise. To do this effectively you need an holistic view of the project and you need to consider the overall cost-benefit for a more complicated design.\nAccelerate cycle time. “Try a bunch of things, and do more of what works”. Once you have find the right direction is time to accelerate the lifecycle. Aim for an experiment a day. Note that, it is extreamly importanto to do the first 3 steps before as “if you’re digging your grave, don’t dig faster.”"
  },
  {
    "objectID": "index.html#my-vision-of-skunkworks-rules",
    "href": "index.html#my-vision-of-skunkworks-rules",
    "title": "Welcome",
    "section": "",
    "text": "The Skunk Works workers must be delegated practically complete control of his program in all aspects.\nThe number of people having any connection with the project must be restricted in an almost vicious manner. Use a small number of good people (10% to 25% compared to the so-called normal systems).\nA very simple drawing and drawing release system with great flexibility for making changes must be provided.\nThere must be a minimum number of reports required, but important work must be recorded thoroughly.\nAccess by outsiders to the project and its personnel must be strictly controlled by appropriate security measures.\nBecause only a few people will be used in engineering and most other areas, ways must be provided to reward good performance by pay not based on the number of personnel supervised."
  },
  {
    "objectID": "index.html#musks-development-principles",
    "href": "index.html#musks-development-principles",
    "title": "Welcome",
    "section": "",
    "text": "Make the requirements less dumb. “Your requirements are definitely dumb, it does not matter who gave them to you.” Uses this step to test assumptions and question-the-question, especialy if it come from a ‘smart person’ since you might not question them enough.\nDelete the part or process. “If you’re not adding things back in at least 10% of the time, you’re clearly not deleting enough.” Starting lean and building up when and if required, but be strongly bias throward removing thinks. Each requirement or constraint must be accountable to a person, so you can ask that person about its relevance and purpose.\nSimplify or optimise the design. “Possibly the most common error of a smart engineer is to optimise a thing that should not exist”. Do your work and go through the first two steps before trying to optimise. To do this effectively you need an holistic view of the project and you need to consider the overall cost-benefit for a more complicated design.\nAccelerate cycle time. “Try a bunch of things, and do more of what works”. Once you have find the right direction is time to accelerate the lifecycle. Aim for an experiment a day. Note that, it is extreamly importanto to do the first 3 steps before as “if you’re digging your grave, don’t dig faster.”"
  }
]